<!DOCTYPE html>
<html  lang="zh">
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
<title>线性回归与梯度下降算法 - Young&#39;s Blog</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />



    <meta name="description" content="【摘要】 本文以线性回归为例，讲解了批量梯度下降、随机梯度下降、小批量梯度下降、冲量梯度下降等算法，由浅入深，并结合精心设计的例子，使读者最快掌握这种最常用的优化方法。每一种优化方法，笔者都基于R语言给出了相应的代码，供读者参考，  梯度下降假如我们有以下身高和体重的数据，我们希望用身高来预测体重。如果你学过统计，那么很自然地就能想到建立一个线性回归模型： $$y=a+bx$$ 其中$a$是截距">
<meta name="keywords" content="机器学习,优化算法,梯度下降">
<meta property="og:type" content="article">
<meta property="og:title" content="线性回归与梯度下降算法">
<meta property="og:url" content="http://yoursite.com/2018/05/05/梯度下降/index.html">
<meta property="og:site_name" content="Young&#39;s Blog">
<meta property="og:description" content="【摘要】 本文以线性回归为例，讲解了批量梯度下降、随机梯度下降、小批量梯度下降、冲量梯度下降等算法，由浅入深，并结合精心设计的例子，使读者最快掌握这种最常用的优化方法。每一种优化方法，笔者都基于R语言给出了相应的代码，供读者参考，  梯度下降假如我们有以下身高和体重的数据，我们希望用身高来预测体重。如果你学过统计，那么很自然地就能想到建立一个线性回归模型： $$y=a+bx$$ 其中$a$是截距">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://yoursite.com/images/og_image.png">
<meta property="og:updated_time" content="2018-05-09T09:00:46.773Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="线性回归与梯度下降算法">
<meta name="twitter:description" content="【摘要】 本文以线性回归为例，讲解了批量梯度下降、随机梯度下降、小批量梯度下降、冲量梯度下降等算法，由浅入深，并结合精心设计的例子，使读者最快掌握这种最常用的优化方法。每一种优化方法，笔者都基于R语言给出了相应的代码，供读者参考，  梯度下降假如我们有以下身高和体重的数据，我们希望用身高来预测体重。如果你学过统计，那么很自然地就能想到建立一个线性回归模型： $$y=a+bx$$ 其中$a$是截距">
<meta name="twitter:image" content="http://yoursite.com/images/og_image.png">







<link rel="icon" href="/images/favicon.svg">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css">


    
    
    
    <style>body>.footer,body>.navbar,body>.section{opacity:0}</style>
    

    
    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">
    

    
    

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">


    
    
    
    

<link rel="stylesheet" href="/css/back-to-top.css">


    
    

    
    
    
    

    
    
<link rel="stylesheet" href="/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>

    
    
    

    
    
    


<link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body class="is-3-column">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/">
            
                <img src="/images/logo.svg" alt="线性回归与梯度下降算法" height="28">
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item"
                href="/">Home</a>
                
                <a class="navbar-item"
                href="/archives">Archives</a>
                
                <a class="navbar-item"
                href="/categories">Categories</a>
                
                <a class="navbar-item"
                href="/tags">Tags</a>
                
                <a class="navbar-item"
                href="/about">About</a>
                
            </div>
            
            <div class="navbar-end">
                
                    
                    
                    <a class="navbar-item" target="_blank" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus">
                        
                        <i class="fab fa-github"></i>
                        
                    </a>
                    
                
                
                
                <a class="navbar-item search" title="搜索" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-8-tablet is-8-desktop is-6-widescreen has-order-2 column-main"><div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2018-05-04T16:00:00.000Z">2018-05-05</time>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    32 分钟 读完 (大约 4805 个字)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                线性回归与梯度下降算法
            
        </h1>
        <div class="content">
            <blockquote>
<p>【摘要】</p>
<p>本文以线性回归为例，讲解了批量梯度下降、随机梯度下降、小批量梯度下降、冲量梯度下降等算法，由浅入深，并结合精心设计的例子，使读者最快掌握这种最常用的优化方法。每一种优化方法，笔者都基于R语言给出了相应的代码，供读者参考，</p>
</blockquote>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>假如我们有以下身高和体重的数据，我们希望用身高来预测体重。如果你学过统计，那么很自然地就能想到建立一个线性回归模型：</p>
<p>$$y=a+bx$$</p>
<p>其中$a$是截距，$b$是斜率，$y$是体重，$x$是身高。</p>
<p><img src="/picture/1525406306009-1525855946681.png" alt="1525406306009"></p>
<p><img src="/picture/1525406028458.png" alt="1525406028458"></p>
<p>我们将身高与体重的关系在Excel里面用折线图表示，并且添加了线性的趋势线。蓝色的线条是真实数据，红色的实线是模型给出的预测值。蓝色线条与红色线条之间的距离绝对值是预测误差。所以，我们要找到最优的$a$和$b$来拟合这条直线，使得我们模型的总误差最小。</p>
<p>$$Error = \frac{1}{2}(Actual\ weight - Predicted\ weight)^2=\frac{1}{2}(Y-Ypred)^2$$</p>
<p>我们使用均方误差来表示模型的误差，由于$Ypred = a + bx$，因此，模型的均方误差可以表示为</p>
<p>$$SSE = \sum \frac{1}{2}(Y-a-bx)^2$$</p>
<p>也就是说，$SSE$是关于$a$和$b$的函数，我们只需要不断调整$a$和$b$，使$SSE$降到最低就可以了。这个时候，我们就可以利用梯度下降算法，来求解$a$和$b$的值。</p>
<p>梯度下降的计算过程如下：</p>
<blockquote>
<p>step 1:随机初始化权重$a$和$b$，计算出误差$SSE$</p>
<p>step 2:计算梯度。    $a$和$b$的轻微变化都会导致$SSE$的变化，因此，我们只需要找到能使$SSE$减小的$a$和$b$的变化方向就可以了。这个方向，一般就是由梯度决定的。</p>
<p>step 3:调整权重值，使得$SSE$不断接近最小值。</p>
<p>step 4:使用新的权重去做预测，并且计算出新的$SSE$。</p>
<p>step 5:重复step2-step3，直到权重不再显著变化为止。</p>
</blockquote>
<p>我们在Excel中进行上述步骤。为了计算能够快一点，我们首先对数据进行Min-Max标准化。得到如下数据：</p>
<p><img src="/picture/1525407652713.png" alt="1525407652713"></p>
<p>step1:随机选取一组权重(此处我们设置a=0,b=1),我们计算出预测值和误差：</p>
<p><img src="/picture/1525408865090.png" alt="1525408865090"></p>
<p>step2:计算梯度</p>
<p>$$\frac{\partial SSE}{\partial a} = \sum-(Y-a-bx)=\sum-(Y-Ypred)$$</p>
<p>$$\frac{\partial SSE}{\partial b}=\sum-(Y-a-bx)x=\sum-(Y-Ypred)x$$</p>
<p>$\frac{\partial SSE}{\partial a}$和$\frac{\partial SSE}{\partial b}$就是梯度，他们决定了$a$和$b$的移动方向和距离。    </p>
<p>step3: 调整权重值，使得$SSE$不断接近最小值。</p>
<p>调整规则为:</p>
<p>$$a_{new} = a_{old} - \eta \nabla a = a_{old} - \eta \cdot \partial SSE/\partial a$$</p>
<p>$$b_{new} = b_{old} - \eta \nabla b = b_{old} - \eta \cdot \partial SSE / \partial b$$</p>
<p>其中，$\eta$是一个被我们称之为学习率(learning rate)的东西，一般设置为0.01或者你希望的任何比较小的数值。</p>
<p>本文选择0.01作为学习率。</p>
<p>$$a_{new} = 0 - 0.01 \times 1.925 = -0.01925$$</p>
<p>$$b_{new} = 1 - 0.01 \times 1.117 = 0.98883$$</p>
<p>step4:使用新的权重去做预测，并且计算出新的$SSE$。</p>
<p><img src="/picture/1525410198759.png" alt="1525410198759"></p>
<p>可以看出，SSE从0.155降低到0.111，说明系数有改善。</p>
<p>step 5:重复step2-step3，直到权重不再显著变化为止。</p>
<p>我们知道，一元线性回归的系数可以用公式计算，我们用R的lm()函数来计算权重，结果为</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">lm(y~x,dat)</span><br><span class="line"></span><br><span class="line">Call:</span><br><span class="line">lm(formula = y ~ x, data = dat)</span><br><span class="line"></span><br><span class="line">Coefficients:</span><br><span class="line">(Intercept)            x  </span><br><span class="line">-<span class="hljs-number">0.1167</span>       <span class="hljs-number">0.9777</span></span><br></pre></td></tr></table></figure>



<p>然后，我在R里面写了一个梯度下降的函数，当精度调到0.0000001的时候，与lm的结果已经很接近了。</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">gradientDescent &lt;- <span class="hljs-keyword">function</span>(dat,start = c(<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),learning_rate = <span class="hljs-number">0.01</span>,tol = <span class="hljs-number">0.001</span>)</span><br><span class="line">&#123;</span><br><span class="line">	a = start[<span class="hljs-number">1</span>]</span><br><span class="line">	b = start[<span class="hljs-number">2</span>]</span><br><span class="line">	x = dat[,<span class="hljs-number">1</span>]</span><br><span class="line">	y = dat[,<span class="hljs-number">2</span>]</span><br><span class="line">	iters = <span class="hljs-number">0</span></span><br><span class="line">	<span class="hljs-keyword">while</span>(<span class="hljs-literal">TRUE</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		Ypred = a + b * x</span><br><span class="line">		old_a = a</span><br><span class="line">		old_b = b</span><br><span class="line">		a = a + learning_rate * sum(y - Ypred)</span><br><span class="line">		b = b + learning_rate * sum((y-Ypred) * x)</span><br><span class="line">		iters = iters + <span class="hljs-number">1</span></span><br><span class="line">		<span class="hljs-keyword">if</span>(abs(a-old_a) &lt;= tol &amp; abs(b-old_b) &lt;= <span class="hljs-number">0.01</span>)</span><br><span class="line">			<span class="hljs-keyword">break</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	list(weights = c(a,b),iters = iters)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">gradientDescent(dat,tol=<span class="hljs-number">0.0000001</span>)</span><br><span class="line">$weights</span><br><span class="line">[<span class="hljs-number">1</span>] -<span class="hljs-number">0.1167315</span>  <span class="hljs-number">0.9776839</span></span><br><span class="line"></span><br><span class="line">$iters <span class="hljs-comment">#迭代了975次</span></span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">975</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>我们常说的梯度，其实是指向量，其方向与切线方向相同。</p>
<p>利用梯度下降法进行权重更新的公式为:</p>
<p>$$weight_{new} = weight_{old} - \eta \cdot \nabla$$</p>
<p>其中，那个倒三角形就是梯度的意思。我们在高中数学学过，切线方向是函数变化速度最快的方向，</p>
</blockquote>
<h2 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h2><p>梯度下降算法，又可以称为Batch-Gradient-Gescent,即批量梯度下降算法。从上面的例子可以看出，批量梯度下降算法，每次更新系数都需要所有的样本参与计算，当样本规模达到一定数量以后，这个更新速度会非常慢。另外，还有可能导致内存溢出。</p>
<p>为了克服批量梯度下降的这个缺点，有人提出了随机梯度下降(Stochastic Gradient Descent)算法，即每次更新系数只需要一个样本参与计算，因此既可以减少迭代次数，节省计算时间，又可以防止内存溢出。</p>
<p>对于上述问题，随机梯度下降的算法过程如下：</p>
<blockquote>
<p>for every $Y_i$:</p>
<p>$Ypred = a + bx$</p>
<p>$a = a + \eta (Y-Ypred)$</p>
<p>$b = b+\eta(Y-Ypred)\cdot x$</p>
</blockquote>
<p>随机梯度下降算法适用于大数量的计算，对于小数据量不一定准确。为了检验随机梯度下降算法，我们构造了一个有10000个样本的数据，同样是计算一元线性回归的系数。</p>
<p>随机梯度下降的函数如下：</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">stochasticGradientDescent &lt;- <span class="hljs-keyword">function</span>(dat,start = c(<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),learning_rate = <span class="hljs-number">0.01</span>,tol = <span class="hljs-number">0.000001</span>,iteratons = <span class="hljs-number">100</span>)</span><br><span class="line">&#123;</span><br><span class="line">	<span class="hljs-comment">#start:初始参数</span></span><br><span class="line">	<span class="hljs-comment">#learning_rate:学习率</span></span><br><span class="line">	<span class="hljs-comment">#tol:精度</span></span><br><span class="line">	<span class="hljs-comment">#iterations:迭代次数</span></span><br><span class="line">	</span><br><span class="line">	dat = as.matrix(dat)</span><br><span class="line">	a = start[<span class="hljs-number">1</span>]</span><br><span class="line">	b = start[<span class="hljs-number">2</span>]</span><br><span class="line">	x = dat[,<span class="hljs-number">1</span>]</span><br><span class="line">	y = dat[,<span class="hljs-number">2</span>]</span><br><span class="line">	iters = <span class="hljs-number">0</span></span><br><span class="line">	<span class="hljs-keyword">while</span>(iters &lt; iteratons)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="hljs-comment">#重排，即将样本的顺序打乱</span></span><br><span class="line">		index = sample(length(x))</span><br><span class="line">		old_a = a</span><br><span class="line">		old_b = b</span><br><span class="line">		<span class="hljs-keyword">for</span>(i <span class="hljs-keyword">in</span> index)</span><br><span class="line">		&#123;</span><br><span class="line">			</span><br><span class="line">			Ypred = a + b * x[i]</span><br><span class="line">			a = a + learning_rate * (y[i] - Ypred)</span><br><span class="line">			b = b + learning_rate * (y[i]-Ypred) * x[i]</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="hljs-keyword">if</span>(abs(a-old_a) &lt;= tol &amp; abs(b-old_b) &lt;= tol)</span><br><span class="line">		 	<span class="hljs-keyword">break</span>;</span><br><span class="line">		learning_rate = learning_rate / (<span class="hljs-number">1</span> + <span class="hljs-number">0.01</span> * iters) <span class="hljs-comment">#自适应学习率</span></span><br><span class="line">		iters = iters + <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">if</span>(iters &gt; iterations)</span><br><span class="line">            <span class="hljs-keyword">break</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	list(weights = c(a,b),iters = iters)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>然后我们构造一个相对大的样本用来检验算法：</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">set.seed(<span class="hljs-number">100</span>)</span><br><span class="line">x &lt;- seq(<span class="hljs-number">1</span>,<span class="hljs-number">10</span>,length.out = <span class="hljs-number">10000</span>)</span><br><span class="line">y &lt;- <span class="hljs-number">2</span> * x + rnorm(<span class="hljs-number">10000</span>) * <span class="hljs-number">10</span> + <span class="hljs-number">2</span></span><br><span class="line">bigdata &lt;- data.frame(x ,y )</span><br><span class="line">plot(x,y)</span><br><span class="line">cor(x,y)</span><br></pre></td></tr></table></figure>

<p><img src="/picture/sgd.png" alt="sgd"></p>
<p>回归结果：</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lm(y~x,data = bigdata )</span><br><span class="line">Call:</span><br><span class="line">lm(formula = y ~ x, data = bigdata)</span><br><span class="line"></span><br><span class="line">Coefficients:</span><br><span class="line">(Intercept)            x  </span><br><span class="line">      <span class="hljs-number">2.004</span>        <span class="hljs-number">2.006</span></span><br></pre></td></tr></table></figure>

<p>随机梯度下降的结果：</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">stochasticGradientDescent(bigdata,learning_rate = <span class="hljs-number">0.001</span>,tol=<span class="hljs-number">0.000000001</span>)</span><br><span class="line">$weights</span><br><span class="line"><span class="hljs-number">2.01138749995603</span> <span class="hljs-number">2.00584502615877</span></span><br><span class="line">$iters</span><br><span class="line"><span class="hljs-number">69</span></span><br></pre></td></tr></table></figure>

<p>批量梯度下降的结果：</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">batchGradientDescent(bigdata,learning_rate = <span class="hljs-number">0.000001</span>,tol = <span class="hljs-number">0.000000001</span>)</span><br><span class="line">$weights</span><br><span class="line"><span class="hljs-number">2.00385275101478</span> <span class="hljs-number">2.00634924457312</span></span><br><span class="line">$iters</span><br><span class="line"><span class="hljs-number">8345</span></span><br></pre></td></tr></table></figure>

<p>可以看到，在同样的精度要求下，随机梯度下降进行59次迭代以后即收敛，而批量梯度下降则需要迭代8345次。</p>
<p>但是随机梯度下降也有一个缺点，即参数更新频率太快，有可能出现目标函数值在最优质附近的震荡现象，因为高频率的参数更新导致了高方差。 同时也可以看出，在相同精度要求下，随机梯度下降计算出来的系数与精确值离差较大，而批量随机下降则更接近精确值。</p>
<h2 id="Mini-batch-Gradient-Descent"><a href="#Mini-batch-Gradient-Descent" class="headerlink" title="Mini-batch Gradient Descent"></a>Mini-batch Gradient Descent</h2><p>小批量梯度下降(Mini-batch Gradient Descent)是介于上述两种方法之间的优化方法，即在更新参数时，只使用一部分样本（一般256以下）来更新参数，这样既可以保证训练过程更稳定，又可以利用批量训练方法中的矩阵计算的优势。</p>
<p>具体更新哪些样本，通常是随机确定的，下面，我们定义一下小批量梯度下降的函数，用来求解上述bigdata的系数。</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">miniBatchGradientDescent &lt;- <span class="hljs-keyword">function</span>(dat,start = c(<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),learning_rate = <span class="hljs-number">0.01</span>,tol = <span class="hljs-number">0.001</span>,batchSize = <span class="hljs-number">256</span>,iterations = <span class="hljs-number">10000</span>)</span><br><span class="line">&#123;</span><br><span class="line">    a = start[<span class="hljs-number">1</span>]</span><br><span class="line">    b = start[<span class="hljs-number">2</span>]</span><br><span class="line">    iters = <span class="hljs-number">0</span></span><br><span class="line">    len = length(y)</span><br><span class="line">    <span class="hljs-keyword">while</span>(<span class="hljs-literal">TRUE</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        mini_index = sample(len,batchSize,replace = <span class="hljs-literal">FALSE</span>)</span><br><span class="line">        x = dat[mini_index,<span class="hljs-number">1</span>]</span><br><span class="line">        y = dat[mini_index,<span class="hljs-number">2</span>]</span><br><span class="line">        Ypred = a + b * x</span><br><span class="line">        error = y - Ypred</span><br><span class="line">        old_a = a</span><br><span class="line">        old_b = b</span><br><span class="line">        a = a + learning_rate * sum(error)</span><br><span class="line">        b = b + learning_rate * sum((error) * x)</span><br><span class="line">        start = rbind(start,c(a,b))</span><br><span class="line">        iters = iters + <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">if</span>(abs(a-old_a) &lt;= tol &amp; abs(b-old_b) &lt;= tol)</span><br><span class="line">            <span class="hljs-keyword">break</span></span><br><span class="line">        <span class="hljs-keyword">if</span>(iters &gt;= iterations)</span><br><span class="line">            <span class="hljs-keyword">break</span></span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    list(weights = c(a,b),iters = iters,coes = start)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">miniBatchGradientDescent(bigdata,learning_rate = <span class="hljs-number">0.0001</span>,tol = <span class="hljs-number">0.00001</span>,batchSize = <span class="hljs-number">100</span>)</span><br><span class="line">$weights</span><br><span class="line"><span class="hljs-number">2.02646349019186</span> <span class="hljs-number">2.0439693915315</span></span><br><span class="line">$iters</span><br><span class="line"><span class="hljs-number">920064</span></span><br></pre></td></tr></table></figure>

<p>小梯度批量梯度下降收敛时需要迭代92万次，这显然有点多了。一般来说，当数据量非常大时，小批量梯度下降比较有效，否则计算结果很有可能出现偏移。</p>
<blockquote>
<p>先mark，偏移的原因待考究。</p>
</blockquote>
<h2 id="Momentum-optimization"><a href="#Momentum-optimization" class="headerlink" title="Momentum optimization"></a>Momentum optimization</h2><p>考虑这样一种情形，小球从山顶往下滚动，一开始很顺利，可是在接近最低点的时候，小球陷入了一段狭长的浅山谷。由于小球一开始并不是直接沿着山谷的方向滚下，因此小球会在这个浅浅的山谷中不断震荡——不断冲上墙壁，接着又从墙壁上滚下来。这种情况并不是我们想看到的，因为这增加了迭代时间。冲量(Momentnum)的引入，使得我们的目标更新的更快了，冲量的更新方式有以下两种，两种方式之间并无太大差异。</p>
<blockquote>
<p>第一种：</p>
<p>$Z^{k+1}=\beta Z_k + \nabla$</p>
<p>$w^{k+1} = w_k - \alpha Z^{k+1}$</p>
<p>其中，$Z$是一个与$w$方向相同的向量，</p>
</blockquote>
<blockquote>
<p>第二种：</p>
<p>$Z^{k+1}=\beta Z^k + \alpha \nabla$</p>
<p>$w^{k+1} = w^k - Z^{k+1}$</p>
</blockquote>
<p><img src="/picture/1525839842032.png" alt="1525839842032"></p>
<p>两者的差别仅仅在于$Z^{k+1}$的系数不同。</p>
<p>通常，这里的学习率要比随机梯度下降小一点，因为随机梯度下降的梯度大一点。$\beta$的取值决定了前一次的梯度有多少被纳入了本次的更新。一般来说，稳定前将$\beta$设置为0.5，稳定后可以设置为0.9或更高。</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#适用于求解一元或多元线性回归的回归系数，返回结果包括截距</span></span><br><span class="line">momentumGradientDescent &lt;- <span class="hljs-keyword">function</span>(dat,beta = <span class="hljs-number">0.9</span>,z = <span class="hljs-number">0</span>,start = rep(<span class="hljs-number">0</span>,dim(dat)[<span class="hljs-number">2</span>]),alpha = <span class="hljs-number">0.001</span>,tol=<span class="hljs-number">0.0000001</span>,iterations = <span class="hljs-number">100</span>)</span><br><span class="line">&#123;</span><br><span class="line">    dataSet = cbind(<span class="hljs-number">1</span>,dat)  <span class="hljs-comment">#将第一列加上1</span></span><br><span class="line">    cols = dim(dataSet)[<span class="hljs-number">2</span>] <span class="hljs-comment">#列数</span></span><br><span class="line">    x = as.matrix(dataSet[,<span class="hljs-number">1</span>:(cols - <span class="hljs-number">1</span>)])  <span class="hljs-comment">#自变量矩阵</span></span><br><span class="line">    y = as.matrix(dataSet[,cols],ncol = <span class="hljs-number">1</span>)  <span class="hljs-comment">#因变量，矩阵</span></span><br><span class="line">    w = as.matrix(start,ncol = <span class="hljs-number">1</span>)  <span class="hljs-comment">#权重矩阵</span></span><br><span class="line">    iters = <span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">while</span>(<span class="hljs-literal">TRUE</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        old_w = w</span><br><span class="line">        old_z = z</span><br><span class="line">        Ypred = x %*% w</span><br><span class="line">        error = y - Ypred</span><br><span class="line">        grad = - t(x) %*% error</span><br><span class="line">        z = beta * old_z + grad</span><br><span class="line">        w = old_w - alpha * z</span><br><span class="line">        <span class="hljs-keyword">if</span>(sum(abs(w-old_w)) &lt; tol)</span><br><span class="line">            <span class="hljs-keyword">break</span>;</span><br><span class="line">        iters = iters + <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">if</span>(is.integer(iterations))</span><br><span class="line">            <span class="hljs-keyword">if</span>(iters &gt;= iterations)</span><br><span class="line">                <span class="hljs-keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    list(weights = as.vector(w),iters = iters)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>利用冲量梯度下降求bigdata的系数：</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">momentumGradientDescent(bigdata,alpha=<span class="hljs-number">0.00001</span>)</span><br><span class="line">$weights</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">2.003851</span> <span class="hljs-number">2.006354</span></span><br><span class="line"></span><br><span class="line">$iters</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">248</span></span><br></pre></td></tr></table></figure>

<p>可以看到，迭代次数明显减少，并且系数与精确值更加接近了。</p>
<h2 id="Nesterov-Accelerated-Gradient"><a href="#Nesterov-Accelerated-Gradient" class="headerlink" title="Nesterov Accelerated Gradient"></a>Nesterov Accelerated Gradient</h2><p>然而，让一个小球盲目地沿着斜坡滚下山是不理想的。我们需要一个更聪明的球，它知道下一步要往哪里去，因此在斜坡有上升的时候，它能够自主调整方向。</p>
<p>Nesterov Accelerated Gradient 是基于冲量梯度下降算法进行改进的一种算法，也是梯度下降算法的变种。</p>
<p>在上一种算法中，我们使用了冲量$\beta Z^k$来调整我们的参数改变量，将上述第二种更新方法改写一下，得到如下式子：</p>
<blockquote>
<p>$Z^{k+1} = \beta Z^k + \alpha \nabla$</p>
<p>$w^{k+1} = w^k - Z^{k+1} = w^k - \beta Z_k - \alpha \nabla $</p>
</blockquote>
<p>$w^k - \beta Z_k $给出了下一个参数位置的近似，指明了参数该如何变化。现在，我们可以不用当前的参数，而是用未来的参数的近似位置来更新我们的参数，即</p>
<blockquote>
<p>$Z^{k+1} = \beta Z^k + \alpha \nabla _wJ(w - \beta Z^k)$</p>
<p>$w^{k+1} = w^k - Z^{k+1}$</p>
</blockquote>
<p>这里，我们仍然设置$\beta$的值在0.9附近。如下图所示，冲量梯度下降先计算当前的梯度（短的蓝色向量），然后根据累积的冲量向前跨越一大步（长的蓝色向量）。NAG首先根据之前的累积梯度向前迈一大步（棕色向量），然后对梯度进行修正（红色向量）。这种利用近似未来参数来更新参数的方法，可以防止梯度更新太快，并且增加了响应能力。</p>
<p><img src="/picture/nesterov_update_vector.png" alt="nesterov_update_vector"> </p>
<blockquote>
<p>假设我们的权重矩阵（系数矩阵）为$w$，自变量$x$，因变量$Y$，则损失函数为：</p>
<p>$$J(w) = 1/2 \sum _{i=1} ^n (Y_i - w’x_i)^2$$</p>
<p>则</p>
<p>$$\nabla J(w) = \frac{\partial J(w)}{\partial w} = - \sum_{i=1}^n(Y_i - w’x_i)x_i’$$</p>
<p>那么</p>
<p>$$\nabla J(w - \beta Z^{k}) = - \sum _{i=1}^n(Y_i - (w’-\beta Z^k)’x_i)x_i’$$</p>
</blockquote>
<p>根据上述思想，编写的NAG代码如下 ：</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#适用于求解一元或多元线性回归的回归系数，返回结果包括截距</span></span><br><span class="line">NAG &lt;- <span class="hljs-keyword">function</span>(dat,beta = <span class="hljs-number">0.9</span>,z = <span class="hljs-number">0</span>,start = rep(<span class="hljs-number">0</span>,dim(dat)[<span class="hljs-number">2</span>]),alpha = <span class="hljs-number">0.001</span>,tol=<span class="hljs-number">0.0000001</span>,iterations = <span class="hljs-number">100</span>)</span><br><span class="line">&#123;</span><br><span class="line">    dataSet = cbind(<span class="hljs-number">1</span>,dat)  <span class="hljs-comment">#将第一列加上1</span></span><br><span class="line">    cols = dim(dataSet)[<span class="hljs-number">2</span>] <span class="hljs-comment">#列数</span></span><br><span class="line">    x = as.matrix(dataSet[,<span class="hljs-number">1</span>:(cols - <span class="hljs-number">1</span>)])  <span class="hljs-comment">#自变量矩阵</span></span><br><span class="line">    y = dataSet[,cols]  <span class="hljs-comment">#因变量，矩阵</span></span><br><span class="line">    w = start  <span class="hljs-comment">#权重矩阵</span></span><br><span class="line">    iters = <span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">while</span>(<span class="hljs-literal">TRUE</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        old_w = w</span><br><span class="line">        old_z = z</span><br><span class="line">        <span class="hljs-comment">#中间过程用mid1 mid2代替</span></span><br><span class="line">        mid1 = apply((old_w - beta * old_z) * t(x),<span class="hljs-number">2</span>,sum)</span><br><span class="line">        mid2 = y - mid1</span><br><span class="line">        grad = - apply(mid2 * x,<span class="hljs-number">2</span>,sum)</span><br><span class="line">        z = beta * old_z + alpha * grad</span><br><span class="line">        w = old_w - z</span><br><span class="line">        <span class="hljs-keyword">if</span>(sum(abs(w-old_w)) &lt; tol)</span><br><span class="line">            <span class="hljs-keyword">break</span>;</span><br><span class="line">        iters = iters + <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">if</span>(is.integer(iterations))</span><br><span class="line">            <span class="hljs-keyword">if</span>(iters &gt;= iterations)</span><br><span class="line">                <span class="hljs-keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    list(weights = as.vector(w),iters = iters)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>用NAG来求bigdata的系数：</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">NAG(bigdata,alpha=<span class="hljs-number">0.000001</span>)</span><br><span class="line">$weights</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">2.003849</span> <span class="hljs-number">2.006350</span></span><br><span class="line"></span><br><span class="line">$iters</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">598</span></span><br></pre></td></tr></table></figure>

<h2 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h2><p>尽管我们可以根据损失函数的梯度来加快更新参数，我们也希望能够根据参数的重要性来决定其更新的幅度。</p>
<p>AdaGrad是一种基于梯度算法的优化算法,它只做了一件事:根据参数来自适应调整学习率。对于不常出现的参数进行较大的更新，对于经常出现的参数进行较少的更新，因此，这种方法非常适合处理稀疏数据。</p>
<p>之前，我们对每一个参数更新所使用的学习率都是一样的，而AdaGrad在每一步都使用不同的学习率对不同的参数进行更新。我们先写出AdaGrad的单个参数的更新方法，然后将其向量化。长话短说，假设$g_{t,i}$表示损失函数对于参数$\theta_i$的梯度：</p>
<p>在步骤$t$:</p>
<p>$$g_{t,i} = \nabla _\theta J(\theta_t,i)$$</p>
<p>那么，对于步骤$t$，使用随机梯度下将对$\theta_i$进行更新的公式为：</p>
<p>$$\theta_{t+1,i} = \theta_{t,i} - \eta_i \cdot g_{t,i}$$</p>
<p>在上述更新过程中，AdaGrad在每一步都对参数$\theta_i$对应的学习率$\eta_i$进行调整，调整的方法基于过去的所有梯度：</p>
<p>$$\theta_{t+1,i} = \theta_{t,i} - \frac{\eta_i}{\sqrt{G_{t,ii} + \epsilon}} \cdot g_{t,i}$$</p>
<p>$G_{t}\in R^{d\times d}$是一个对角矩阵，第$i$个对角元素是历史上损失函数对$\theta_i$的所有梯度的平方和，$\epsilon$是一个平滑参数，防止分母为0，通常取$10^{-8}$。有趣的是，如果不加开方，算法表现极差。</p>
<p>因为$G_t$包含了过去所有参数梯度的平方和，因此我们可以将其向量化：</p>
<p>$$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}}\odot g_t$$</p>
<p>AdaGrad的一个最大的好处是不用手动调整学习率的大小，通常设置默认值为0.01，然后顺其自然就好了。</p>
<p>AdaGrad的一个较大的缺点是，分母是不断增大的，当迭代次数不断增加时，分母会逐渐趋于无穷大，学习率进而趋于无穷小，此时，算法将变得不再有效。</p>
<p>我们延用上述符号来写出AdaGrad的函数。</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#SGD为基础</span></span><br><span class="line">AdaGrad &lt;- <span class="hljs-keyword">function</span>(dat,theta,learning_rate = <span class="hljs-number">0.01</span>,start = rep(<span class="hljs-number">0</span>,dim(dat)[<span class="hljs-number">2</span>]),tol = <span class="hljs-number">0.000001</span>,iterations = <span class="hljs-number">1000</span>)</span><br><span class="line">&#123;</span><br><span class="line">    dataSet = cbind(<span class="hljs-number">1</span>,dat) <span class="hljs-comment">#将自变量进行增广，第一列全为1</span></span><br><span class="line">    cols = dim(dataSet)[<span class="hljs-number">2</span>] <span class="hljs-comment">#列数</span></span><br><span class="line">    rows = dim(dataSet)[<span class="hljs-number">1</span>] <span class="hljs-comment">#行数</span></span><br><span class="line">    x = as.matrix(dataSet[,<span class="hljs-number">1</span>:(cols - <span class="hljs-number">1</span>)])  <span class="hljs-comment">#自变量矩阵</span></span><br><span class="line">    y = dataSet[,cols]  <span class="hljs-comment">#因变量，矩阵</span></span><br><span class="line">    g = <span class="hljs-number">0</span></span><br><span class="line">    theta = start  <span class="hljs-comment">#权重矩阵</span></span><br><span class="line">    iters = <span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">while</span>(<span class="hljs-literal">TRUE</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        old_theta = theta</span><br><span class="line">        index = sample(rows)</span><br><span class="line">        <span class="hljs-keyword">for</span>(i <span class="hljs-keyword">in</span> index)</span><br><span class="line">        &#123;</span><br><span class="line">            grad = - (y[i] - sum(theta * x[i,])) * x[i,]</span><br><span class="line">            g = g + grad * grad</span><br><span class="line">            theta = theta - learning_rate / sqrt(g + <span class="hljs-number">1e-8</span>) * grad</span><br><span class="line">        &#125;</span><br><span class="line">        iters = iters + <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">if</span>(is.integer(iterations))</span><br><span class="line">            <span class="hljs-keyword">if</span>(iters &gt; iterations)</span><br><span class="line">                <span class="hljs-keyword">break</span>;</span><br><span class="line">        <span class="hljs-keyword">if</span>(sum(abs(old_theta - theta)) &lt; tol)</span><br><span class="line">            <span class="hljs-keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    list(weights = as.vector(theta),iters = iters)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>AdaGrad的计算结果如下：</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">AdaGrad(bigdata)</span><br><span class="line">$weights</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">2.008219</span> <span class="hljs-number">2.005682</span></span><br><span class="line"></span><br><span class="line">$iters</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">6579</span></span><br></pre></td></tr></table></figure>

<h2 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h2><p>AdaDelta是在AdaGrad的基础上发展而来的，目的是解决AdaGrad算法中学习率的单调减少问题。AdaDelta不再采用累积梯度平方和的方法来调整学习率，而是根据一些固定的$w$的大小来限制过去累积梯度的窗口。</p>
<p>AdaDelta不再无效率地存储历史梯度的平方和，而将历史梯度平方和定义为衰减均值(decaying average)。第$t$步的移动平均值$E[g^2]_t$仅仅取决于过去的平均值和当前梯度（有点类似于momentum）：</p>
<p>$$E[g^2]<em>t = \gamma E[g^2]</em>{t-1} + (1-\gamma)g_t^2$$</p>
<p>同样的，我们把$\gamma$设置在0.9附近。为清楚起见，我们重新写下更新规则：</p>
<blockquote>
<p>$$\Delta \theta <em>t = - \eta \cdot g</em>{t,i}$$</p>
<p>$$\theta _{t+1} = \theta_t + \Delta \theta _t$$</p>
</blockquote>
<p>根据AdaGrad我们推出AdaDelta的参数更新公式：</p>
<blockquote>
<p>$$\Delta \theta _t = - \frac{\eta}{\sqrt{G_t + \epsilon}} \odot g_t$$</p>
</blockquote>
<p>我们只需把$G_t$替换$E[g^2]_t$就行了：</p>
<blockquote>
<p>$$\theta_t = - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}}$$</p>
</blockquote>
<p>因为分母刚好是梯度的均方根，我们将其简写为：</p>
<blockquote>
<p>$$\Delta \theta_t = -\frac{\eta}{RMS[g]_t}g_t$$</p>
</blockquote>
<p>作者们发现上述更新中的单位不一致（可以理解为量纲不一致），因此，他们定义了另外一个指数衰减均值，这次不用梯度的平方了，而是用参数的平方来进行更新：</p>
<p>$$E[\Delta \theta^2]<em>t = \gamma E[\Delta \theta^2]</em>{t-1} + (1-\gamma)\Delta\theta_t^2$$</p>
<p>参数更新的均方误差即：</p>
<p>$$RMS[\Delta\theta]_t = \sqrt{E[\Delta\theta^2]_t + \epsilon}$$</p>
<p>因为$RMS[\theta]<em>t$是未知的，我们可以用之前所有的更新过的参数的RMS来代替。将之前的学习率$\eta$换成$RMS[\Delta\theta]</em>{t-1}$，那么，我们得出AdaDelta的更新规则：</p>
<p>$$\Delta\theta_t = -\frac{RMS[\Delta\theta]_{t-1}}{RMS[g]_t}g_t$$</p>
<p>$$\theta_{t+1} = \theta_t + \Delta\theta_t$$</p>
<p>AdaDelta甚至不需要初始化学习率，因为在更新规则中已经不见它的身影了。</p>
<p>根据上述思路，我们写出AdaDelta的函数:</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">RMS &lt;- <span class="hljs-keyword">function</span>(x)</span><br><span class="line">&#123;</span><br><span class="line">    sqrt(mean(x^<span class="hljs-number">2</span>) - mean(x)^<span class="hljs-number">2</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">AdaDelta &lt;- <span class="hljs-keyword">function</span>()</span><br></pre></td></tr></table></figure>

<h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p><strong>上述所有改进方法均可以运用于批量梯度下降、小批量梯度下降和随机梯度下降！</strong></p>
<p>[1]<a href="http://netedu.xauat.edu.cn/jpkc/netedu/jpkc/gdsx/homepage/5jxsd/51/513/5308/530807.htm" target="_blank" rel="noopener">http://netedu.xauat.edu.cn/jpkc/netedu/jpkc/gdsx/homepage/5jxsd/51/513/5308/530807.htm</a></p>
<p>[2]<a href="https://www.kdnuggets.com/2017/04/simple-understand-gradient-descent-algorithm.html" target="_blank" rel="noopener">https://www.kdnuggets.com/2017/04/simple-understand-gradient-descent-algorithm.html</a></p>
<p>[3]<a href="https://distill.pub/2017/momentum/" target="_blank" rel="noopener">https://distill.pub/2017/momentum/</a></p>
<p>[4]<a href="http://ruder.io/optimizing-gradient-descent/index.html" target="_blank" rel="noopener">http://ruder.io/optimizing-gradient-descent/index.html</a></p>

        </div>
        
        <div class="level is-size-7 is-uppercase">
            <div class="level-start">
                <div class="level-item">
                    <span class="is-size-6 has-text-grey has-mr-7">#</span>
                    <a class="has-link-grey -link" href="/tags/优化算法/">优化算法</a>, <a class="has-link-grey -link" href="/tags/机器学习/">机器学习</a>, <a class="has-link-grey -link" href="/tags/梯度下降/">梯度下降</a>
                </div>
            </div>
        </div>
        
        
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3>
        <div class="buttons is-centered">
            
                
<div class="notification is-danger">
    You forgot to set the <code>qrcode</code> for Alipay. Please set it in <code>_config.yml</code>.
</div>

                
                
<div class="notification is-danger">
    You forgot to set the <code>qrcode</code> for Wechat. Please set it in <code>_config.yml</code>.
</div>

                
                <!-- Visit https://www.paypal.com/donate/buttons/ to get your donate button -->

<div class="notification is-danger">
    You forgot to set the <code>business</code> and <code>currency_code</code> for Paypal. Please set it in <code>_config.yml</code>.
</div>

                
                
<div class="notification is-danger">
    You forgot to set the <code>url</code> Patreon. Please set it in <code>_config.yml</code>.
</div>

                
        </div>
    </div>
</div>



<div class="card card-transparent">
    <div class="level post-navigation is-flex-wrap is-mobile">
        
        <div class="level-start">
            <a class="level level-item has-link-grey  article-nav-prev" href="/2018/05/09/排序算法/">
                <i class="level-item fas fa-chevron-left"></i>
                <span class="level-item">排序算法</span>
            </a>
        </div>
        
        
        <div class="level-end">
            <a class="level level-item has-link-grey  article-nav-next" href="/2018/05/04/牛顿法/">
                <span class="level-item">平方根与牛顿法</span>
                <i class="level-item fas fa-chevron-right"></i>
            </a>
        </div>
        
    </div>
</div>


</div>
                




<div class="column is-4-tablet is-4-desktop is-3-widescreen  has-order-1 column-left ">
    
        
<div class="card widget">
    <div class="card-content">
        <nav class="level">
            <div class="level-item has-text-centered">
                <div>
                    
                        <img class="image is-128x128 has-mb-6" src="/images/avatar.png" alt="Your name">
                    
                    
                    <p class="is-size-4 is-block">
                        Your name
                    </p>
                    
                    
                    <p class="is-size-6 is-block">
                        Your title
                    </p>
                    
                    
                    <p class="is-size-6 is-flex is-flex-center has-text-grey">
                        <i class="fas fa-map-marker-alt has-mr-7"></i>
                        <span>Your location</span>
                    </p>
                    
                </div>
            </div>
        </nav>
        <nav class="level is-mobile">
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        文章
                    </p>
                    <p class="title has-text-weight-normal">
                        25
                    </p>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        分类
                    </p>
                    <p class="title has-text-weight-normal">
                        2
                    </p>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        标签
                    </p>
                    <p class="title has-text-weight-normal">
                        22
                    </p>
                </div>
            </div>
        </nav>
        <div class="level">
            <a class="level-item button is-link is-rounded" href="https://github.com/ppoffice" target="_blank">
                关注我</a>
        </div>
        
        
        <div class="level is-mobile">
            
            <a class="level-item button is-white is-marginless" target="_blank"
                title="Github" href="https://github.com/ppoffice">
                
                <i class="fab fa-github"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank"
                title="Facebook" href="https://facebook.com">
                
                <i class="fab fa-facebook"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank"
                title="Twitter" href="https://twitter.com">
                
                <i class="fab fa-twitter"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank"
                title="Dribbble" href="https://dribbble.com">
                
                <i class="fab fa-dribbble"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank"
                title="RSS" href="/">
                
                <i class="fas fa-rss"></i>
                
            </a>
            
        </div>
        
    </div>
</div>
    
        
    
        

<div class="card widget">
    <div class="card-content">
        <div class="menu">
        <h3 class="menu-label">
            链接
        </h3>
        <ul class="menu-list">
        
            <li>
                <a class="level is-mobile" href="https://hexo.io" target="_blank">
                    <span class="level-left">
                        <span class="level-item">Hexo</span>
                    </span>
                    <span class="level-right">
                        <span class="level-item tag">hexo.io</span>
                    </span>
                </a>
            </li>
        
            <li>
                <a class="level is-mobile" href="https://github.com/ppoffice" target="_blank">
                    <span class="level-left">
                        <span class="level-item">PPOffice</span>
                    </span>
                    <span class="level-right">
                        <span class="level-item tag">github.com</span>
                    </span>
                </a>
            </li>
        
        </ul>
        </div>
    </div>
</div>


    
        
<div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                分类
            </h3>
            <ul class="menu-list">
            <li>
        <a class="level is-marginless" href="/categories/NLP/">
            <span class="level-start">
                <span class="level-item">NLP</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/大数据/">
            <span class="level-start">
                <span class="level-item">大数据</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li>
            </ul>
        </div>
    </div>
</div>
    
        
<div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            标签云
        </h3>
        <a href="/tags/ETL/" style="font-size: 10px;">ETL</a> <a href="/tags/Lubridate/" style="font-size: 10px;">Lubridate</a> <a href="/tags/MapReduce/" style="font-size: 10px;">MapReduce</a> <a href="/tags/R/" style="font-size: 10px;">R</a> <a href="/tags/clustering/" style="font-size: 10px;">clustering</a> <a href="/tags/kmeans/" style="font-size: 10px;">kmeans</a> <a href="/tags/优化算法/" style="font-size: 10px;">优化算法</a> <a href="/tags/岭回归/" style="font-size: 10px;">岭回归</a> <a href="/tags/感知机/" style="font-size: 10px;">感知机</a> <a href="/tags/排序/" style="font-size: 10px;">排序</a> <a href="/tags/数据仓库/" style="font-size: 10px;">数据仓库</a> <a href="/tags/数据结构/" style="font-size: 10px;">数据结构</a> <a href="/tags/机器学习/" style="font-size: 20px;">机器学习</a> <a href="/tags/梯度下降/" style="font-size: 10px;">梯度下降</a> <a href="/tags/深度学习/" style="font-size: 10px;">深度学习</a> <a href="/tags/爬虫/" style="font-size: 16.67px;">爬虫</a> <a href="/tags/矩阵分解/" style="font-size: 13.33px;">矩阵分解</a> <a href="/tags/神经网络/" style="font-size: 10px;">神经网络</a> <a href="/tags/线性回归/" style="font-size: 10px;">线性回归</a> <a href="/tags/自然语言处理/" style="font-size: 10px;">自然语言处理</a> <a href="/tags/逐步回归/" style="font-size: 10px;">逐步回归</a> <a href="/tags/降维/" style="font-size: 10px;">降维</a>
    </div>
</div>

    
    
        <div class="column-right-shadow is-hidden-widescreen ">
        
            
<div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            最新文章
        </h3>
        
        <article class="media">
            
            <a href="/2018/07/02/R语言简明教程/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="R语言简明教程">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-07-02T03:43:55.696Z">2018-07-02</time></div>
                    <a href="/2018/07/02/R语言简明教程/" class="has-link-black-ter is-size-6">R语言简明教程</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2018/05/09/排序算法/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="排序算法">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-05-08T16:00:00.000Z">2018-05-09</time></div>
                    <a href="/2018/05/09/排序算法/" class="has-link-black-ter is-size-6">排序算法</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2018/05/05/梯度下降/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="线性回归与梯度下降算法">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-05-04T16:00:00.000Z">2018-05-05</time></div>
                    <a href="/2018/05/05/梯度下降/" class="has-link-black-ter is-size-6">线性回归与梯度下降算法</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2018/05/04/牛顿法/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="平方根与牛顿法">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-05-04T02:50:15.447Z">2018-05-04</time></div>
                    <a href="/2018/05/04/牛顿法/" class="has-link-black-ter is-size-6">平方根与牛顿法</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2018/05/03/逻辑回归/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="逻辑回归">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-05-02T16:00:00.000Z">2018-05-03</time></div>
                    <a href="/2018/05/03/逻辑回归/" class="has-link-black-ter is-size-6">逻辑回归</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
    </div>
</div>

        
            <div class="card widget">
    <div class="card-content">
        <div class="menu">
        <h3 class="menu-label">
            归档
        </h3>
        <ul class="menu-list">
        
        <li>
            <a class="level is-marginless" href="/archives/2018/07/">
                <span class="level-start">
                    <span class="level-item">七月 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2018/05/">
                <span class="level-start">
                    <span class="level-item">五月 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">5</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2018/04/">
                <span class="level-start">
                    <span class="level-item">四月 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">16</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2018/03/">
                <span class="level-start">
                    <span class="level-item">三月 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">3</span>
                </span>
            </a>
        </li>
        
        </ul>
        </div>
    </div>
</div>
        
            <div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                标签
            </h3>
            <div class="field is-grouped is-grouped-multiline">
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/ETL/">
                        <span class="tag">ETL</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Lubridate/">
                        <span class="tag">Lubridate</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/MapReduce/">
                        <span class="tag">MapReduce</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/R/">
                        <span class="tag">R</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/clustering/">
                        <span class="tag">clustering</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/kmeans/">
                        <span class="tag">kmeans</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/优化算法/">
                        <span class="tag">优化算法</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/岭回归/">
                        <span class="tag">岭回归</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/感知机/">
                        <span class="tag">感知机</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/排序/">
                        <span class="tag">排序</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/数据仓库/">
                        <span class="tag">数据仓库</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/数据结构/">
                        <span class="tag">数据结构</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/机器学习/">
                        <span class="tag">机器学习</span>
                        <span class="tag is-grey">9</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/梯度下降/">
                        <span class="tag">梯度下降</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/深度学习/">
                        <span class="tag">深度学习</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/爬虫/">
                        <span class="tag">爬虫</span>
                        <span class="tag is-grey">4</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/矩阵分解/">
                        <span class="tag">矩阵分解</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/神经网络/">
                        <span class="tag">神经网络</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/线性回归/">
                        <span class="tag">线性回归</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/自然语言处理/">
                        <span class="tag">自然语言处理</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/逐步回归/">
                        <span class="tag">逐步回归</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/降维/">
                        <span class="tag">降维</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
            </div>
        </div>
    </div>
</div>
        
        </div>
    
</div>

                




<div class="column is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only has-order-3 column-right ">
    
        
<div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            最新文章
        </h3>
        
        <article class="media">
            
            <a href="/2018/07/02/R语言简明教程/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="R语言简明教程">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-07-02T03:43:55.696Z">2018-07-02</time></div>
                    <a href="/2018/07/02/R语言简明教程/" class="has-link-black-ter is-size-6">R语言简明教程</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2018/05/09/排序算法/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="排序算法">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-05-08T16:00:00.000Z">2018-05-09</time></div>
                    <a href="/2018/05/09/排序算法/" class="has-link-black-ter is-size-6">排序算法</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2018/05/05/梯度下降/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="线性回归与梯度下降算法">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-05-04T16:00:00.000Z">2018-05-05</time></div>
                    <a href="/2018/05/05/梯度下降/" class="has-link-black-ter is-size-6">线性回归与梯度下降算法</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2018/05/04/牛顿法/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="平方根与牛顿法">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-05-04T02:50:15.447Z">2018-05-04</time></div>
                    <a href="/2018/05/04/牛顿法/" class="has-link-black-ter is-size-6">平方根与牛顿法</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2018/05/03/逻辑回归/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="逻辑回归">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-05-02T16:00:00.000Z">2018-05-03</time></div>
                    <a href="/2018/05/03/逻辑回归/" class="has-link-black-ter is-size-6">逻辑回归</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
    </div>
</div>

    
        <div class="card widget">
    <div class="card-content">
        <div class="menu">
        <h3 class="menu-label">
            归档
        </h3>
        <ul class="menu-list">
        
        <li>
            <a class="level is-marginless" href="/archives/2018/07/">
                <span class="level-start">
                    <span class="level-item">七月 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2018/05/">
                <span class="level-start">
                    <span class="level-item">五月 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">5</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2018/04/">
                <span class="level-start">
                    <span class="level-item">四月 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">16</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2018/03/">
                <span class="level-start">
                    <span class="level-item">三月 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">3</span>
                </span>
            </a>
        </li>
        
        </ul>
        </div>
    </div>
</div>
    
        <div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                标签
            </h3>
            <div class="field is-grouped is-grouped-multiline">
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/ETL/">
                        <span class="tag">ETL</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Lubridate/">
                        <span class="tag">Lubridate</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/MapReduce/">
                        <span class="tag">MapReduce</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/R/">
                        <span class="tag">R</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/clustering/">
                        <span class="tag">clustering</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/kmeans/">
                        <span class="tag">kmeans</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/优化算法/">
                        <span class="tag">优化算法</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/岭回归/">
                        <span class="tag">岭回归</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/感知机/">
                        <span class="tag">感知机</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/排序/">
                        <span class="tag">排序</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/数据仓库/">
                        <span class="tag">数据仓库</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/数据结构/">
                        <span class="tag">数据结构</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/机器学习/">
                        <span class="tag">机器学习</span>
                        <span class="tag is-grey">9</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/梯度下降/">
                        <span class="tag">梯度下降</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/深度学习/">
                        <span class="tag">深度学习</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/爬虫/">
                        <span class="tag">爬虫</span>
                        <span class="tag is-grey">4</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/矩阵分解/">
                        <span class="tag">矩阵分解</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/神经网络/">
                        <span class="tag">神经网络</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/线性回归/">
                        <span class="tag">线性回归</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/自然语言处理/">
                        <span class="tag">自然语言处理</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/逐步回归/">
                        <span class="tag">逐步回归</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/降维/">
                        <span class="tag">降维</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
            </div>
        </div>
    </div>
</div>
    
    
</div>

            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/">
                
                    <img src="/images/logo.svg" alt="线性回归与梯度下降算法" height="28">
                
                </a>
                <p class="is-size-7">
                &copy; 2019 冯洋洋&nbsp;
                Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> & <a
                        href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a>
                
                </p>
            </div>
            <div class="level-end">
            
                <div class="field has-addons is-flex-center-mobile has-mt-5-mobile is-flex-wrap is-flex-middle">
                
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" title="Creative Commons" href="https://creativecommons.org/">
                        
                        <i class="fab fa-creative-commons"></i>
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/">
                        
                        <i class="fab fa-creative-commons-by"></i>
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus">
                        
                        <i class="fab fa-github"></i>
                        
                    </a>
                </p>
                
                </div>
            
            </div>
        </div>
    </div>
</footer>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("zh-CN");</script>


    
    
    
    <script src="/js/animation.js"></script>
    

    
    
    
    <script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
    <script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
    <script src="/js/gallery.js" defer></script>
    

    
    

<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>


    
    
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        'HTML-CSS': {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
});
</script>

    
    

<a id="back-to-top" title="回到顶端" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/js/back-to-top.js" defer></script>


    
    

    
    
    
    

    
    
    
    
    
    <script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>
    <script src="/js/clipboard.js" defer></script>
    

    
    
    


<script src="/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="想要查找什么..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: '文章',
                PAGES: '页面',
                CATEGORIES: '分类',
                TAGS: '标签',
                UNTITLED: '(无标题)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js" defer></script>
<link rel="stylesheet" href="/css/search.css">
<link rel="stylesheet" href="/css/insight.css">
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>