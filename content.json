{"pages":[{"title":"about","text":"","link":"/about/index-1.html"},{"title":"关于我","text":"人生，十之八九不如意。 如有问题，欢迎邮件或者留言！","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"Tagcloud","text":"","link":"/tags/index.html"}],"posts":[{"title":"Introduction to ETL","text":"","link":"/2018/03/30/Introduction-to-ETL/"},{"title":"R语言简明轿车","text":"R语言介绍数据结构","link":"/2018/07/02/R语言简明教程/"},{"title":"hello world","text":"","link":"/2019/05/30/hello-world/"},{"title":"新浪新闻爬虫","text":"获取标题列表新浪新闻总是不断更新的，因此有可能每次查询的返回新闻数量都是不一样的，所以我们最好不要通过设置固定的条数取爬取。 从上图可以看出，只要有“下一页”存在，我们就可以从下一页获取链接，从而开始下一页的爬取。 import requests import numpy as np from bs4 import BeautifulSoup import re import time import json #第一页的URL startURL = 'http://search.sina.com.cn/?c=news&q=%D0%F0%C0%FB%D1%C7+%BF%D5%CF%AE&range=all&time=2018&stime=&etime=&num=20' url = startURL allNewsURL = [] errorPages = [] count = 1 while url: try: res = requests.get(url) print('已经爬取了第',count,'页！') print('URL:',url) soup = BeautifulSoup(res.text,'html.parser') for news in soup.findAll('h2'): allNewsURL.append(news.find('a').get('href')) pages = soup.find('div',{'id':\"_function_code_page\"}).findAll('a') if pages[-1].get('title') == '下一页': url = 'http://search.sina.com.cn' + pages[-1].get('href') else: break time.sleep(np.random.rand() + 2) count += 1 except AttributeError: print('ip被封！') with open('xuliyakongxi.csv','a') as f: for url in allNewsURL: f.write(url+'\\n') 获取文章详情获取文章详情需要注意以下两点： 不同的新闻来源具有不同的channel。 不是所有的新闻都具有关键词、源链接等信息，因此需要进行异常处理。 URLs = [] with open('xuliyakongxi.csv','r') as f: for url in f.readlines(): url = url.strip('\\n') URLs.append(url) allNews = [] count = 0 comments = [] notMatch= [] for url in URLs: try: currentNews = [] request = requests.get(url,timeout=4) request.encoding = 'utf-8' soup = BeautifulSoup(request.text,'html.parser') newsid = re.search('doc-i(.+).shtml',url).group(1) #新闻ID allChannels = ['gz','shc','gn','gj','jc','cj','kj','sh'] #不同来源的新闻对应不同的channel，此处将所有的channel进行遍历，并加入了异常处理 for channel in allChannels: try: comm = 'http://comment5.news.sina.com.cn/page/info?version=1&format=json&channel='+channel+'&newsid=comos-'+ newsid res_comm = requests.get(comm) commentsDesc = json.loads(res_comm.text) engageNum = commentsDesc['result']['count']['total'] #参与 commentsNum = commentsDesc['result']['count']['show'] #评论 comments.append(commentsDesc['result']['count']) except: if channel == 'sh': print('没有匹配') engageNum = None commentsNum = None notMatch.append(comm) else: continue else: break; titleAndKeywords = soup.find('title').text tk = titleAndKeywords.split('|') title = tk[0] #标题 if len(tk) > 2: keywords = tk[1:(len(tk)-1)] keywords.append(tk[-1].split('_')[0]) #关键词 else: keywords = tk[-1].split('_')[0] paras = soup.find('div',{'class':'article'}).text #正文，存在脏数据 try: date = soup.find('span',{'class':'date'}).text #日期 except: date = None try: sourceUrl = soup.find('a',{'class':'source'}).get('href') #源链接 source = soup.find('a',{'class':'source'}).text #来源 except: sourceUrl = None source = None currentNews.append(url) #url currentNews.append(title) #标题 currentNews.append(keywords) #关键词 currentNews.append(paras) #正文 currentNews.append(date) #日期 currentNews.append(sourceUrl) #源链接 currentNews.append(source) #来源 currentNews.append(engageNum) #参与人数 currentNews.append(commentsNum) #评论人数 allNews.append(currentNews) count += 1 print('已经爬取',str(count),'个页面.') print('comment URL:',comm) time.sleep(1+abs(np.random.rand())) if count % 20 == 0: time.sleep(5) # if count > 36: # break; except Exception as e: print(e) continue","link":"/2018/04/27/sina_scrapy/"},{"title":"MapReduce计算线性回归的系数","text":"先修知识设多元线性回归方程的模型为 Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+\\cdots+\\beta_pX_p可令$X_0=1$，则模型可写做： Y=\\beta_0X_0+\\beta_1X_1+\\beta_2X_2+\\cdots+\\beta_pX_p表示成矩阵形式为： Y=\\beta X其中， \\beta = \\left[\\begin{matrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_p \\end{matrix} \\right] X= \\left[\\begin{matrix} &x_{10} &x_{11} &\\cdots &x_{1p} \\\\ &x_{20} &x_{21} &\\cdots &x_{2p} \\\\ &\\vdots &\\vdots &\\ddots &\\vdots \\\\ &x_{n0} &x_{n1} &\\cdots &x_{np} \\end{matrix}\\right]则线性回归方程的最小二乘系数估计值为: \\hat{\\beta} = (X'X)^{-1}X'Y编程思路利用MapReduce计算系数是，由于输入数据是一行一行进行读取的，因此在计算的时候，不可能直接利用矩阵乘法进行计算。这里，我们假设输入的数据格式为： x_0\\ x_1\\ x_2\\ \\cdots x_p\\ y即 1\\ x_1\\ x_2\\ \\cdots x_p\\ y首先，考虑最简单的，计算$X’Y$，即计算 \\left[\\begin{matrix} &x_{10} &x_{20} &x_{30} &\\cdots &x_{n0} \\\\ &x_{11} &x_{21} &x_{31} &\\cdots &x_{n1} \\\\ &x_{12} &x_{22} &x_{32} &\\cdots &x_{n2} \\\\ &\\vdots &\\vdots &\\vdots &\\ddots &\\vdots \\\\ &x_{1p} &x_{2p} &x_{3p} &\\cdots &x_{np} \\end{matrix}\\right] \\times \\left[\\begin{matrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_n \\end{matrix}\\right]=\\left[\\begin{matrix} &x_{10}y_1+&x_{20}y_2+&x_{30}y_3+&\\cdots+&x_{n0}y_n \\\\ &x_{11}y_1+&x_{21}y_2+&x_{31}y_3+&\\cdots+&x_{n1}y_n \\\\ &x_{12}y_1+&x_{22}y_2+&x_{32}y_3+&\\cdots+&x_{n2}y_n \\\\ &\\vdots &\\vdots &\\vdots &\\ddots &\\vdots \\\\ &x_{1p}y_1+&x_{2p}y_2+&x_{3p}y_3+&\\cdots+&x_{np}y_n \\end{matrix}\\right]观察右侧矩阵可以发现，$y_1$总是与第一列相乘，$y_2$总是与第二列相乘，……，以此类推。而第一列实际上就是我们读取数据的第一行，第二列是读取数据的第二行……。根据这种规律，我们每读取一行，就让当前的$y$与所有的自变量相乘，然后把所有的结果累加求和，即为我们想要的结果。 上面已经解决了矩阵自变量矩阵的转置与因变量向量的乘积，即： part1 = X'Y那么，矩阵转置与矩阵的乘积$X’X$仍然可以仿照上述的方法进行。 X'X = \\left[\\begin{matrix} &x_{10} &x_{20} &x_{30} &\\cdots &x_{n0} \\\\ &x_{11} &x_{21} &x_{31} &\\cdots &x_{n1} \\\\ &x_{12} &x_{22} &x_{32} &\\cdots &x_{n2} \\\\ &\\vdots &\\vdots &\\vdots &\\ddots &\\vdots \\\\ &x_{1p} &x_{2p} &x_{3p} &\\cdots &x_{np} \\end{matrix}\\right] \\times\\left[\\begin{matrix} &x_{10} &x_{11} &\\cdots &x_{1p} \\\\ &x_{20} &x_{21} &\\cdots &x_{2p} \\\\ &x_{30} &x_{31} &\\cdots &x_{2p} \\\\ &\\vdots &\\vdots &\\ddots &\\vdots \\\\ &x_{n0} &x_{n1} &\\cdots &x_{np} \\end{matrix} \\right]=\\left[\\begin{matrix} &x_{10} &x_{20} &x_{30} &\\cdots &x_{n0} \\\\ &x_{11} &x_{21} &x_{31} &\\cdots &x_{n1} \\\\ &x_{12} &x_{22} &x_{32} &\\cdots &x_{n2} \\\\ &\\vdots &\\vdots &\\vdots &\\ddots &\\vdots \\\\ &x_{1p} &x_{2p} &x_{3p} &\\cdots &x_{np} \\end{matrix}\\right]\\times\\left[\\begin{matrix} x_0, x_1, x_2, \\cdots, x_p \\end{matrix}\\right]可以把上述既然课程看作是进行了$p+1$次的矩阵转置与向量乘积过程。当读取第一行时，我们可以得到一个矩阵： step1 = \\left[\\begin{matrix} &x_{10}x_{10} &x_{11}x_{10} &\\cdots &x_{1p}x_{10} \\\\ &x_{10}x_{11} &x_{11}x_{11} &\\cdots &x_{1p}x_{11} \\\\ &x_{10}x_{12} &x_{11}x_{12} &\\cdots &x_{1p}x_{12} \\\\ &\\vdots &\\vdots &\\ddots &\\vdots \\\\ &x_{10}x_{1p} &x_{11}x_{1p} &\\cdots &x_{1p}x_{1p} \\end{matrix}\\right]同理，当读取第二行的时候，同样可以得到一个矩阵： step2 = \\left[ \\begin{matrix} &x_{20}x_{20} &x_{21}x_{20} &\\cdots &x_{2p}x_{20} \\\\ &x_{20}x_{21} &x_{21}x_{21} &\\cdots &x_{2p}x_{21} \\\\ &x_{20}x_{22} &x_{21}x_{12} &\\cdots &x_{2p}x_{12} \\\\ &\\vdots &\\vdots &\\ddots &\\vdots \\\\ &x_{20}x_{2p} &x_{21}x_{2p} &\\cdots &x_{2p}x_{2p} \\end{matrix}\\right]此时，将$step1\\ step2$对应元素相加，即得到我们的更新矩阵。迭代下去，最终读取完所在的数据，即为$X’X$的结果。 当数据量很大的时候，用MapReduce方法进行计算，hadoop会将数据按照block的大小切分成若干块，每一块都执行mapper函数，在reducer里面把所有的mapper结果加起来，最后计算$(X’X)^{-1}X’Y$. 编程实现由于矩阵的输出不容易处理，因此在得到矩阵的时候，可以将其拉长为向量，这样就可以使用标准化输出函数print将计算结果输出，以便于reducer使用。 mapper函数如下： #! /usr/bin/anaconda2/bin/python # -*- coding:UTF-8 -*- import sys import numpy as np def read_input(file): for line in file: yield line.strip() def matmulti(): input = read_input(sys.stdin) innerLength = 0 length = 0 for line in input: fields = line.split(\",\") if innerLength == 0: innerLength = len(fields) - 1 data1 = np.array([0.0 for _ in range(innerLength)]) temp = np.array(fields,float)[:innerLength]*float(fields[innerLength]) data1 = data1 + temp if length == 0: length = len(fields) - 1 data2 = np.diag(np.zeros(length)) for index in range(length): data2[index] += np.array(fields[:length],dtype = float)*float(fields[index]) return data1,data2 data1,data2 = matmulti() data1 = list(data1) length = len(data2) data2 = data2.reshape(1,length**2) data2 = list(data2[0]) data = data1 + data2 print(\"\\t\".join(str(i) for i in data)) reducer函数如下： #! /usr/bin/anaconda2/bin/python # -*- coding:UTF-8 -*- import sys import numpy as np import math def read_input(file): for line in file: yield line.strip() input = read_input(sys.stdin) length = 0 for line in input: fields = line.split(\"\\t\") if length == 0: length = len(fields) data = np.array([0.0 for _ in range(length)]) fields = np.array(fields,dtype = float) data += fields lenght = len(data) varnums = int((-1+math.sqrt(1+4.0*length))/2.0) part1 = np.mat(data[:varnums]) part1 = part1.T part2 = data[varnums:] part2 = part2.reshape(varnums,varnums) part2 = np.mat(part2) part2 = part2.I result = part2*part1 print result 用R随机生成一份样本量为100万的数据，R回归结果为： Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 19.9995945 0.0602316 332.045","link":"/2018/04/01/mapreduce-LR/"},{"title":"囚徒困境","text":"2018年4月27日18时10分许，米脂县第三中学学生放学途中遭犯罪嫌疑人袭击,造成19名学生受伤，其中7人死亡。 有同学会说，嫌犯只有一个人，却导致了7人死亡的惨剧，如果大家一起上前反抗，肯定能阻止这次犯罪，将伤害降到最低。 这让我想起了囚徒困境，因为人都是理智的，都会做出对自有利的选择，所以没有人上前反抗，大家只顾着跑，即使有人受了伤害。 为此我们做出如下假设： A和B同时反抗，则歹徒被制服，无人伤亡； A上前反抗，B逃跑，则A死亡，B安全； A和B同时逃跑，则A和B有可能死亡。 绘制出如下二维表： 对于甲，有两种选择，即 1.上前阻止 如果乙上前阻止，则甲安全，乙安全 如果乙逃跑，则甲死亡，乙安全 2.逃跑 如果乙上前阻止，则甲安全，乙死亡 如果乙逃跑，则甲可能死亡，乙可能死亡 即对于甲来说，如果其上前阻止，那么乙的后果都会比甲要好，即乙都是安全的，那么甲不可能选择上前阻止。因此甲肯定选择逃跑。 而如果甲选择了逃跑，乙这时只有两种方案： 上前阻止，则甲安全，乙死亡 逃跑，则甲可能死亡，乙可能死亡 以上两个方案中，对乙最有利的是方案2，即乙肯定也会选择逃跑。 分析到此结束，不难理解，所有人都只顾着才跑，而没有人上前反抗了。","link":"/2018/05/02/囚徒困境/"},{"title":"小札-1","text":"","link":"/2018/03/31/小札-1/"},{"title":"支持向量机","text":"","link":"/2018/04/04/支持向量机/"},{"title":"排序算法","text":"排序算法简介冒泡排序所有算法程序#基础数据结果为python中的list testArr = [6,2,7,3,8,9] def inverse(array): lst = array.copy() length = len(array) mid = int((length - 1) / 2) for i in range(mid+1): temp = lst[i] lst[i] = lst[length-i-1] lst[length-i-1] = temp return lst #冒泡排序 def bubbleSort(array,desc = False): lst = array.copy() length = len(array) #默认升序,降序只需改变符号 if length == 1: return lst else: for i in range(length-1): for j in range(i+1,length): if lst[i] > lst[j]: temp = lst[i] lst[i] = lst[j] lst[j] = temp return lst #快速排序 def shortQuickSort(array,left,right): i = left j = right if i >= j: return array key = array[i] while i < j: while i < j and key = array[i]: i += 1 array[j] = array[i] array[i] = key print(array) shortQuickSort(array,left,i-1) shortQuickSort(array,j+1,right) return array def longQuickSort(array): def simInsertSort(array): lst = array.copy() if len(lst) == 0: return lst else: for i in range(1,len(lst)): current = lst[i] k = i for j in range(i-1,-1,-1): if current < lst[j]: lst[j+1] = lst[j] k = j lst[k] = current return lst def shellSort(): pass def simSelectSort(array): lst = array.copy() if len(lst) == 1: return lst else: for i in range(len(lst)-1): minVal = lst[i] k = i for j in range(i+1,len(lst)): if lst[j] < minVal: minVal = lst[j] k = j temp = lst[i] lst[i] = lst[k] lst[k] = temp return lst def heapSort(): pass def mergerSort(): pass def radixSort(): pass def bucketSort(): pass if __name__ == \"__main__\": simSelectSort(testArr) start = time.time() b = simInsertSort(a) time.time() - start def fab(n): if n == 1 or n == 2: return n else: return fab(n-1) + fab(n-2) #任何递归都可以通过循环实现，因此，快速排序也使用循环 def fab2(n): if n == 1 or n ==2 : return n else: a = 1 b = 2 for i in range(2,n): temp = b b = a + b a = temp return b","link":"/2018/05/09/排序算法/"},{"title":"新浪新闻爬虫","text":"获取标题列表新浪新闻总是不断更新的，因此有可能每次查询的返回新闻数量都是不一样的，所以我们最好不要通过设置固定的条数取爬取。 从上图可以看出，只要有“下一页”存在，我们就可以从下一页获取链接，从而开始下一页的爬取。 import requests import numpy as np from bs4 import BeautifulSoup import re import time import json #第一页的URL startURL = 'http://search.sina.com.cn/?c=news&q=%D0%F0%C0%FB%D1%C7+%BF%D5%CF%AE&range=all&time=2018&stime=&etime=&num=20' url = startURL allNewsURL = [] errorPages = [] count = 1 while url: try: res = requests.get(url) print('已经爬取了第',count,'页！') print('URL:',url) soup = BeautifulSoup(res.text,'html.parser') for news in soup.findAll('h2'): allNewsURL.append(news.find('a').get('href')) pages = soup.find('div',{'id':\"_function_code_page\"}).findAll('a') if pages[-1].get('title') == '下一页': url = 'http://search.sina.com.cn' + pages[-1].get('href') else: break time.sleep(np.random.rand() + 2) count += 1 except AttributeError: print('ip被封！') with open('xuliyakongxi.csv','a') as f: for url in allNewsURL: f.write(url+'\\n') 获取文章详情获取文章详情需要注意以下两点： 不同的新闻来源具有不同的channel。 不是所有的新闻都具有关键词、源链接等信息，因此需要进行异常处理。 URLs = [] with open('xuliyakongxi.csv','r') as f: for url in f.readlines(): url = url.strip('\\n') URLs.append(url) allNews = [] count = 0 comments = [] notMatch= [] for url in URLs: try: currentNews = [] request = requests.get(url,timeout=4) request.encoding = 'utf-8' soup = BeautifulSoup(request.text,'html.parser') newsid = re.search('doc-i(.+).shtml',url).group(1) #新闻ID allChannels = ['gz','shc','gn','gj','jc','cj','kj','sh'] #不同来源的新闻对应不同的channel，此处将所有的channel进行遍历，并加入了异常处理 for channel in allChannels: try: comm = 'http://comment5.news.sina.com.cn/page/info?version=1&format=json&channel='+channel+'&newsid=comos-'+ newsid res_comm = requests.get(comm) commentsDesc = json.loads(res_comm.text) engageNum = commentsDesc['result']['count']['total'] #参与 commentsNum = commentsDesc['result']['count']['show'] #评论 comments.append(commentsDesc['result']['count']) except: if channel == 'sh': print('没有匹配') engageNum = None commentsNum = None notMatch.append(comm) else: continue else: break; titleAndKeywords = soup.find('title').text tk = titleAndKeywords.split('|') title = tk[0] #标题 if len(tk) > 2: keywords = tk[1:(len(tk)-1)] keywords.append(tk[-1].split('_')[0]) #关键词 else: keywords = tk[-1].split('_')[0] paras = soup.find('div',{'class':'article'}).text #正文，存在脏数据 try: date = soup.find('span',{'class':'date'}).text #日期 except: date = None try: sourceUrl = soup.find('a',{'class':'source'}).get('href') #源链接 source = soup.find('a',{'class':'source'}).text #来源 except: sourceUrl = None source = None currentNews.append(url) #url currentNews.append(title) #标题 currentNews.append(keywords) #关键词 currentNews.append(paras) #正文 currentNews.append(date) #日期 currentNews.append(sourceUrl) #源链接 currentNews.append(source) #来源 currentNews.append(engageNum) #参与人数 currentNews.append(commentsNum) #评论人数 allNews.append(currentNews) count += 1 print('已经爬取',str(count),'个页面.') print('comment URL:',comm) time.sleep(1+abs(np.random.rand())) if count % 20 == 0: time.sleep(5) # if count > 36: # break; except Exception as e: print(e) continue","link":"/2018/04/27/新浪爬虫/"},{"title":"神经网络和深度学习","text":"感知机前向神经网络S型神经元梯度下降法反向神经网络编程实现","link":"/2018/04/13/神经网络/"},{"title":"线性模型","text":"","link":"/2018/04/04/线性模型/"},{"title":"特征选择","text":"","link":"/2018/04/04/特征选择/"},{"title":"平方根与牛顿法","text":"","link":"/2018/05/04/牛顿法/"},{"title":"贝叶斯分类器","text":"","link":"/2018/04/04/贝叶斯分类器/"},{"title":"降维方法","text":"线性判别分析(LDA)主成分分析(PCA)奇异值分解(SVD)非负矩阵分解(NMF)tsne分解","link":"/2018/04/04/降维方法/"},{"title":"Lubridate包简介","text":"lubridate包简介该包的描述文档介绍道：lubridate包包含了一些处理时间点和时间段的函数：快速并且友好的分割、提取和更新时间的函数（比如从日期中提取年、月、日等信息），在日期上进行代数运算.lubridate包运用了一致并且易于记忆的语法，使得用户在使用该包的时候会感到非常简单和愉快. library(lubridate) 各种函数介绍add_with_rollbackadd_with_rollback:Add and subtract months to a date without exceeding the last day of the new month. add_with_rollback:将某个日期加上或者减取若干个月，不考虑新获得的月份的最后一天的实际值.比如2009-10-31，加上一个月就是2009-11-31，但是11月没有31号，所以会显示NA.如果使用%m+%，则会以自然月的方式相加. 例子```rmay &lt;- ymd_hms(“1994-05-31 07:40:38”)may + months(1:3) #六月31显示为NAmay %m+% months(1:3) #不会有错误 leap &lt;- ymd(“2012-02-29”)leap + years(1) #NAleap %m+% years(1) #2013-02-28 ### am am:Does date time occur in the am or pm? am:判断是否是上午，显然，其需要的参数应该包含一个具体的一天中的时间信息，如果只提供日期信息，则默认为00:00:00，显示为上午时间. + 例子 ```r dt &lt;- ymd(&quot;2012-02-02&quot;) am(dt) time &lt;- ymd_hm(&quot;2012-02-01 13:20&quot;) am(time) as.durationas.duration:as.duration changes Interval, Period and numeric class objects to Duration objects. Numeric objects are changed to Duration objects with the seconds unit equal to the numeric value. as.duration:将interval、period和数字转换为时间间隔——时间段. 例子span","link":"/2018/04/01/Lubridate_notebook/"},{"title":"Selenium爬取MOOC网课程信息","text":"近期在写一份关于大数据相关的作业，需要搜索近年来市面上关于大数据的书籍信息和课程信息。其中一位同学负责在当当网上爬取书籍信息，我就负责爬取MOOC网的课程信息。 刚开始的时候，以为MOOC网作为一个公益性网站，安全性不会那么高，因此会比较好爬。然而我还是太天真了，网站上一大批JavaScript让我不知所措。好在经过一段时间的探索，终于能够成功爬取了。 网站分析打开MOOC官网，在搜索框输入“大数据”关键词，发现返回了99条数据（当时的情况），也就是说，有99个关于大数据的课程。 但是，只有课程列表是不行的。就像爬取淘宝网站的时候，获取到了商品列表，还需要进入到商品的详情页面，然后抓取我们需要的信息。在这里，我们同样需要这样的方法。 但是，通过Google浏览器的检查功能可以发现，你几乎无法在课程页面获取什么东西——因为几乎都是动态变化的。我试图获取每个课程上面的超链接，然后进入到具体的详情页面，但是很显然直接使用requests方法是不行的。 后来经过同学指点发现此处需要通过post方法，获取到response，返回的response里面才具有我们需要的详情页面的信息（其实也就是每个课程的id，通过该id可以构造详情页面） 代码设计获取课程id经过上面的分析，我首先找到了商品id存储的页面，如下图所示,我发现当我点击下一页的时候，会多出图中红色方框部分的网址，说明该网址是我请求的response，点击preview查看预览也印证了我的猜测。 问题搞清楚了，下面使用requests包的post函数发送请求，然后分析获取到的response。 import requests import urllib.parse as up #准备进行搜索的关键词 keywords = ['大数据','机器学习','数据挖掘','数据科学','人工智能'] #转换成URL编码 def quote(x): return up.quote(x) #转换编码 keywords = list(map(quote,keywords)) #URL前缀 startUrl = \"http://www.icourse163.org/search.htm?search=\" #构造URL urls = [] for kws in keywords: urls.append(startUrl+kws) #post的URL jsurl = \"http://www.icourse163.org/dwr/call/plaincall/MocSearchBean.searchMocCourse.dwr\" #请求头 headers = { \"Accept\":\"*/*\", \"Accept-Encoding\":\"gzip,deflate\", \"Accept-Language\":\"zh-CN,zh;q=0.9\", \"Connection\":\"keep-alive\", \"Content-Length\":\"522\", \"Content-Type\":\"text/plain\", \"Host\":\"www.icourse163.org\", \"Origin\":\"http://www.icourse163.org\" #Refere是我们查询的时候对应的URL，也需要根据不同的关键词进行调整 #\"Referer\":\"http://www.icourse163.org/search.htm?search=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0\" } #发送的数据 payload = { \"callCount\":\"1\", \"scriptSessionId\":\"${scriptSessionId}190\", \"httpSessionId\":\"907805e60a6540c4a268164e9e89ac4c\", \"c0-scriptName\":\"MocSearchBean\", \"c0-methodName\":\"searchMocCourse\", \"c0-id\":\"0\", #c0-e1的string是我们查询的关键词，需要根据不同的关键词进行更改 #\"c0-e1\":\"string:%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0\", #c0-e2的number表示获取的是第几页数据，需要动态变化 #\"c0-e2\":\"number:1\", \"c0-e3\":\"boolean:true\", \"c0-e4\":\"null:null\", \"c0-e5\":\"number:0\", \"c0-e6\":\"number:30\", \"c0-e7\":\"number:20\", \"c0-param0\":\"Object_Object:{keyword:reference:c0-e1,pageIndex:reference:c0-e2,highlight:reference:c0-e3,categoryId:reference:c0-e4,orderBy:reference:c0-e5,stats:reference:c0-e6,pageSize:reference:c0-e7}\", \"batchId\":\"1511830181483\" } #构造一个空字典，用于存储课程列表中每一门课程的id courses = {} #分析response for i in range(0,len(urls)): headers[\"Referer\"] = urls[i] string = \"string:\" + keywords[i] payload[\"c0-e1\"] = string for j in range(1,20): #大致查询了一下，课程数量不会超过20页 page = \"number:\" + str(j) payload[\"c0-e2\"] = page #目前为止，上面请求的部分已经做完 response = requests.post(data=payload,url=jsurl,headers = headers) courseid = re.findall(pattern=r'courseId=([0-9]{0,20})',string=response.text) if(len(courseid) == 0): break; else: kw = up.unquote(keywords[i]) if not kw in courses.keys(): courses[kw] = courseid else: courses[kw].extend(courseid) 获取详情上面已经获取到了课程的id，我们只需要使用该id构造课程详情页的URL就行了。 上图展示了课程详情页的URL信息，总结可以发现，前面的部分”http://www.icourse163.org/course/“ 都是一样的，只有后面的大学简称和id是变化的。而且大学简称可以使用任何非空值……利用上面的信息，构造好需要的URL，然后就可以使用selenium进行爬取了。 #使用无头浏览器phantomjs获取页面信息 browser = webdriver.PhantomJS('C:/phantomjs/bin/phantomjs.exe') #data用来存储我们获取到的数据 data = None data = pd.DataFrame({\"course_name\":\"\",\"start_times\":\"\",\"lasting\":\"\",\"start_date\":\"\",\"end_date\":\"\", \"rollnum\":\"\",\"coursehrs\":\"\",\"outline\":\"\",\"key_word\":\"\"},index=[\"0\"]) #data frame的行索引 index = 0 for k in courses.keys(): #k是键 for v in courses[k]: #v是值 #page是构造的课程详情页URL page = \"http://www.icourse163.org/course/ABC-\" + str(v) #get数据 browser.get(page) #每个页面之间停顿3秒，否则有可能还没有渲染成功，获取不到数据 #这应该是一种隐式等待 time.sleep(3) #info是我们需要的一系列信息，根据id(j-center)返回 info = browser.find_element_by_id('j-center').text info = re.sub(re.compile(\"\\n\"),\"\",info) info = re.sub(re.compile(r'[0-9]{2}:[0-9]{2}'),\"\",string=info) #1.课程名称 course_name = browser.find_element_by_tag_name('h1').text #2.第几次开课 start_times = re.search(pattern=\"第([0-9])次开课\",string=info) if not start_times is None: start_times = start_times.group(1) else: start_times = \"NA\" #3.持续时长 lasting = re.search(pattern=\"课程已进行至([0-9]{0,2}\\/[0-9]{0,2})周\",string=info) if not lasting is None: lasting = lasting.group(1) else: lasting = \"NA\" #4.开始日期 start_date = re.search(pattern= r\"开课：([0-9]{0,4}[年]{0,1}[0-9]{0,2}月[0-9]{0,2}日)\",string=info) if not start_date is None: start_date = start_date.group(1) else: start_date = \"NA\" #5.结束日期 end_date = re.search(pattern = r\"结束：([0-9]{0,4}[年]{0,1}[0-9]{0,2}月[0-9]{0,2}日)\",string=info) if not end_date is None: end_date = end_date.group(1) else: end_date = \"NA\" #6.参与人数 rollnum = re.search(pattern = r\"([0-9]{0,9})人参加\",string = info) if not rollnum is None: rollnum = rollnum.group(1) else: rollnum = \"NA\" #7.课程时长 coursehrs = re.search(pattern=r\"课程时长(.*?)周\",string=info) if not coursehrs is None: coursehrs = coursehrs.group(1) else: coursehrs = \"NA\" #8.课程概述 outline = browser.find_element_by_id('j-rectxt2').text if outline is None: outline = \"NA\" data.loc[index] = {\"course_name\":course_name,\"start_times\":start_times,\"lasting\":lasting,\"start_date\":start_date, \"end_date\":end_date,\"rollnum\":rollnum,\"coursehrs\":coursehrs,\"outline\":outline,\"key_word\":k} index = index + 1 print(\"已经获取第%d个课程数据！\"%(index)) 结果展示数据获取完毕以后，把存储在内存中的数据输出到Excel from pandas import ExcelWriter writer = ExcelWriter(\"MOOC.xlsx\") data.to_excel(writer,\"mooc\") writer.save() 最终展示在Excel中的数据如下图：","link":"/2018/04/01/selenium-网易云课堂/"},{"title":"决策树算法","text":"几种主要的决策树决策树算法的关键是选择最优划分属性，据此人们提出了三种决策树模型。 ID3决策树信息熵(Information entropy)是度量样本集合纯度最常用的一种指标，假定当前样本集合$D$中第$k$类样本所占的比例为$p_k,(k=1,2,\\cdots,n)$，则$D$的信息熵定义为： Ent(D)=-\\sum _{k=1}^np_k\\ log_2p_k$Ent(D)$的值越小，则$D$的纯度越高 其实我们在高中化学就接触过“熵”的概念，指的是物质的混乱程度。是纯度的对立面。我们说熵越大，混乱程度越高，也就是纯度越低。同理，熵越小，混乱程度越低，即纯度越高。 假设离散属性$a$有$V$个可能的取值$\\{a^1,a^2,\\cdots,a^V\\}$,若使用$a$来对样本集$D$进行划分，则会产生$V$个分支结点，其中第$v$个分支结点包含了$D$中所有在属性$a$上取值为$a^v$的样本，记为$D^v$。我们可以根据上式计算法$D^v$的信息熵，再考虑到不同的分支结点所包含的样本数不同，给分支结点赋予权重$|D^v|/|D|$，即样本越多的分支结点的影响越大，于是计算出用属性$a$对样本集$D$进行划分所获得的“信息增益”： Gain(D,a) = Ent(D) - \\sum _{v=1}^V \\frac{|D^v|}{|D|}Ent(D^v)一般来说，信息增益越大，意味着使用属性$a$来进行划分所获得的“纯度提升”越大。因此，我们总是选择使得信息增益最大的那个属性来进行划分。 我们用下面的例子来说明划分的过程。该数据是一份医学数据，根据病人的一些特征，给出佩戴硬质隐形眼镜、软质隐形眼镜和不佩戴隐形眼镜的建议。数据共有5个变量，其中4个自变量，1个因变量。 age of patient：简称age，患者年龄，(1) young, (2) pre-presbyopic, (3) presbyopic spectacle prescription：简称sp，视力情况，(1)近视myope，(2)远视hypermetrope astigmatic：是否散光，(1) no, (2) yes tear production rate: 简称tpr，眼泪生成率，(1) reduced, (2) normal suggestion：1 : hard contact lenses, 2 : soft contact lenses, 3 : should not id age sp astigmatic tpr suggestion 1 1 1 1 1 3 2 1 1 2 1 3 3 1 2 1 1 3 4 1 2 2 1 3 5 2 1 1 1 3 6 2 1 2 1 3 7 2 2 1 1 3 8 2 2 2 1 3 9 3 1 1 1 3 10 3 1 2 1 3 11 3 2 1 1 3 12 3 2 2 1 3 13 1 1 2 2 1 14 1 2 2 2 1 15 2 1 2 2 1 16 3 1 2 2 1 17 1 1 1 2 2 18 1 2 1 2 2 19 2 1 1 2 2 20 2 2 1 2 2 21 3 2 1 2 2 22 2 2 2 2 3 23 3 1 1 2 3 24 3 2 2 2 3 我们首先计算出总的信息熵： Ent(D) = \\sum_{k=1}^3 -p_klog_2p_k = -(\\frac{4}{24}log_2\\frac{4}{24} + \\frac{5}{24}log_2\\frac{5}{24} + \\frac{15}{24}log_2\\frac{15}{24})=1.326我们首先选择age作为分支结点，age有三个取值，分布为1,2,3。当age=1时，suggestion取值为2个1,2个2,4个3。则当age=1时，$D^1$的信息熵为： Ent(D^1) = \\sum_{k=1}^3 -p_klog_2p_k=-(\\frac{2}{8}log_2\\frac{2}{8} + \\frac{2}{8}log_2\\frac{2}{8} + \\frac{4}{8}log_2\\frac{4}{8})=1.5同理， Ent(D^2)== \\sum_{k=1}^3 -p_klog_2p_k=-(\\frac{1}{8}log_2\\frac{1}{8} + \\frac{2}{8}log_2\\frac{2}{8} + \\frac{5}{8}log_2\\frac{5}{8})=1.299Ent(D^2)== \\sum_{k=1}^3 -p_klog_2p_k=-(\\frac{1}{8}log_2\\frac{1}{8} + \\frac{1}{8}log_2\\frac{1}{8} + \\frac{6}{8}log_2\\frac{6}{8})=1.061那么，age的信息增益为： \\begin{aligned}Gain(D,age) = & Ent(D) - \\sum _{v=1}^3\\frac{|D^v|}{|D|}Ent(D^v) \\\\ =& 1.326 - (\\frac{1}{3}\\times 1.5 + \\frac{1}{3}\\times 1.299+ \\frac{1}{3}\\times 1.061)=0.0393\\end{aligned}类似的，我们可以计算出其他属性的信息增益： Gain(D,sp)=0.0395Gain(D,astigmatic ) = 0.377Gain(D,tpr) = 0.549可以看到，$tpr$的信息增益最大，因此把它选为划分属性，下图表示基于$tpr$进行划分的结果。 然后，决策树按照同样的规则，对上面两个已经生成的结点继续划分。这个时候，决策树有以下三个停止原则： 当前结点包含的样本全部属于同一个类别，无需划分。上述左子树就是这种情况； 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分。 当前结点包含的样本集合为空，不能划分。 接下来，由于左子树无法继续划分，因此我们继续划分右子树。此时，右子树相当于根节点，我们计算其他三个属性的信息增益。 上述计算出的右子树的熵为1.555. 则有： Gain(D^2,age)=0.221Gain(D^2,sp)=0.095Gain(D^2,astigmatic) = 0.771此轮选择的划分属性为$astigmatic$，下图表示继续划分的结果。 截止目前，决策树还没结束，以上两个新的叶子节点包含的样本仍然归属于不同的类型，因此还需要继续进行划分。 我们仍然沿用上面的符号，根据计算可知， Ent(D^1) = 0.650$Ent(D^2) = 0.918$ 目前还剩下age和sp两个属性，计算出相应的信息增益： Gain(D^1,age) = 0.32Gain(D^1,sp) = 0.191Gain(D^2,age) = 0.251Gain(D^2,sp) = 0.459因此，对于$D^1$，选择age进行划分，对于$D^2$，选择sp进行划分。划分结果如下。 截止目前，我们的决策树还没有完全将样本划分开来。比如21和23分属于2和3。但是目前每个分支只剩下一个属性可以继续划分，因此我们不需要再计算信息增益，直接划分即可。 上述每个叶子结点包含的样本都归属于同一类别，且所有的属性被用于划分。 上面的决策树是根据所有样本进行划分的，因此无法用来进行预测。如果要用于预测，应该把数据分成训练集和测试集。 ID3决策树就是根据信息增益来选择最优划分属性，然后构建决策树的。该算法简单易懂，十分容易上手。 C4.5决策树如果在上述划分的过程中，把id也作为一个候选属性参与划分，那么可计算出$Gain(D,id)=1.326$，是最大的。但是根据id会划分出24个分支，再出现一个新样本的话，则无法预测该样本属于哪一类，也就是泛化能力较差。 实际上，信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，C4.5算法使用增益率(gain ratio)来选择最有划分属性。延用上述符号，增益率定义为： Gain\\_ratio(D,a) = \\frac{Gain(D,a)}{IV(a)}其中 IV(a) = - \\sum _{v=1}^V\\frac{|D^v|}{|D|}log_2\\frac{|D^v|}{|D|}称为$a$的固有值。属性$a$的可能取值数目却大，则$IV(a)$的值通常会越大。 CART决策树CART树使用基尼指数(Gini index)来选择划分属性，延用上述符号，数据集$D$的纯度可用基尼值来度量： Gini(D) = \\sum _{k=1}^n\\sum _{k' \\ne k}p_kp_{k'} = 1- \\sum _{k=1}^n p_k^2直观来说，$Gini(D)$反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率。因此，Gini(D)越小，则数据的纯度越高。 假设所有的类别都是一样的，则Gini(D)=0,纯度最高 那么，属性$a$的基尼指数定义为： Gini\\_index(D,a)=\\sum _{v=1}^V\\frac{|D^v|}{|D|}Gini(D^v)于是，我们在候选属性集合$A$中，选择那个使得划分后基尼指数最小的属性作为划分属性。 可以看出，CART树的构建比前两种树都简单一点。 剪枝处理前剪枝后剪枝","link":"/2018/04/04/决策树/"},{"title":"爬取淘宝商品列表","text":"前段时间老师让我爬取淘宝的商品列表以及其商品详情数据，期间遇到了很多问题。最困难的就是淘宝的价格数据是以Ajax异步加载的，这些数据暂时还没有能力获取到。 下面介绍一下基本思路。 首先，通过抓取商品列表的商品ID获取商品的身份标识，然后根据商品ID跳转到具体的商品列表，对其他属性进行抓取。 观察两条商品列表的URL： https://s.taobao.com/search?q=帽子&amp;imgfile=&amp;commend=all&amp;ssid=s5-e&amp;search_type=item&amp;sourceId=tb.index&amp;spm=a21bo.50862.201856-taobao-item.1&amp;ie=utf8&amp;initiative_id=tbindexz_20170806&amp;bcoffset=7&amp;ntoffset=7&amp;p4ppushleft=1%2C48&amp;s=0 https://s.taobao.com/search?q=帽子&amp;imgfile=&amp;commend=all&amp;ssid=s5-e&amp;search_type=item&amp;sourceId=tb.index&amp;spm=a21bo.50862.201856-taobao-item.1&amp;ie=utf8&amp;initiative_id=tbindexz_20170806&amp;bcoffset=4&amp;ntoffset=4&amp;p4ppushleft=1%2C48&amp;s=44 https://s.taobao.com/search?q=帽子&amp;imgfile=&amp;commend=all&amp;ssid=s5-e&amp;search_type=item&amp;sourceId=tb.index&amp;spm=a21bo.50862.201856-taobao-item.1&amp;ie=utf8&amp;initiative_id=tbindexz_20170806&amp;bcoffset=4&amp;ntoffset=4&amp;p4ppushleft=1%2C48&amp;s=88 这是前三个页面的URL，可以发现，除了”q=”和”s=”后面的数据不一样，其他的都是一样的，因此，可以把URL简化为： https://s.taobao.com/search?q=keyword&amp;s=pagenum 其中，q代表搜索的关键词，s代表商品列表的页数，0代表第一页，44代表第二页，88代表第三页…… 先以此URL抓取商品的ID等信息。 代码如下。 import re import requests from pandas import * from collections import OrderedDict def getDetails(startpage,endpage): #如果需要爬取具体的商品详情，页数过多可能会出现异常，此函数可以用来控制一次爬取的页数 url_head='https://s.taobao.com/search?q=帽子&s=' #这是淘宝搜索列表的url前面的相同部分，q=''代表搜索的关键词，s=''代表第几页，s=0为第1页s=44为第二页，以此类推 url_list=[url_head+str(i*44) for i in range(startpage-1,endpage)] #生成需要爬取的商品列表url #定义存储商品列表数据数据的列表 nid_list=[] raw_title_list=[] view_price_list=[] view_sales_list=[] item_loc_list=[] for url in url_list: resp=requests.get(url) print(resp.url) nid=re.findall(pattern='\"nid\":\"(.*?)\"',string=resp.text) #商品id,唯一，可以此跳转到其商品详情页面，然后进行其他信息的抓取 raw_title=re.findall(pattern='\"raw_title\":\"(.*?)\"',string=resp.text) #商品名称 view_price=re.findall(pattern='\"view_price\":\"(.*?)\"',string=resp.text) #商品价格 view_sales=re.findall(pattern='\"view_sales\":\"(.*?)\"',string=resp.text) #商品销量 item_loc=re.findall(pattern='\"item_loc\":\"(.*?)\"',string=resp.text) #发货地址 #逐个存储 nid_list.extend(nid) raw_title_list.extend(raw_title) view_price_list.extend(view_price) view_sales_list.extend(view_sales) item_loc_list.extend(item_loc) #生成数据框 dt={'商品id':nid_list,'商品名称':raw_title_list,'商品价格':view_price_list, '商品销量':view_sales_list,'商品发货地址':item_loc_list} df=DataFrame(dt) #根据字典生成数据框 #写入Excel writer1=ExcelWriter(\"taobao_details.xlsx\") #新建一个空白Excel工作簿 df.to_excel(writer1,\"Sheet1\") #将df写入Sheet1工作表 writer1.save() 上述代码可以获取到商品ID、商品名称、商品价格、商品销量、发货地址信息。接下来，利用商品ID信息，跳转到具体的商品详情页面，对其他属性进行抓取。 对其他属性的抓取，主要着眼于两个方面。一个是商品的描述、服务于物流评分，这个虽然淘宝与天猫的HTML页面有所差别，但是获取方式大同小异。另外，所有的商品详情都放在了一个列表里面，因此用beautifulsoup提取十分方便。 抓取淘宝商品详情的代码如下： def getTaoBaoDetails(url): import requests import re from bs4 import BeautifulSoup # from pandas import DataFrame from collections import OrderedDict res=requests.get(url) soup=BeautifulSoup(res.text,\"html.parser\") dd=soup.select(\".tb-shop-rate dd\") #获取描述、服务、物流的数字信息，该信息存放在一个列表，需要使用正则表达式提取 dd_value=[] if len(dd)>0: try: for d in range(0,3): dd_value.append(re.search(pattern=r'[\\s]*([0-9]\\.[0-9])[\\s]*',string=dd[d].text).group(1)) except IndexError as err: print(res.url) #下面的语句获取属性列表 attrs=soup.select(\".attributes-list li\") attrs_name=[] attrs_value=[] for attr in attrs: attrs_name.append(re.search(r'(.*?):[\\s]*(.*)',attr.text).group(1)) attrs_value.append(re.search(r'(.*?):[\\s]*(.*)',attr.text).group(2)) allattrs=OrderedDict() #存放该产品详情页面所具有的属性 for k in range(0,len(attrs_name)): allattrs[attrs_name[k]]=attrs_value[k] info=OrderedDict() #存放该商品所具有的全部信息 #下面三条语句获取描述、服务、物流的评分信息 if len(dd_value)>0: info['描述']=dd_value[0] info['服务']=dd_value[1] info['物流']=dd_value[2] else: info['描述']='NA' info['服务']='NA' info['物流']='NA' #下面的语句用来判断该商品具有哪些属性，如果具有该属性，将属性值插入有序字典，否则，该属性值为空 #适用场景 if '适用场景' in attrs_name: info['适用场景']=allattrs['适用场景'] else: info['适用场景']='NA' #适用对象 if '适用对象' in attrs_name: info['适用对象']=allattrs['适用对象'] else: info['适用对象']='NA' #款式 if '款式' in attrs_name: info['款式']=allattrs['款式'] else: info['款式']='NA' #尺码 if '尺码' in attrs_name: info['尺码']=allattrs['尺码'] else: info['尺码']='NA' #帽顶款式 if '帽顶款式' in attrs_name: info['帽顶款式']=allattrs['帽顶款式'] else: info['帽顶款式']='NA' #帽檐款式 if '帽檐款式' in attrs_name: info['帽檐款式']=allattrs['帽檐款式'] else: info['帽檐款式']='NA' #檐形 if '檐形' in attrs_name: info['檐形']=allattrs['檐形'] else: info['檐形']='NA' #主要材质 if '主要材质' in attrs_name: info['主要材质']=allattrs['主要材质'] else: info['主要材质']='NA' #人群 if '人群' in attrs_name: info['人群']=allattrs['人群'] else: info['人群']='NA' #品牌 if '品牌' in attrs_name: info['品牌']=allattrs['品牌'] else: info['品牌']='NA' #风格 if '风格' in attrs_name: info['风格']=allattrs['风格'] else: info['风格']='NA' #款式细节 if '款式细节' in attrs_name: info['款式细节']=allattrs['款式细节'] else: info['款式细节']='NA' #颜色分类 if '颜色分类' in attrs_name: info['颜色分类']=allattrs['颜色分类'] else: info['颜色分类']='NA' #适用季节 if '适用季节' in attrs_name: info['适用季节']=allattrs['适用季节'] else: info['适用季节']='NA' #适用年龄 if '适用年龄' in attrs_name: info['适用年龄']=allattrs['适用年龄'] else: info['适用年龄']='NA' return info 抓取天猫商品详情的代码如下： def getTmallDetails(url): import requests import re from bs4 import BeautifulSoup # from pandas import DataFrame from collections import OrderedDict res=requests.get(url) soup=BeautifulSoup(res.text,\"html.parser\") # dt=soup.select(\".tb-shop-rate dt\") #获取描述、服务、物流的文本信息，该信息存放在一个列表，需要使用正则表达式提取 dd=soup.select(\".shop-rate ul li\") #获取描述、服务、物流的数字信息，该信息存放在一个列表，需要使用正则表达式提取 # dt_name=[] dd_value=[] # for t in range(0,3): # dt_name.append(dt[t].text) if len(dd)>0: for d in dd: dd_value.append(re.search(r'([0-9][.][0-9])',d.text).group()) #下面的语句获取属性列表 attrs=soup.select('#J_AttrUL li') attrs_name=[] attrs_value=[] for attr in attrs: attrs_name.append(re.search(r'(.*?):[\\s]*(.*)',attr.text).group(1)) attrs_value.append(re.search(r'(.*?):[\\s]*(.*)',attr.text).group(2)) allattrs=OrderedDict() #存放该产品详情页面所具有的属性 for k in range(0,len(attrs_name)): allattrs[attrs_name[k]]=attrs_value[k] info=OrderedDict() #存放该商品所具有的全部信息 #下面三条语句获取描述、服务、物流的评分信息 if len(dd_value)>0: info['描述']=dd_value[0] info['服务']=dd_value[1] info['物流']=dd_value[2] else: info['描述']='NA' info['服务']='NA' info['物流']='NA' #下面的语句用来判断该商品具有哪些属性，如果具有该属性，将属性值插入有序字典，否则，该属性值为空 #适用场景 if '适用场景' in attrs_name: info['适用场景']=allattrs['适用场景'] else: info['适用场景']='NA' #适用对象 if '适用对象' in attrs_name: info['适用对象']=allattrs['适用对象'] else: info['适用对象']='NA' #款式 if '款式' in attrs_name: info['款式']=allattrs['款式'] else: info['款式']='NA' #尺码 if '尺码' in attrs_name: info['尺码']=allattrs['尺码'] else: info['尺码']='NA' #帽顶款式 if '帽顶款式' in attrs_name: info['帽顶款式']=allattrs['帽顶款式'] else: info['帽顶款式']='NA' #帽檐款式 if '帽檐款式' in attrs_name: info['帽檐款式']=allattrs['帽檐款式'] else: info['帽檐款式']='NA' #檐形 if '檐形' in attrs_name: info['檐形']=allattrs['檐形'] else: info['檐形']='NA' #主要材质 if '主要材质' in attrs_name: info['主要材质']=allattrs['主要材质'] else: info['主要材质']='NA' #人群 if '人群' in attrs_name: info['人群']=allattrs['人群'] else: info['人群']='NA' #品牌 if '品牌' in attrs_name: info['品牌']=allattrs['品牌'] else: info['品牌']='NA' #风格 if '风格' in attrs_name: info['风格']=allattrs['风格'] else: info['风格']='NA' #款式细节 if '款式细节' in attrs_name: info['款式细节']=allattrs['款式细节'] else: info['款式细节']='NA' #颜色分类 if '颜色分类' in attrs_name: info['颜色分类']=allattrs['颜色分类'] else: info['颜色分类']='NA' #适用季节 if '适用季节' in attrs_name: info['适用季节']=allattrs['适用季节'] else: info['适用季节']='NA' #适用年龄 if '适用年龄' in attrs_name: info['适用年龄']=allattrs['适用年龄'] else: info['适用年龄']='NA' return info 最后，将所获取的信息合并在一起，输出为Excel： url_start='https://item.taobao.com/item.htm?id=' info_df=DataFrame() for id in nid_list: url_detail=url_start+str(id) res=requests.get(url_detail) if not isnull(re.search('tmall',res.url)): detial=getTmallDetails(url_detail) else: detial=getTaoBaoDetails(url_detail) detial['商品id']=id info_df=info_df.append(detial,ignore_index=True) writer2=ExcelWriter(\"detail.xlsx\") info_df.to_excel(writer2,\"Sheet1\") writer2.save() 上述代码应该与第一部分的代码合并在一起，这样会同时输出两个Excel，如果每10个页面输出一次，则需要对输出的Excel重命名，否则下一次输出会覆盖前一次的数据。","link":"/2018/04/01/爬虫-淘宝/"},{"title":"逻辑回归","text":"引言逻辑回归常用来处理分类问题，最常用来处理二分类问题。 生活中经常遇到具有两种结果的情况（冬天的北京会下雪，或者不会下雪；暗恋的对象也喜欢我，或者不喜欢我；今年的期末考试会挂科，或者不会挂科……）。对于这些二分类结果，我们通常会有一些输入变量，或者是连续性，或者是离散型。那么，我们怎样来对这些数据建立模型并且进行分析呢？ 我们可以尝试构建一种规则来根据输入变量猜测二分输出变量，这在统计机器学上被称为分类。然而，简单的给一个回答“是”或者“不是”显得太过粗鲁，尤其是当我们没有完美的规则的时候。总之呢，我们不希望给出的结果就是武断的“是”或“否”，我们希望能有一个概率来表示我们的结果。 一个很好的想法就是，在给定输入$X$的情况下，我们能够知道Y的条件概率$Pr(Y|X)$。一旦给出了这个概率，我们就能够知道我们预测结果的准确性。 让我们把其中一个类称为1，另一个类称为0。（具体哪一个是1，哪一个是0都无所谓）。$Y$变成了一个指示变量，现在，你要让自己相信，$Pr(Y=1)=EY$，类似的，$Pr(Y=1|X=x)=E[Y|X=x]$。 假设$Y$有10个观测值，分别为 0 0 0 1 1 0 1 0 0 1.即6个0,4个1.那么，$Pr(Y=1)=\\frac{count(1)}{count(n)}=\\frac{4}{10}=0.4$，同时，$EY=\\frac{sum(Y)}{count(n)}=\\frac{4}{10}=0.4$ 换句话说，条件概率是就是指示变量（即$Y$)的条件期望。这对我们有帮助，因为从这个角度上，我们知道所有关于条件期望的估计。我们要做的最直接的事情是挑选出我们喜欢的平滑器，并估计指示变量的回归函数，这就是条件概率函数的估计。 有两个理由让我们放弃陷入上述想法。 概率必须介于0和1之间，但是我们在上面估计出来的平滑函数的输出结果却不能保证如此，即使我们的指示变量$y_i$不是0就是1； 另一种情况是，我们可以更好地利用这个事实，即我们试图通过更显式地模拟概率来估计概率。 假设$Pr(Y=1|X=x)=p(x;\\theta)$,$p$是参数为$\\theta$的函数。进一步，假设我们的所有观测都是相互独立的，那么条件似然函数可以写成： \\prod _{i=1}^nPr(Y=y_i|X=x_i)=\\prod _{i=1}^np(x_i;\\theta)^{y_i}(1-p(x_i;\\theta))^{1-y_i}回忆一下，对于一系列的伯努利试验$y_1,y_2,\\cdots,y_n$，如果成功的概率都是常数$p$，那么似然概率为： \\prod _{i=1}^n p^{y_i}(1-p)^{1-y_i}我们知道，当$p=\\hat{p}=\\frac{1}{n}\\sum _{i=1}^ny_i$时，似然概率取得最大值。如果每一个试验都有对应的成功概率$p_i$，那么似然概率就变成了 \\prod _{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i}不添加任何约束的通过最大化似然函数来估计上述模型是没有意义的。当$\\hat{p_i}=1$的时候，$y_i=1$，当$\\hat{p_i}=0$的时候，$y_i=0$。我们学不到任何东西。如果我们假设所有的$p_i$不是任意的数字，而是相互连接在一起的，这些约束给我们提供了一个很重要的参数，我们可以通过这个约束来求得似然函数的最大值。对于我们正在讨论的这种模型，约束条件就是$p_i=p(x_i;\\theta)$，当$x_i$相同的时候，$p_i$也必须相同。因为我们假设的$p$是未知的，因此似然函数是参数为$\\theta$的函数，我们可以通过估计$\\theta$来最大化似然函数。 逻辑回归总结一下：我们有一个二分输出变量$Y$，我们希望构造一个关于$x$的函数来计算$Y$的条件概率$Pr（{Y=1|X=x}）$，所有的未知参数都可以通过最大似然法来估计。到目前为止，你不会惊讶于发现统计学家们通过问自己“我们如何使用线性回归来解决这个问题”。 最简单的一个想法就是令$p(x)$为线性函数。无论$x$在什么位置，$x$每增加一个单位，$p$的变化量是一样的。由于线性函数不能保证预测结果位于0和1之间，因此从概念上线性函数就不适合。另外，在很多情况下，根据我们的经验可知，当$p$很大的时候，对于$p$的相同的变化量，$x$的变化量将会大于$p$在1/2附近的变化量。线性函数做不到这样。 另一个最直观的想法是令$log\\ p(x)$为$x$的线性函数。但是对数函数只在一个方向上是无界的，因此也不符合要求。 最后，我们选择了$p$经过logit变换以后的函数$ln\\frac{p}{1-p}$。这个函数就很好啊，满足了我们的所有需求。 最后一个选择就是我们所说的逻辑回归。一般的，逻辑回归的模型可以表示为如下形式： ln\\frac{p(x)}{1-p(x)}=\\beta_0+x\\beta根据上式，解出$p$ p(x;b,w)=\\frac{e^{\\beta_0+x\\beta}}{1+e^{\\beta_0+x\\beta}}=\\frac{1}{1+e^{-(\\beta_0+x\\beta)}}为了最小化错分率，当$p\\ge 0.5$的时候，我们预测$Y=1$，否则$Y=0$。这意味着当$\\beta_0+x\\beta$非负的时候，预测结果为1，否则为预测结果为0.因此，逻辑回归为我么提供了一个线性分类器。决策边界是$\\beta_0+x\\beta=0$，当$x$是一维的时候，决策边界是一个点，当$x$是二维的时候，决策边界是一条直线，以此类推。空间中某个点到决策边界的距离为$\\beta_0/||\\beta||+x\\cdot\\beta/||\\beta||$.逻辑回归不仅告诉我们两个类的决策边界，还以一种独特的方式根据样本点到决策边界的距离给出该点分属于某类的概率。当$||\\beta||$越大的时候，概率取极端值（0或1）的速度就越快。上述说明使得逻辑回归不仅仅是一个分类器，它能做出更简健壮、更详细的预测，并能以一种不同的方式进行拟合;但那些强有力的预测可能是错误的。 似然函数和逻辑回归因为逻辑回归的预测结果是概率，而不是类别，因此我们可以用似然函数来拟合模型。对于每一个样本点，我们有一个特征向量$x_i$，这个向量的维度就是特征的个数。同时还有一个观测类别$y_i$。当$y_i=1$的时候，该类的概率为$p$，否则为$1-p$。因此，似然函数为： L(\\beta_0,\\beta) = \\prod _{i=1}^np(x_i)^{y_i}(1-p(x_i))^{1-y_i}对数似然函数为： \\begin{aligned} \\ell(\\beta_0,\\beta) &= \\sum_{i=1}^ny_iln(p(x_i))+(1-y_i)ln(1-p(x_i)) \\\\&=\\sum_{i=1}^n(y_i(ln\\frac{p(x_i)}{1-p(x_i)})+ln(1-p(x_i))) \\\\&=\\sum_{i=1}^ny_i(\\beta_0+x_i\\beta)-ln(1+e^{\\beta_0+x_i\\beta}) \\end{aligned} 为了表示方便，统一将$\\beta_0,\\beta$表示成$\\beta$,则$\\ell$对$\\beta$的一阶导数为： \\begin{aligned} \\frac{\\partial \\ell}{\\partial \\beta} &=\\sum _{i=1}^n[y_i-\\frac{e^{\\beta^Tx_i}}{1+\\beta^Tx_i}]x_i\\\\& = \\sum _{i=1}^n(y_i-p_i)x_i\\end{aligned}多分类逻辑回归如果$Y$有多个类别，我们仍然可以使用逻辑回归。假如有$k$个类别，分别是$0,1,\\cdots,k-1$，对于每一个类$k$，其都有对应的$\\beta_0$和$\\beta$，每个类对应的概率为: P(Y=c|X=x)=\\frac{e^{\\beta_0^c+x\\beta^c}}{\\sum e^{\\beta_0^c+x\\beta^c}}观察上式可以发现，二分类逻辑回归求是多分类逻辑回归的特例. 在这里，读者可能比较好奇，根据上式，二分类逻辑回归的分母中的1是怎么来的。其实，无论有多少个类，我们总是将第一类的系数设置为0，那么类别为0的那部分在分母中对应的就是1.这样做对模型的通用性没有任何影响。 有读者可能会问，为什么偏要把第一个类的系数设置为0，而不是其他的类。事实上，你可以设置任何一个类的系数为0，并且最终计算出来的结果都是一样的。所以，按照惯例，我们都是把第一个类的系数设置为0. 牛顿法求解参数为了求出待估参数$\\beta$，我们利用Newton-Raphson算法。首先对对数似然函数求二阶偏导： \\frac{\\partial ^2\\ell (\\beta)}{\\partial \\beta \\partial \\beta^T}=-\\sum_{i=1}^nx_ix_i^Tp_i(1-p_i)注意，上面的$x_i$是个向量，也就是上面所说的特征向量，维度为特征个数加一。即假设原始数据为$n\\times m$矩阵，其中n表示观测数，m表示特征数。则$x_i$的长度为m+1。根据上述说明，上面的二阶偏导实际上是一个$(m+1)\\times (m+1)$的矩阵。 如果给定一个$\\hat{\\beta}^{old}$，则一步牛顿迭代为（梯度下降）： \\hat{\\beta}^{new}=\\hat{\\beta}^{old}-(\\frac{\\partial ^2\\ell (\\beta)}{\\partial \\beta \\partial \\beta^T})^{-1} \\cdot\\frac{\\partial \\ell({\\beta})}{\\partial \\beta}将上述式子表示成矩阵的形式就是： \\frac{\\partial \\ell(\\beta)}{\\partial \\beta}=X^T(y-p)\\frac{\\partial ^2\\ell (\\beta)}{\\partial \\beta \\partial \\beta^T}=-X^TWX其中，$X$为原始自变量矩阵，$y$为类别向量，$p$为预测概率向量，$W$是一个$n\\times n$对角矩阵，第$i$个元素取值为$p(x_i,\\hat{\\beta}^{old})(1-p(x_i,\\hat{\\beta}^{old}))$. 联立上述两个式子，可以得出参数的迭代公式： \\begin{aligned}\\hat{\\beta}^{new} &=\\hat{\\beta}^{old}+(X^TWX)^{-1}X^T(y-p) \\\\ &=(X^TWX)^{-1}X^TW(X\\hat{\\beta^{old}}+W^{-1}(y-p)) \\\\ &=(X^TWX)^{-1}X^TWz \\end{aligned}其中，$z=X\\hat{\\beta^{old}}+W^{-1}(y-p)$. 实际上，$X^TWX$是一个黑塞矩阵 H(\\beta) = \\frac{\\partial ^2\\ell (\\beta)}{\\partial \\beta \\partial \\beta^T}即目标函数对$\\beta$的二阶偏导，那么，上述迭代公式也可以写作： \\begin{aligned} \\hat{\\beta}^{new} &=\\hat{\\beta}^{old} - H(\\beta)^{-1}\\frac{\\partial \\ell(\\beta)}{\\beta} \\\\ &= \\hat{\\beta}^{old} - H(\\beta)^{-1}X^T(y-p) \\end{aligned}上述是我们熟悉的牛顿迭代公式。 矩阵相乘的计算不算复杂，但是当数据量上升以后，黑塞矩阵的求逆就非常复杂了，因此衍生出许多拟牛顿算法，本节不讨论优化算法。 很明显，本例的目标函数就是对数似然函数$\\ell(\\beta)$，也就是求其最大值。然而，很多同学已经习惯了牛顿法求最小值，因此，为了大家看着方便，下面介绍梯度下降法求解逻辑回归。 只需要在上述似然函数前面加一个负号，本例就变成了一个梯度下降的问题了。为了形式上好看，还可以在前面对数似然函数求一个均值，即除以样本量。 假设$J(\\beta)$是我们的目标函数，则 \\begin{aligned} J(\\beta) &= -\\frac{1}{n}\\ell(\\beta) \\\\ &=-\\frac{1}{n}\\sum_{i=1}^ny_iln(p(x_i))+(1-y_i)ln(1-p(x_i)) \\end{aligned}此时我们的梯度公式就变成了： \\frac{\\partial J(\\beta)}{\\partial \\beta}=-\\frac{1}{n}\\sum _{i=1}^n(y_i-p_i)x_i=\\frac{1}{n}\\sum _{i=1}^n(p_i-y_i)x_i我们的二阶偏导数就变成了 \\frac{\\partial ^2J(\\beta)}{\\partial \\beta\\partial\\beta^T} =\\frac{1}{n} \\sum_{i=1}^nx_ix_i^Tp_i(1-p_i)那么，此时为了求得我们的回归系数，即求使得$J(\\beta)$最小的系数。牛顿迭代公式就变成了： \\hat{\\beta}^{new} = \\hat{\\beta}^{old}-H^{-1}\\nabla=\\hat{\\beta}^{old}-\\frac{1}{n}(X^T\\cdot diag(p)\\cdot diag(1-p) \\cdot X)^{-1}\\cdot \\frac{1}{n}X^T(p-y)按照上述思路，编程实现逻辑回归求解是比较简单的。 #迭代函数 from numpy import * def sigmoid(x): return 1.0/(1.0+exp(-x)) def LogitReg(x,y,tol = 0.001,maxiter = 1000): samples,features = x.shape #分别表示观测样本数量和特征数量 features += 1 #全部转换为矩阵 xdata = array(ones((samples,features))) xdata[:,0] = 1 xdata[:,1:] = x xdata = mat(xdata) #sample行，features列的输入 y = mat(y.reshape(samples,1)) #label，一个长度为samples的向量 #首先初始化beta，令所有的系数为1,生成一个长度为features的列向量 beta = mat(zeros((features,1))) iternum = 0 #迭代计数器 #计算初始损失 loss0 = float('inf') J = [] while iternum < maxiter: try: p = sigmoid(xdata*beta) #计算似然概率 nabla = 1.0/samples*xdata.T*(p-y) #计算梯度 H = 1.0/samples*xdata.T*diag(p.getA1())* diag((1-p).getA1())*xdata #计算黑塞矩阵 loss = 1.0/samples*sum(-y.getA1()*log(p.getA1())-(1-y).getA1()*log((1-p).getA1())) #计算损失 J.append(loss) beta =beta - H.I * nabla #更新参数 iternum += 1 #迭代器加一 if loss0 - loss < tol: break loss0 = loss except: H = H + 0.0001 #通常当黑塞矩阵奇异的时候，将矩阵加上一个很小的常数。 break return beta,J #预测函数 def predictLR(data,beta): data = array(data) if len(data.shape) == 1: length = len(data) newdata = tile(0,length+1) newdata[0] = 1 newdata[1:] = data newdata = mat(newdata) pass else: shape = data.shape newdata = zeros((shape[0],shape[1]+1)) newdata[:,0] = 1 newdata[:,1:] = data newdata = mat(newdata) return sigmoid(newdata*beta) df = pd.read_csv('df.csv',header=None) df = array(df) df.shape xdata = df[:,:3] ydata = df[:,3] beta,J = LogitReg(xdata,ydata) #拟合 testdata = xdata[1:10,] predictLR(testdata,beta) matrix([[ 0.4959212 ], [ 0.44642627], [ 0.47419207], [ 0.42209742], [ 0.41802565], [ 0.51283217], [ 0.44833226], [ 0.41252982], [ 0.47853786]]) http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf [1]周志华.机器学习[M].清华大学出版社,2016. [2]李航著.统计学习方法[M].清华大学出版社,2012.","link":"/2018/05/03/逻辑回归/"},{"title":"集成学习","text":"个体与集成 Hoeffding不等式：给定m个取值在[0,1]区间的独立随机变量$x_1,x_2,\\cdots,x_n$，对任意$\\epsilon &gt; 0$有如下等式成立： P(|\\frac{1}{m}\\sum _{1=1}^mx_i-\\frac{1}{m}E(x_i)|\\ge \\epsilon) \\le 2e^{-2m\\epsilon^2} Boosting提升（boosting）方法是一种常用的机器学习方法，应用广泛且有效。在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。 提升方法的基本思路提升方法基于这样一种思想：对于一个复杂的任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好。实际上，就是“三个臭皮匠赛过诸葛亮”的道理。 历史上，Kearns和Valiant首先提出了“强可学习（strong learnable）”和“弱可学习（weakly learnable）”的概念。指出：在概率近似正确学习的框架中，一个概念（一个类），如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的；一个概念，如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。非常有趣的是Schapire后来证明强可学习与弱可学习是等价的，也就是说，在PAC学习的框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。 这样一来，问题便成为，在学习中，如果已经发现了“弱学习算法”，那么能否将它提升（boost）为“强学习算法”。大家知道，发现弱学习算法比发现强学习算法容易得多。那么如何具体实施提升，便成为开发提升方法时所要解决的问题。关于提升方法的研究很多，有很多算法被提出。最具有代表性的是AdaBoost算法。 对于分类问题而言，给定一个训练样本集，求比较粗糙的分类规则（弱分类器）要比求精确的分类规则（强分类器）容易的多。提升方法就是从弱学习算法出发，反复学习，得到一系列弱分类器，然后组合这些弱分类器，构成一个强分类器。大多数的提升方法都是改变训练数据的概率分布（训练数据的权值分布），针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。 这样，对提升方法来说，有两个问题需要回答：一是在每一轮如何改变数据的权值或概率分布；二是如何将弱分类器组合为一个强分类器。关于第1个问题，AdaBoost的做法是，提高那些被前一轮弱分类器分错的样本权值，而降低那些被正确分类样本的权值。这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注。于是，分类问题被一系列弱分类器“分而治之”。至于第2个问题，即弱分类器的组合，AdaBoost采取加权多数表决的方法。具体地，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。 AdaBoost的巧妙之处在于它将这些想法自然且有效地实现在一种算法里。 AdaBoost现在叙述AdaBoost算法。假设给定一个二分类的训练数据集 T=\\{(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)\\}其中，每个样本点由实例与标记组成。实例$x_i\\in R^n$,标记$y_i\\in \\{-1,1\\}$。AdaBoost利用以下算法，从训练数据中学习一系列弱分类器或基本分类器，并将这些弱分类器线性组合为一个强分类器。 输入：训练数据集$T=\\{(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)\\}$,其中$x_i\\in R^n$,标记$y_i\\in \\{-1,1\\}$；弱分类算法。 输出：最终分类器$G(x)$。 初始化训练数据的权值分布， D_1=(w_{11},\\cdots,w_{1i},\\cdots,w_{1N}),w_{1i}=\\frac{1}{N},i=1,2,\\cdots,N 对$m=1,2,\\cdots,M$ 使用具有权值分布$D_m$的训练数据集学习，得到基本分类器 G_m(x):R^n \\rightarrow \\{-1,1\\} 计算$G_m(x)$在训练数据上的分类误差率 e_m=\\sum _{i=1}^NP(G_m(x)\\ne y_i)=\\sum _{i=1}^Nw_{mi}I(G_m(x)\\ne y_i) 计算$G_m(x)$的系数 \\alpha _m = \\frac{1}{2} ln \\frac{1-e_m}{e_m} 更新训练数据集的权值分布 D_{m+1} = (w_{m+1,1},\\cdots,w_{m+1,i},\\cdots,w_{m+1,N})w_{m+1,i} = \\frac{w_{mi}}{Z_m}e^{-\\alpha _m y_i G_m(x_i)},i=1,2,\\cdots,N 这里，$Z_m$是规范化因子 Z_m = \\sum _{i=1}^N e^{-\\alpha _m y_i G_m(x_i)}它使$D_{m+1}$成为一个概率分布。 构建基本分类器的线性组合 f(x) = \\sum _{m=1}^M \\alpha_m G_m(x)得到最终的分类器 G(X) = sign(f(x)) = sign(\\sum _{m=1}^M \\alpha_m G_m(x))对AdaBoost算法作如下说明： 步骤1 假设数据集具有均匀的权值分布，即每个训练样本在基本分类器中作用相同，这一假设保证第1步能够在原始数据集上学习基本分类器$G_1(x)$。 步骤2 AdaBoost反复学习基本分类器，在每一轮$m=1,2,\\cdots,M$中顺次地执行下列操作： （a）使用当前分布$D_m$加权的训练数据集，学习基本分类器$G_m(x)$。 （b）计算基本分类器$G_m(x)$在加权训练数据集上的分类误差率： e_m = \\sum _{i=1}^N P(G_m(x_i)\\ne y_i) = \\sum _{G_m(x_i)\\ne y_i} w_{mi}这里，$w_{mi}$表示第$m$轮中第$i$个实例的权值，$\\sum _{i=1}^N w_{mi}=1$。这表明，$G_m(x)$在加权的训练数据上的分类误差率是被$G_m(x)$误分类样本的权值之和，由此可以看出数据权值分布$D_m$与基本分类器$G_m(x)$的分类误差率的关系。 （c）计算基本分类器$G_m(x)$的系数$\\alpha _m$，$\\alpha _m$表示$G_m(x)$在最终分类器中的重要性。由(2)可知，当$e_m\\le \\frac{1}{2}$时，$\\alpha_m \\ge 0$，并且$\\alpha_m$随着$e_m$的减小而增大，所以分类误差率越小的基本分类器在最终分类器中的所用越大。 （d）更新训练数据的权值分布为下一轮作准备，式(4)可以写成： w_{m+1,i}=\\left\\{\\begin{aligned}\\frac{w_{mi}}{Z_m}e^{-\\alpha_m} & & G_m(x_i)=y_i \\\\\\frac{w_{mi}}{Z_m}e^{\\alpha_m} & & G_m(x_i) \\ne y_i \\end{aligned}\\right.由此可知，被基本分类器$G_m(x)$误分类样本的权值得以扩大，而被正确分类样本的权值却得以缩小。两相比较，由式(2)知误分类样本的权值被放大$e^{2\\alpha_m}=\\frac{1-e_m}{e_m}$倍。因此，误分类样本在下一轮学习中起更大的作用。不改变所给的训练数据，而不断改变训练数据的权值分布，使得训练数据在基本分类器的学习中起到不同的作用，这是AdaBoost的一个特点。 步骤3 线性组合$f(x)$实现$M$个基本分类器的加权表决。系数$\\alpha_m$表示了基本分类器$G_m(x)$的重要性，这里，所有$\\alpha_m$之和并不为1.$f(x)$的符号决定实例$x$的分类，$f(x)$的绝对值表示分类的确信度。利用基本分类器的线性组合构建最终分类器是AdaBoost的另一个特点。 GBDTXGBoostXGBoost是”极端梯度上升”(Extreme Gradient Boosting)的简称,从技术上说，XGBoost是Extreme Gradient Boosting的缩写。它的流行源于在著名的Kaggle数据科学竞赛上被称为”奥托分类”的挑战。它可以处理多种目标函数，包括回归，分类和排序，是一个较为全面的分类器。由于其他许多分类器，不管是强分类器或是集成分类器，在预测性能上的强大但是相对缓慢的实现，如上一章的Adaboost集成算法，不管是在在运行时间上还是在内存占有上开销都很大。XGBoost成为很多比赛的理想选择。XGBoost包还添加了做交叉验证和发现关键变量的额外功能。在优化模型时，这个算法使用了好几个参数。所以为了提高模型的表现，参数的调整十分必要。本节将讨论这些因素。 XGBoost优势XGBoost算法总结起来大致其有三个优点：高效、准确度、模型的交互性。 正则化：标准GBDT提升树算法的实现没有像XGBoost这样的正则化步骤。正则化用于控制模型的复杂度，对减少过拟合也是有帮助的。XGBoost也正是以“正则化提升”技术而闻名。 并行处理：XGBoost可以实现并行处理，相比GBM有了速度的飞跃。不过，需要注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。XGBoost的并行是在特征粒度上的。决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点）。因此XGBoost在R重定义了一个自己数据矩阵类DMatrix。XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复利用索引地使用这个结构，获得每个节点的梯度，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。 高度灵活性：XGBoost允许用户定义自定义优化目标和评价标准，它对模型增加了一个全新的维度，所以我们的处理不会受到任何限制。 缺失值处理：XGBoost内置处理缺失值的规则。 用户需要提供一个和其它样本不同的值，然后把它作为一个参数传进去，以此来作为缺失值的取值。XGBoost在不同节点遇到缺失值时采用不同的处理方法，并且会学习未来遇到缺失值时的处理方法。 剪枝：当分裂时遇到一个负损失时，传统GBDT会停止分裂。因此传统GBDT实际上是一个贪心算法。XGBoost会一直分裂到指定的最大深度(max_depth)，然后回过头来剪枝。如果某个节点之后不再有正值，它会去除这个分裂。这种做法的优点，当一个负损失（如-2）后面有个正损失（如+10）的时候，就显现出来了。GBM会在-2处停下来，因为它遇到了一个负值。但是XGBoost会继续分裂，然后发现这两个分裂综合起来会得到+8，因此会保留这两个分裂。 内置交叉验证：XGBoost允许在每一轮boosting迭代中使用交叉验证。因此，可以方便地获得最优boosting迭代次数。而传统的GBDT使用网格搜索，只能检测有限个值。 XGBoost算法推导XGBoost 在函数空间中用牛顿法进行优化。首先，boosting是一种加法模型。XGBoost同样属于GBDT梯度提升法，模型的基分类器都包含有树，对于给定的数据集D={($x_i​$,$y_i​$)}，XGBoost进行additive learning，学习K棵树，采用以下函数对样本进行预测。 \\widehat{y}=\\phi(x_i)=\\sum_{k=1}^{K}f_{k}(x_i)\\qquad f_k\\in F这里F是函数空间，$f(x)$是回归树CART。 F=\\{f(x)=w_{q(x)}\\}(q:R^m\\to T,w\\in R)$q(x)$标识将样本x分到了某个叶子节点上，$w$是叶子节点的分数，（leaf score），所以$w_q(x)$ 表示回归树对样本的预测值。回归树的预测输出是实数分数，可以用于回归，分类，排序等任务中，对于回归问题，可以直接作为目标值，对于分类问题，需要映射成概率，比如采用逻辑函数，然后可以控制阈值，进行两种分类错误的把控。XGBoost对传统的提升树算法Adaboost等的改进，在于在参数空间的目标函数中加入了正则化项，来惩罚模型的复杂程度，进而控制过拟合。和Adaboost一样都是通过最小化损失函数求解最优模型，并加入了阈值，如下公式所示： L(\\phi)=\\sum_{i}l(\\widehat{y}_l,y_i)+\\sum_{k}\\Omega(f_k)误差函数可以是square loss，logloss等，也可以自己定义损失函数，只要能够求出目标函数的梯度和Hessian矩阵，用户就可以自定义训练模型时的目标函数 这也正是XGBoost的优势之一，可以通过研究目的的不同自己定义损失函数，在公式（3-3）中，相比于原始的GBDT，XGBoost的目标函数多了正则项，是学习出来的模型更加不容易过拟合。衡量树的复杂程度主要与树的深度，内部节点的个数，叶子节点的个数，叶子节点的权重有关，因此XGBoost对这些参量进行了约束。得出了正则项为： \\Omega(f)=\\gamma T+\\frac{1}{2}\\rho\\|w\\|^2正则项对每棵树的复杂程度都应进行惩罚，对每个节点进行了复杂度的惩罚，从另一种角度来说也就进行了自动的剪枝。另外，还可以选择使用线性模型替代树模型，从而得到带$L1+L2$惩罚的线性回归正则项可以是$L1$正则，$L2$正则 。第$t$次迭代后，模型的预测等于前$t-1$次的模型预测加上第t棵树的预测。将目标函数在前$t-1$次的模型$y_i^{t-1}$处进行泰勒展开，并将常数项去掉即得到： L^{(t)}=\\sum_{i=1}^{n}[g_if_t(x_i)+\\frac{1}{2}h_if_t^2(x_i)]+\\Omega(f_t)公式中，$g_i=\\delta_{\\widehat{y}^{(t-1)}}l(y_i,\\widehat{y}^{(t-1)})\\qquad h_i=\\delta^2_{\\widehat{y}^{(t-1)}}l(y_i,\\widehat{y}^{(t-1)})$把$f_t$, $\\Omega(f_t)$写成树结构的形式，得到： L^{(t)}=\\sum_{i=1}^{n}[g_iw_{q(x_i)}+\\frac{1}{2}h_iw^2_{q(x_i)}]+\\gamma T+\\rho\\frac{1}{2}\\sum_{j=i}^{T}w_j^2则目标函数可以写成按叶节点累加的形式： L^{(t)}=\\sum_{j=1}^{T}[G_iw_j+\\frac{1}{2}(H_j+\\rho)w_j^2]+\\gamma T如果确定了树的结构（即$q(x)$确定），为了使目标函数最小，可以令其导数为0，解得每个叶节点的最优预测分数为： W_j^*=-\\frac{G_j}{H_j+\\rho}\\widehat{L}^*=-\\frac{1}{2}\\sum_{j=1}^{T}\\frac{G_j^2}{H_j+\\rho}+\\gamma T公式（3-9）的负部衡量了每个叶子节点对总体损失的的贡献，我们希望损失越小越好，则公式的（3-9）的负部值越大越好。因此，对一个叶子节点进行分裂，分裂前后的增益定义为： Gain=\\frac{G_L^2}{H_L+\\rho}+\\frac{G_R^2}{H_R+\\rho}-\\frac{(G_L+G_R)^2}{H_L+H_R+\\rho}-\\gammaGain的值越大，分裂后L减小越多。所以当对一个叶节点分割时，计算所有候选特征值所对应的gain，选取gain最大的进行分割。但由于精确遍历所有可能的分割点是效率很低的，所以，实际上XGBoost采用的是对于每个特征，不是简单地按照样本个数进行分位，而是以二阶导数值作为权重，进行分位点的选择，以此减少计算复杂度。在学习每棵树前，提出候选切分点，这也是XGBoost可以实现并行的原因之一，可以提前切割分位点。 最后，XGBoost算法还借鉴了bagging的bootstrap自助法行抽样，还借鉴了随机森林的列抽样，即特征抽样。这样减少过拟合同时还降低了计算复杂度。 Bagging &amp; 随机森林","link":"/2018/04/04/集成学习/"},{"title":"改进的删一法交叉验证与逐步回归比较","text":"定义基本函数下面主要定义cholesky分解函数，以及求解线性方程组的两个函数和标准化函数。 代码分割线 ## mchol函数将对称方阵分解为一个下三角矩阵乘以该矩阵转置的形式, #函数返回值为下三角矩阵 #输入：欲分解的矩阵x #输出：cholesky分解所得矩阵L mchol","link":"/2018/04/26/岭回归/"},{"title":"线性回归与梯度下降算法","text":"【摘要】 本文以线性回归为例，讲解了批量梯度下降、随机梯度下降、小批量梯度下降、冲量梯度下降等算法，由浅入深，并结合精心设计的例子，使读者最快掌握这种最常用的优化方法。每一种优化方法，笔者都基于R语言给出了相应的代码，供读者参考， 梯度下降假如我们有以下身高和体重的数据，我们希望用身高来预测体重。如果你学过统计，那么很自然地就能想到建立一个线性回归模型： y=a+bx其中$a$是截距，$b$是斜率，$y$是体重，$x$是身高。 我们将身高与体重的关系在Excel里面用折线图表示，并且添加了线性的趋势线。蓝色的线条是真实数据，红色的实线是模型给出的预测值。蓝色线条与红色线条之间的距离绝对值是预测误差。所以，我们要找到最优的$a$和$b$来拟合这条直线，使得我们模型的总误差最小。 Error = \\frac{1}{2}(Actual\\ weight - Predicted\\ weight)^2=\\frac{1}{2}(Y-Ypred)^2我们使用均方误差来表示模型的误差，由于$Ypred = a + bx$，因此，模型的均方误差可以表示为 SSE = \\sum \\frac{1}{2}(Y-a-bx)^2也就是说，$SSE$是关于$a$和$b$的函数，我们只需要不断调整$a$和$b$，使$SSE$降到最低就可以了。这个时候，我们就可以利用梯度下降算法，来求解$a$和$b$的值。 梯度下降的计算过程如下： step 1:随机初始化权重$a$和$b$，计算出误差$SSE$ step 2:计算梯度。 $a$和$b$的轻微变化都会导致$SSE$的变化，因此，我们只需要找到能使$SSE$减小的$a$和$b$的变化方向就可以了。这个方向，一般就是由梯度决定的。 step 3:调整权重值，使得$SSE$不断接近最小值。 step 4:使用新的权重去做预测，并且计算出新的$SSE$。 step 5:重复step2-step3，直到权重不再显著变化为止。 我们在Excel中进行上述步骤。为了计算能够快一点，我们首先对数据进行Min-Max标准化。得到如下数据： step1:随机选取一组权重(此处我们设置a=0,b=1),我们计算出预测值和误差： step2:计算梯度 \\frac{\\partial SSE}{\\partial a} = \\sum-(Y-a-bx)=\\sum-(Y-Ypred)\\frac{\\partial SSE}{\\partial b}=\\sum-(Y-a-bx)x=\\sum-(Y-Ypred)x$\\frac{\\partial SSE}{\\partial a}$和$\\frac{\\partial SSE}{\\partial b}$就是梯度，他们决定了$a$和$b$的移动方向和距离。 step3: 调整权重值，使得$SSE$不断接近最小值。 调整规则为: a_{new} = a_{old} - \\eta \\nabla a = a_{old} - \\eta \\cdot \\partial SSE/\\partial ab_{new} = b_{old} - \\eta \\nabla b = b_{old} - \\eta \\cdot \\partial SSE / \\partial b其中，$\\eta$是一个被我们称之为学习率(learning rate)的东西，一般设置为0.01或者你希望的任何比较小的数值。 本文选择0.01作为学习率。 a_{new} = 0 - 0.01 \\times 1.925 = -0.01925b_{new} = 1 - 0.01 \\times 1.117 = 0.98883step4:使用新的权重去做预测，并且计算出新的$SSE$。 可以看出，SSE从0.155降低到0.111，说明系数有改善。 step 5:重复step2-step3，直到权重不再显著变化为止。 我们知道，一元线性回归的系数可以用公式计算，我们用R的lm()函数来计算权重，结果为 lm(y~x,dat) Call: lm(formula = y ~ x, data = dat) Coefficients: (Intercept) x -0.1167 0.9777 然后，我在R里面写了一个梯度下降的函数，当精度调到0.0000001的时候，与lm的结果已经很接近了。 gradientDescent","link":"/2018/05/05/梯度下降/"},{"title":"Word2Vec-语言模型的前世今生","text":"引言 在机器学习领域，语言识别和图像识别都比较容易做到。语音识别的输入数据可以是音频频谱序列向量所构成的matrix，图像识别的输入数据是像素点向量构成的矩阵。但是文本是一种抽象的东西，显然不能直接把文本数据喂给机器当做输入，因此这里就需要对文本数据进行处理。 现在，有这么一个有趣的例子，我接下来要讲的模型就可以做到。 首先给出一个例子，Paris - France + America = ? 从我们人的角度来看,Paris是法国的首都，那么首都减去国家再加上一个国家，很可能表示的就是另一个国家的首都。因此这里的结果就是华盛顿Washington.机器想做到这一点，并不容易。 众所周知，只有标量或者向量可以应用加减法，抽象的自然语言该如何做到呢？ 一个很自然的想法就是，自然语言能否表示成数学的形式，这样就可以更加方便地研究其规律了。 答案是肯定的。 现在我们可以进行思考，如何将文本中的词语用数学的形式表达出来,也就是说，文本中藏着哪些数学形式需要我们去挖掘。 文本中各个词语出现的频数是有限的，这是一个可以提取的数学形式 从逻辑的角度出发，词语之间不可能是独立的，一个词语的出现肯定与另一个或者若干个词语有关系。这就涉及到词语共现的层面了。 统计语言模型和大多数的词向量表示都是基于以上两点考虑的。 词向量的表现形式主要分为两种，一种是one-hot(one-hot representation)表示方式，将词表示成一个很长的向量，向量的长度就是词典的长度；另一种表示方法是分布式表示(distributed representation).同时，分布式表示方法又可以分为基于矩阵的表示方法、基于聚类的表示方法和基于神经网络的表示方法。 首先，最简单的就是one-hot表示方法，将词表示成一个很长的向量，向量的分量只有一个1，其他全为0，1所对应的位置就是该词在词汇表中的索引。 这样表示有两个缺点： 容易受维度灾难(the curse of dimentionality)的困扰； 没有考虑到词之间的关系(similarity)。 现在主要应用的都是分布式表示形式了。下面介绍一种简单的分布式表示形式——基于矩阵的表示形式。 如下表所示： Probability and Ratio k = solid k = gas k = water k = fashion $p(k\\vert ice)$ $1.9\\times 10^{-4}$ $6.6\\times 10^{-5}$ $3.0\\times 10^{-3}$ $1.7\\times 10^{-5}$ $p(k\\vert steam$) $2.2\\times 10^{-5}$ $7.8\\times 10^{-4}$ $2.2\\times 10^{-3}$ $1.8\\times 10^{-5}$ $p(k\\vert ice)/(p(k\\vert steam)$ $8.9$ $8.5\\times 10^{-2}$ $1.36$ $0.96$ 简单说一下上面的矩阵。 假设我们对一些热力学短语或者词语的概念感兴趣，我们选择i=ice，k=steam，我们想看看ice和steam的关系，可以通过他们与其他词语的共现频率来研究。这些其他词语我们称之为探测词。这里我们选择探测词k为solid，gas，water和fashion。显然，ice与solid的相关性较高，但是与steam相关性较低，因此我们期望看到的是$p_{ik}/p_{jk}$比值比较大。对于探测词gas，我们期望看到的是$p_{ik}/p_{jk}$比值比较小。而water和fashion与ice和steam的关系要么都十分密切，要么都不怎么密切，因此对于这两个探测词，$p_{ik}/p_{jk}$应该接近于1. 上表是基于一个很大的语料库统计得出的，符合我们的预期。相比于单独使用原始概率，概率比值可以更好的区分相关词语和不相关词语，比如solid和gas与water和fashion；也可以很容易区分两个相关词。 那么，在正式介绍自然语言处理，或者说wrod2vec之前，有必要介绍以下统计语言模型。它是现在所有语言模型的基础。 第二个需要讲的分布式表示方式是基于神经网络的表示方法。在此之前，有必要讲一下传统的统计语言模型，毕竟它对语言模型影响深远。 统计语言模型1. 引言给出以下三个句子： 美联储主席本·伯南克昨天告诉媒体7000亿美元的救助资金将借给上百家银行、保险公司和汽车公司 美主席联储本·伯南克告诉昨天媒体7000亿美元的资金救助将借给百上家银行、保险公司和汽公车司 美主车席联储本·克告诉昨天公司媒体7000伯南亿美行元的金将借给百救助上家资银、保险公司和汽 对于第一个句子，语句通畅，意思也很不明白；对于第二个句子，虽然个别词语调换了位置，但也不影响阅读，我们仍然能够知道表达的是什么意思；对于第三个句子，我们就很难知道具体表示什么意思了。 如果问你为什么第三个句子不知道表达什么，你可能会说句子混乱，语义不清晰。在上个世纪70年代的时候，科学家们也是这样想的，并且试图让计算机去判断一个句子的语义是否清晰，然而，这样的方法是走不通的。 贾里尼克想到了一种很好的统计模型来解决上述问题。判断一个句子是否合理，只需要看它在所有句子中出现的概率就行了。第一个句子出现的概率大概是$10^{-20}$,第二个句子出现的概率大概是$10^{-25}$，第三个句子出现的概率大概是$10^{-70}$，第一个句子出现的可能性最大，因此这个句子最为合理。 那么，如何计算一个句子出现的概率呢，我们可以把有史以来人类说过的话都统计一遍，这样就能很方便的计算概率了。然而，你我都知道这条路走不通。 假设想知道S在文本中出现的可能性，也就是数学上所说的S的概率，既然$S=w_1,w_2,…,w_n$,那么不妨把S展开表示， P(S)=P(w_1,w_2,...,w_n)利用条件概率的公式，S这个序列出现的概率等于每一个词出现的条件概率的乘乘积，展开为： P(w_1,w_2,...,w_n)=P(w_1)P(w_2\\vert w_1)P(w_3\\vert w_1,w_2)\\cdots P(w_n\\vert w_1,w_2,\\cdots w_{n-1})计算$P(w_1)$很容易，$P(w_2\\vert w_1)$也还能算出来，$P(w_3\\vert w_1,w_2)$已经非常难以计算了。 2. 偷懒的马尔科夫(Andrey Markov)假设上面的n不取很长，而只取2个，那么就可以大大减少计算量。即在此时，假设一个词$w_i$出现的概率只与它前面的$w_{i-1}$有关，这种假设称为1阶马尔科夫假设。 现在，S的概率就变得简单了： P(w_1,w_2,...,w_n) \\approx P(w_1)P(w_2\\vert w_1)那么，接下来的问题就变成了估计条件概率$P(w_i\\vert w_{i-1})$,根据它的定义， P(w_i\\vert w_{i-1}) = \\frac{P(w_i,w_{i-1})}{P(w_{i-1})}, 当样本量很大的时候，基于大数定律，一个短语或者词语出现的概率可以用其频率来表示，即 P(w_i,w_{i-1})\\approx \\frac{count(w_i,w_{i-1})}{count(*)} P(w_{i-1}) \\approx \\frac{count(w_{i-1})}{count(*)}其中，$count(i)$表示词$i$出现的次数，$count$表示语料库的大小。 那么 P(w_i\\vert w_{i-1}) = \\frac{P(w_i,w_{i-1})}{P(w_{i-1})} \\approx \\frac{count(w_i,w_{i-1})}{count(w_{i-1})}3. 高阶语言模型在前面的模型中，每个词只与前面1个词有关，和更前面的词就没有关系了，这似乎简单的有点过头了。那么，假定每个词$w_i$都与前面的N-1个词有关，而与更前面的词无关，这样，当前词的概率只取决于前面N-1个词的联合概率，即 P(w_1\\vert w_1,w_2,\\cdots w_{i-1}) \\approx P(w_1\\vert w_{i-N+1},w_{i-N+2},\\cdots w_{i-1}), 上面这种假设被称为n-1阶马尔科夫假设，对应的模型称为N元模型。N=2就是二元模型，N=1其实就是上下文无关的模型，基本不怎么使用。 上面的模型看起来已经很完美了，但是考虑以下两个问题，对于二元模型： 如果此时$count(w_i,w_{i-1})=0$，是否可以说$P(w_i\\vert w_{i-1})=0$ ? 如果此时$count(w_i,w_{i-1})=count(w_{i-1})$，是否可以说$P(w_i\\vert w_{i-1})=1$ ? 显然，不能这么武断。 但是，实际上上述两种情况肯定是会出现的，尤其是语料足够大的时候，那么，我们怎么解决上述问题呢？ 古德和图灵给出了一个很漂亮的重新估计概率的公式，这个公式后来被称为古德-图灵估计。 古德图灵的原理是： 对于没有看见的事件，我们不能认为他发生的概率就是0，因此从概率的总量中，分配一个很小的比例给这些没有看见的事件。这样一来，看见的那些事件的概率就要小于1了，因此，需要将所有看见的事件的概率调小一点。至于小多少，要根据“越是不可信的统计折扣越多”的方法进行。 假定在语料库中出现$r$次的词有$N_r$个，特别的，未出现的词数量为$N_0$，语料库大小为$N$，那么，很显然， N = \\sum _{r=1}^ \\infty rN_r, 出现$r$次的词在整个语料库中的相对频度则是${rN_r}/{N}$，如果不做任何处理，这个相对频度作为这些词的概率。但是当$r$比较小的时候，统计上可能不可靠，因此需要使用一个更小的次数$d_r$来表示，古德-图灵按照如下公式计算$d_r$: d_r=(r+1)\\cdot N_{r+1}/N_r, 显然 \\sum_r d_r\\cdot N_r = \\sum_r (r+1)\\cdot N_{r+1} = N, 此时， d_0 = (0+1)\\cdot N_1/N_0 = \\frac{N_1}{N_0} > 0在实际处理的时候，一般对出现次数超过某个阈值的词，频率不下调。 基于这种思想，估计二元模型概率的公式如下： P(w_i\\vert w_{i-1})= \\begin{cases} f(wi\\vert w_{i-1}) & {if\\quad count(w_{i-1},w_i)} \\ge T \\\\ f_{gt}(w_i\\vert w_{i-1})& if\\quad 0 \\le count(w_{i-1},w_i) < T \\\\ Q(w_{i-1})\\cdot f(w_i) & otherwise \\end{cases}, 其中, $f(\\cdot)$ 表示相对频度，即频率。 Q(w_{i-1})=\\frac{1-\\sum_{w_i\\quad seen}P(w_i \\vert w_{i-1})}{\\sum _{wi\\quad unseen}f(w_i)}上面这种方法称为卡茨退避法。 n-gram模型的作用就是，基于语料库计算出各种词串出现的概率，遇到一个句子的时候，可以直接使用上面所计算的概率，把所有的概率连乘，就得到了整个句子的概率。 4. 机器学习的思想机器学习的套路是，对所研究的问题建模，构造一个目标函数，然后优化参数，最后用这个目标函数进行预测。 对于统计语言模型，常使用最大对数似然作为目标函数，即 L = \\sum_{w\\in C}logP(w\\vert context(w)), 在n-gram模型中，$context(w_i) = (w_{i-n+1},w_{i-n+2},\\cdots,w_{i-1})$, 由此可见，概率$P(w_i\\vert context(w_i))$是关于$w$和$context(w)$的函数，即 P(w\\vert context(w)) = F(w,context(w),\\theta), 一旦$F$确定下来了，任何概率都可以使用这个函数进行计算了。 似乎到这里，我们仍然不知道词向量是什么，因为n-gram模型中根本没有用到词向量。那么，接下来将要介绍的神经网络语言模型则是实实在在用到了词向量。之所以提到统计语言模型，是因为它是其他语言模型的基础，我们得知道语言模型是干嘛的，然后再对语言模型进行展开。 神经网络语言模型上面介绍的n-gram模型相信我们已经十分清楚了，但是n-gram模型的一个突出的确定就是，n的设置不宜过大，n从2到3提升效果显著，但是从3-4提升的效果就没那么好了。而且随着n的增大，参数的数量是以几何形势增长的。 因此，n-gram模型只能提取某个词前面两到三个词的信息，而不能提取更多的信息了。然而很明显的是，整文本序列中，包含更前面的词能够提供比仅仅2到3个词更多的信息，这也是神经网络语言模型着重要解决的问题之一。 神经网络模型主要在以下两点上寻求更大的进步： n-gram模型没有考虑上下文中更多的词提供的信息 n-gram模型没有考虑词与此之间的相似性。 举个栗子： 如果在一个语料库中，”the cat is walking in the bedroom”出现了5000次，而”a dog is running in the room”只出现了5次，n-gram模型得出的结果是前面一个句子的可能性会比后面一个句子大得多。但是实际上，这两个句子是相似的，他们在真实的情况下出现的概率也应该是相仿的。 神经网络语言模型可以概括为以下三点： 将词汇表中的每个词表示成一个在m维空间里的实数形式的分布式特征向量 使用序列中词语的分布式特征向量来表示连接概率函数 同时学习特征向量和概率函数的参数 特征向量表示词的不同特征：每一个词都是向量空间内的一个点。特征的个数通常都比较小，比如30，60或者100，远远小于词汇表的长度。概率函数是在给定一个词前面的若干词的情况下，该词出现的条件概率。调整概率函数的参数，使得训练集的对数似然达到最大。每个词的特征向量是通过训练得到的，也可以用先验知识进行初始化。 训练集是词序列$W_1,W_2,…,W_T,W_T\\in V$,$V$是词汇表，是一个很大但是有限的集合。 目标是找到一个好的模型，使得$f(W_t,…,W_{t-n+1})=\\hat{P}(W_t|W_1^{n-1})$ 唯一的约束条件是$\\sum _{i=1}^{|V|}f(i,W_{t-1},…,W_t-n+1)=1,f&gt;0$ 我们将函数$f(W_t,…,W_{t-n+1})$分解为以下两个部分： 映射$C$，将$V$中的所有元素映射为真实的向量$C(i)\\in R^m$，$C(i)$代表词汇表中的每个词的分布式特征向量，实际上，$C$是一个由自由参数构成的$|V|\\times m$矩阵。其中$|V|$代表词汇表的大小，也就是词汇表中的词数量。 每个词的概率函数是由$C$来表示的：函数$g$将输入的词特征向量$(C(W_{t-n+1}),…,C(W_{t-1}))$映射为词$W_t$前面$n-1$个词的条件概率分布。 f(i,W_{t-1},...,W_{t-n+1})=g(i,C(W_{t-1}),...,C(W_{t-n+1}))函数$f$是由两个映射组成的($C\\&amp;g$),$C$是所有词共享的。每一个部分都与一些参数有关。 映射$C$的参数实际上就是特征向量本身，表示为一个$|V|\\times m$矩阵，矩阵的每一行代表词$i$的特征向量$C(i)$. 函数$g$的参数是$\\omega$，所有的参数就是$\\theta = (C,\\omega)$. 当寻找到使得带惩罚项的训练语料库的对数似然率最大的$\\theta$，那么训练完成。 L = \\frac{1}{T}\\sum _t log f(W_t,W_{t-1},...,W_t-n+1)+R(\\theta)$R(\\theta)$是惩罚项，只作用于神经网络的权重和矩阵$C$。自由参数的规模是$V$的线性函数，也是$n$的线性函数。 在下面的大多数试验中，神经网络只有一个隐藏层，外加一个映射层。还有一个可选的直连层。所以说实际上有两个隐藏层，但是由于影射层只是做了一个线性变换，并没有添加新的信息，因此不能视为真正的隐藏层。所以真正的隐藏层就只有一个。 从图中可以看出，最底层实际上就是一些单一的词，表示成one-hot形式，即长度为词汇表的长度。然后，每个one-hot向量分别与投影矩阵C相乘，则原来长度为$|V|$的one-hot向量，经过线性变换以后，缩短为一个长度为$m$的向量，其中m就是我们设置的特征的个数，一般在2个数量级。投影完成以后，将所有的特征向量按照顺序首尾相连，形成一个长度为$m(n-1)$的向量，以词向量作为隐藏层的输入，隐藏层的激活函数为双曲正切函数$tanh$。输出层接受隐藏层的输出作为输入，经过一个softmax函数进行转换，得到最终的输出P. 即 \\hat{P}(w_t\\vert w_{t-1},...,w_{t-n+1})=\\frac{e^{y_{w_t}}}{\\sum_i e^{y_i}}, 其中 y = b+Wx + Utanh(d+Hx)双曲正切函数逐个应用于隐藏层的各个单元。当没有直连的时候，$W=0$，$x$是首尾相连的特征向量： x = (C(w_{t-1}),C(w_{t-2}),...,C(w_{t-n+1}))令$h$是隐藏层的单元数，$m$是特征向量的长度，当没有直连的时候，$W=0$。那么，所有的自由参数就是： 输出层的偏置$b$，长度为|V| 隐藏层的偏置$d$，长度为$h$ 隐藏层到输出层的权重矩阵$U$,是一个$|V|\\times h$矩阵 词向量到输出层的权重矩阵$W$，是一个$|V|\\times (n-1)m$矩阵 隐藏层权重$H$，是一个$h\\times (n-1)m$矩阵 特征矩阵$C$，是一个$|V|\\times m$矩阵 此时的输出$y$实际上是一个长度为$|V|$的向量，那么分量$y_{wt}$不能表示给出前(n-1)个词的情况下$w_t$的概率，因此需要进行一次归一化。 那么，我们所有的参数如下： \\theta = (b,d,W,U,H,C)主要的计算量都集中在隐藏层到输出层的以及输出层的归一化计算。 使用随机梯度下降进行参数求解： \\theta \\leftarrow \\theta + \\epsilon\\frac{\\partial log \\hat{P}(w_t\\vert w_{t-1},w_{t-2},w_{t-n+1})}{\\partial \\theta}其中，$\\epsilon$是学习率。 当训练结束以后，矩阵$C$就是我们需要的词向量，每一行代表该位置的词的向量。得到了词向量，就可以进行许多有趣的分析。 比如： 文本聚类 计算文本相似度 其他应用 Skip-Gram 模型基本版的skip-gram模型是十分简单的。我们将会训练一个只有一个隐藏层的简单的神经网络来完成我们的任务，但是当神经网络训练完成以后，我们实际上并不使用这个网络做什么，而是获得隐藏层的权重矩阵，这个矩阵实际上就是我们需要的词向量(word vectors) skip-gram模型需要完成这样的工作：给定一个词，预测其周围的词，或者说在其附近的词。这个神经网络将会计算出我们从词汇表中选出的每个“候选”邻居词的概率。 这里的附近需要说明一下，实际上在算法里有一个”window size”参数，用来控制窗口大小，如果选择参数的值为5，则会预测该词前后各5个词的概率。 skip-gram模型的输入需要经过特殊的调整，不同于上述神经网络语言模型，应该首先将这里的语料整理成词对(word-pair)。 比如对于语句”The quick brown fox jumps over the lazy dog.”此处我们选择window size = 2. 具体操作如图所示： 对于单词”The”，取其前后两个词与其凑成词对，这里”The”前面没有单词，因此取后面两个，凑成两个词对，分别是(the,quick),(the,brown)。 对于单词quick，其前面有1个单词，后面有2个单词，可以凑成3对，分别为：(quick,the),(quick,brown),(quick,fox)。 对于单词brown，其前面有两个单词，后面有两个单词，可以凑成4对，分别是：(brown,the),(brown,quick),(browm,fox),(brown,fox)。 以此类推，可以把语料库中的所有文本调整成上述词对。 所有的词对都应该是(input,output)形式。 有了词对，接下来看一下skip-gram的最简单的模型长什么样子。 在上图中，可以清晰地看出，skip-gram模型是一个简单的神经网络，有一个隐藏层（实际上在作者的论文中是以投影层的形式表述的），该隐藏层并没有对应的激活函数。 输入数据仍然是one-hot向量，向量只有一个分量是1，其他全为0.向量的长度为词汇表的长度。 对下面所要用的符号进行说明： $i$：输入的one-hot向量，长度为|V| $|V|$：词汇表的长度 $P$：输入层到投影层的$|V|\\times m$权重矩阵 $W$：投影层到输出层的$m\\times |V|$权重矩阵 $y$：输出层输出结果 由于这个神经网络没有激活函数，因此看起来比较简单。 神经网络的计算过程如下： 首先，将输入词向量(one-hot)投影到隐藏层(投影层)，即i^T\\times P=1\\times |V|\\times |V| \\times m=1\\times m 将隐藏层的结果乘以隐藏层到输出层的权重矩阵$W$，即1\\times m \\times m\\times |V|=1\\times |V| 将输出层的输出结果进行softmax归一化，即 y=\\frac{e^{y}}{\\sum_{i\\in y}e^i }此时$y$是一个$1\\times |V|$向量，向量的每个分量代表给定词$w_i$的情况下，其相邻词是词汇表中对应词的概率。将$y$与output的one-hot向量相乘，就可以得到给定$w_i$的情况下，其相邻词是output的概率，即 P(w_o\\vert w_i)=y\\times w_o, 其中，$w_o$是词output的向量形式。 以上矩阵相乘只是为了说明维度变化，并不是实际上的矩阵乘法。 用矩阵说明如下： 假定我们的词库大小为10，输出层到投影层的权重矩阵为$10\\times 4$矩阵。 我们的输入样本是词对(brown,quick).其中，brown的one-hot表示形式如图中的蓝色向量所示，quick的one-hot表示形式如图中的浅绿色向量所示。 蓝色向量是我们的输入向量，即one-hot向量 黄色矩阵是输入层到投影层的权重矩阵 绿色向量是输入向量与第一个权重矩阵的向量乘积 棕色矩阵是投影层到输出层的权重矩阵 灰色向量是绿色向量与棕色矩阵的矩阵乘积 红色向量是对灰色向量进行了一次$softmax$归一化计算 浅绿色矩阵是词对中的输出词对应的one-hot向量 深红色数值是我们计算出的输出词对应的概率 skip-gram的目标是对于训练样本$w_1,w_2,\\cdots,w_T$,最大化如下平均对数似然概率： \\frac{1}{T}\\sum _{t=1}^T\\sum_{-c\\le i\\le c,j\\ne 0}log p(w_{t+j}\\vert w_t)实际上，以上模型是难以实现的，因为计算 $\\nabla logp(w_O\\vert w_I)$的代价随着$W$的增大而增大(W表示词汇表的长度),经常达到$10^5-10^7$数量级。 Hierarchical Softmax哈夫曼树哈夫曼树是一种最优二叉树，它是这样定义的： 给定n个权值作为n个叶子结点，构造一棵二叉树，若带权路径长度达到最小，称这样的二叉树为最优二叉树。 哈夫曼树的构造方法如下： (1) 将$w_1,w_2,\\cdots,w_n$看成是有n 棵树的森林(每棵树仅有一个结点)； (2) 在森林中选出两个根结点的权值最小的树合并，作为一棵新树的左、右子树，且新树的根结点权值为其左、右子树根结点权值之和； (3)从森林中删除选取的两棵树，并将新树加入森林； (4)重复(2)、(3)步，直到森林中只剩一棵树为止，该树即为所求得的哈夫曼树 假设我们的权值为：26 24 15 10 17 18 10 27 首先对上面的权值按照从小到大排序： 排序后的权值为：10 10 15 17 18 24 26 27 然后。我们按照上述步骤构建哈夫曼树。 此处我们约定将大的权值放在左子树。 —-Begin 选择最小的两个权值10，10，合并成一个新树的左右子树。新树的权值为20，删除合并的两个权值，将20加入到森林，此时的权值为（仍然进行排序）：15 17 18 20 24 26 27 选择最小的15和17合并，删除15和17，将32加入到森林，此时的权值为：18 20 24 26 27 32 选择最小的18和20合并，删除18和20，将38加入到森林，此时的权值为：24 26 27 32 38 选择最小的24和26合并，删除24和26，将20加入到森林，此时的权值为：27 32 38 50 选择最小的27和32合并，删除27和32，将59加入到森林，此时的权值为：38 50 59 选择最小的38和50合并，删除38和50，将88加入到森林，此时的权值为：59 88 选择最小的59和88合并，删除59和88，将147加入到森林，此时的权值为：147 —-End 根据上述权值构造的哈夫曼树如下： 背景为黄色的是新生成的树 逻辑回归逻辑回归通常用来处理二分类问题，因变量通常只有两个可能的取值，自变量既可以是连续型变量，也可以是分类变量。 利用sigmoid函数，对于任意的样本$x=(x_1,x_2,\\cdots,x_n)^T$，可将二分类问题的h函数(hypothesis)写成如下形式： h_\\theta(x)=\\sigma(\\theta_0+\\theta_1x_1+\\cdots+\\theta_nx_n), 其中$\\theta =(\\theta_0,\\theta_1,\\cdots,\\theta_n)$为待定参数，为了简化起见，可以引入$x_0=1$将$x$扩展为$(x_0,x_1,\\cdots,x_n)$,于是，$h_\\theta$可简写为 h_\\theta(x)=\\sigma(\\theta^Tx)=\\frac{1}{1+e^{-\\theta^Tx}}sigmoid函数图像如下： 函数$h_\\theta(x)$的值有特殊的含义，它表示结果取1的概率，因此对于输入$x$,分类结果为类别1和类别0的概率分别为： P(y=1\\vert x;\\theta)=h_\\theta(x) P(y=0\\vert x;\\theta)=1-h_\\theta(x)上式也可以写成如下综合形式： P(y\\vert x;\\theta)=(h_\\theta(x))^y(1-h_\\theta(x))^{1-y}sigmoid函数具有很好的导数特征 已知 \\sigma(x) = \\frac{1}{1+e^{-x}}则 \\sigma(x)^{'}=\\frac{e^{-x}}{(1+e^{-x})^2}=\\sigma(x)(1-\\sigma(x))另外 [log\\sigma(x)]^{'}=1-\\sigma(x),[log(1-\\sigma(x))^{'}=- \\sigma(x)Hierarchical Softmaxhierarchical softmax使用一颗二叉树的叶子结点来代表输出层对应词汇表中每个词的输出结果。每个结点都代表了它的孩子结点的相关概率。这种方式定义了一种给词分配概率的随机游走解决方案。 准确地说，从根结点出发，每一个词都能以一条确定的路径抵达。对所使用的符号作如下阐述： $w$：代表词汇表中的词，用叶子结点表示 $n(w,j)$：从根结点到达某个叶子结点的路径上的第$j$个结点，根结点为第一个结点 $L(w)$：路径的长度，也就是结点的个数 因此，$n(w,1)=root,n(w,L(w))=w$ $n$：非叶子结点 $ch(n)$：非叶子结点的孩子结点 $[[x]]$：if x is true,than 1,else than -1 hierarchical softmax定义$P(w_o\\vert w_i)$如下： P(w\\vert w_i)=\\prod_{j=1}^{L(w)-1}\\sigma([[n(w,j+1)=ch(n(w,j))]]\\cdot (v^{'}_{n(w,j)})^T v_{w_i})其中 \\sigma(x)=1/(1+e^{-x})可以证明： \\sum _{i=1}^Wp(w\\vert w_i)=1那么，hierarchical softmax框架下的skip-gram结构是什么样的呢? 如上图所示，与basic版本的skip-gram模型相比，hierarchical softmax版本的模型输出层不再是线性结构，而是树形结构。 上面已经说过，线性结构的skip-gram模型的参数规模十分庞大，最大能够达到$10^7$数量级，再加上动辄10亿级别的语料库，训练代价很高，效率就特别低了。 引入了hierarchical softmax以后，输出层的维度得到了大幅降低。正如作者所说，计算$logp(w_o\\vert w_i)$和$\\nabla logp(w_o\\vert w_i)$的代价是与$L(w_o)$也就是哈夫曼树的深度成正比的。 下面假装正经地推导参数的更新过程： 假设我们的训练样本是【今天我上课迟到了，然后被老师批评了】，对应词汇表中的词汇为【今天 我 上课 迟到 了 然后 被 老师 批评】 假设在语料库中，上述词汇对应的频数如下表所示： 词汇 频率 今天 8 我 15 上课 2 迟到 1 了 20 然后 10 被 5 老师 3 批评 2 将频率看做是哈夫曼树的权值，将大的权值放在左子树，小的权值放在右子树。 绘制出的带权值的哈夫曼树如下： 说明如下： 黄色的结点表示新生成的树 $\\theta_i^w \\in R^m$，非叶子结点对应的向量 $d_1^w,d_2^w,\\cdots,d_{l(w)-1}^w \\in \\{0,1\\}$：词$w$的哈夫曼编码，由$l(w)-1$位编码构成，$d_j^w$对应对应路径中第$j$个非叶子结点的编码(0 or 1). 根据基于hierarchical softmax的skip-gram模型，投影层将输入的one-hot向量投影成$R^m$空间中的向量，即输出层接受的输入为投影层的输出，即$v_m$。 目标函数仍然是最大化对数似然概率，那么，当前的重点是条件概率函数的构造。 skip-gram模型将其定义为 P(context(w_i)\\vert w_i)=\\prod _{u\\in context(w_i)}p(u\\vert w_i)而根据hierarchical softmax的思想，$p(u\\vert w_i)$可以写出如下形式： p(u\\vert w_i)=\\prod _{j=2}^{l(w)}p(d_j^u\\vert v_m;\\theta_{j-1}^u)其中 p(d_j^u\\vert v_m;\\theta_{j-1}^u)=[\\sigma(v_m^T\\theta^u_{j-1})]^{1-d_j^u}\\cdot[1-\\sigma(v_m^T\\theta^u_{j-1})]^{d_j^u}那么，我们的目标函数可以写成如下形式： L=\\sum _{w\\in C} log \\prod _{u\\in context(w)}\\prod _{j=2}^{l(w)}[\\sigma(v_m^T\\theta^u_{j-1})]^{1-d_j^u}\\cdot[1-\\sigma(v_m^T\\theta^u_{j-1})]^{d_j^u}=\\sum _{w\\in C}\\sum _{u \\in context(w)}\\sum _{j=2}^{l(w)}\\{(1-d_j^u)\\cdot log[\\sigma(v_m^T\\theta^u_{j-1})]+d_j^u\\cdot log[1-\\sigma(v_m^T\\theta^u_{j-1})]\\}上面的函数就是skip-gram的目标函数，为了方便推导梯度，将三重求和符号下花括号的内容简记为$L(w,u,j)$，即 L(w,u,j)=(1-d_j^u)\\cdot log[\\sigma(v_m^T\\theta^u_{j-1})]+d_j^u\\cdot log[1-\\sigma(v_m^T\\theta^u_{j-1})]首先考虑$L$关于$\\theta_{j-1}^u$的梯度计算： \\frac{\\partial L}{\\partial \\theta_{j-1}^u}=\\frac{\\partial L(w,u,j)}{\\partial \\theta_{j-1}^u}=\\frac{\\partial \\{(1-d_j^u)\\cdot log[\\sigma(v_m^T\\theta^u_{j-1})]+d_j^u\\cdot log[1-\\sigma(v_m^T\\theta^u_{j-1})]\\}} {\\partial \\theta_{j-1}^u}=[1-d_j^u-\\sigma(v_m^T\\theta_{j-1}^u)]\\cdot v_m于是，$\\theta_{j-1}^u$的更新公式可写为： \\theta_{j-1}^u\\leftarrow \\theta_{j-1}^u+\\epsilon [1-d_j^u-\\sigma(v_m^T\\theta_{j-1}^u)]\\cdot v_m接下来考虑$L$对$v_m$的梯度。由于在$L$中，$v_m$与$\\theta_{j-1}^u$具有对称性，因此可以根据上述所求，直接写出$v_m$的更新公式： v_m\\leftarrow v_m+\\epsilon \\sum _{u\\in C}\\sum _{j=2}^{l(w)}\\frac{\\partial L(w,u,j)}{\\partial v_m}其中，$\\epsilon$是学习率。 Negative Sampling and Subsampling of Frequent Words考虑到basic版本的skip-gram模型拥有两个异常巨大的权重矩阵，再加上一个10亿数量级的语料库，神经网络跑起来会非常吃力。 word2vec的作者是这样处理这个问题的，主要有以下三个创新点： 将常用词对或者词组看成一个单独的词 subsampling(降采样)出现频率很高的词以减小训练样本的大小 使用一种被称为”Negative Sampling”的方法来改变优化目标，这种方法在训练时只优化与训练样本有关的很小一丢丢权重。 值的一提的是，应用Subsampling和NEG方法不仅可以减小计算压力，还能提高最终产生的词向量的质量。 Negative Sampling回想一下那个basic版本的skip-gram，神经网络的真实值或者说标签是一个one-hot矩阵，也就是说，只有一个分量是1，而其他分量都是0(成千上万个0)。但是我们在更新神经网络权重的时候，对于所有的输出为0的权重都进行了更新，这样效率是比较差的。 negative sampling的思想就是，当我们在训练一个特定样本的时候，能不能只更新几个输出为0的权重，这样计算起来就比较轻松了。 我们把one-hot向量分量为负的位置所对应的词称为negative words，在训练一次样本时，我们只更新几个(假如说5个吧)negative words，同时也更新我们的pisitive words(即分量为1所对应的位置在词汇表中对应的词)。 作者在论文中说到，当样本量比较小的时候，选择5-20个negative words效果会比较好，当样本量比较大的时候，2-5个negative words就能得到很好的效果。 现在假设我们在basic model中需要更新的权重矩阵为$300\\times 10000$矩阵，那么basic model每次迭代需要更新$3\\times 10^6$个参数，而采用了negative sampling 方法以后，我们只需要更新5个negative words和1个positive words对应的权重，也就是$300\\times (5+1)=1800$个权重，是原来的$1800/3\\times 10^6=0.06\\%$. 想法是好的，我们要怎么选择这5个negative words呢？ 我们知道，语料库中的每个词都有一定的频率，那么我们就利用频率这个信息，对negative words进行采样。 由此可见，高频词被选为negative words的概率就比较大，同理，低频词被选为negative words的概率就比较小。 作者的处理方法是，赋予每一个词被选为negative words的概率，具体计算公式如下： P(w_i)=\\frac{f(w_i)^{0.75}}{\\sum _{j=0}^nf(w_i)^{0.75}}, 至于为什么是0.75，这是作者及其团队经过不断试验得出的效果比较好的，没有特别的原因。 计算出了概率，我们又怎么选择相应的negative words呢？ 作者在其代码中给出了答案。 下面给出了具体的方法： 构造一个很长的数组,作者的数组长度达到了1000000 将词汇表中的每个词对应的index(索引)向数组中填充多次，填充的次数是这样计算的： 根据一元模型可以从语料库生成样本（也就是把语料库中的文本以词的形式展示） 填充次数times的计算公式为： times = P(w_i)\\times 100000 欲选出你的negative words，只需要在0-100M之间随机生成一个整数，以这个整数为索引，在数组中查找元素，该元素也是一个索引，根据这个元素从词汇表中查找negative words。 不难理解，$P(w_i)$大的被选中的概率就大。 Subsampling of Frequent Words让我们再看一遍词对的生成过程。 如上图所示，我们设定window size = 2 来生成样本。对于包含”the”的词对来说，有以下两个问题： 当我们在寻找词对的时候，(“fox”,”the”)所能提供的信息并不比”fox”多。然而，”the”几乎在上面的所有词对中都有出现。 形如(“the”,…)这样的词对已经远远超过了我们的需求。 word2vec 应用了一种称之为subsampling的方法来解决这个问题。 对于我们遇到每一个语料库中的词，都有一定的概率将它从语料库中删除，删除的概率与该词出现的频率有关。 如果我们将window size 设置为10，我们是这样删除”the”相关样本的： 我们在训练其他词的时候，”the”不会出现在他们的窗口 当输入词是”the”的时候，将样本数减少10个(不减少的情况下是20) 那么，我们怎么决定是否删除一个词呢？ 假设$w_i$是待定删除的词，$z(w_i)$是$w_i$在语料库中出现的频率,$P(w_i)$是保留该词的概率: P(w_i)=(\\sqrt{\\frac{z(w_i)}{0.001}}+1)\\cdot \\frac{0.001}{z(w_i)}0.001也是一个经验参数，如果比0.001还要小，那么保留词的概率就会更小。 下面是$P(w_i)$函数的图像： 从图像可以看出： 当频率等于0.0026的时候，被保留的概率为1，也就是说，当频率大于0.0026的时候，就有可能被删除。 当频率为0.0074的时候，有一半的概率会被保留 当频率为0.1的时候，被保留的概率就骤减到0.1 Learning Phraselearning phrase 方法使得样本更加接近真实世界。考虑以下句子： New York is a beautiful and modern city where I want to have a travel. 我们设置窗口大小为4，对beautiful进行采样，获得如下样本： (beautiful,New) (beautiful,York) (beautiful,is) (beautiful,a) (beautiful,New) (beautiful,and) (beautiful,modern) (beautiful,city) 采用一元的方法，会将常用的词组分开，从而降低词向量的质量。 我们可以采取一个简单的数据驱动的方法，来对两个词是否能组成词组进行打分： score(w_i,w_j) = \\frac{count(w_iw_j)-\\delta}{count(w_i)\\times count(w_j)}$\\delta$作为一个折扣系数的作用，用来防止那些不怎么经常一起出现的词语形成词组。当score超过了我们设置的阈值时，将两个词视为词组。 上述打分程序可以多进行几次，以获得三元组或者更多词的词组。 参考文献[1]McCormick, C. (2016, April 19). Word2Vec Tutorial - The Skip-Gram Model. Retrieved from http://www.mccormickml.com [2]Pennington J, Socher R, Manning C. Glove: Global Vectors for Word Representation[C]// Conference on Empirical Methods in Natural Language Processing. 2014:1532-1543. [3]Mikolov T, Le Q V, Sutskever I. Exploiting Similarities among Languages for Machine Translation[J]. Computer Science, 2013. [4]Mikolov T, Chen K, Corrado G, et al. Efficient Estimation of Word Representations in Vector Space[J]. Computer Science, 2013. [5]Bengio, Y &amp; Ducharme, Réjean &amp; Vincent, Pascal. (2000). A Neural Probabilistic Language Model. Journal of Machine Learning Research. 3. 932-938. 10.1162/153244303322533223. [6]NSS,(JUNE 4, 2017).An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec.Retrieved from https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/ [7]peghoty,2014年07月.word2vec 中的数学原理详解.http://blog.csdn.net/itplus/article/details/37969519 [8]吴军．数学之美[M]．北京：人民邮电出版社，2014. ==========================The End=============================","link":"/2018/03/27/词向量/"},{"title":"k-means聚类算法学习笔记","text":"聚类算法介绍k-means算法介绍k-means聚类是最初来自于信号处理的一种矢量量化方法，现被广泛应用于数据挖掘。k-means聚类的目的是将n个观测值划分为k个类，使每个类中的观测值距离该类的中心（类均值）比距离其他类中心都近。 k-means聚类的一个最大的问题是计算困难，然而，常用的启发式算法能够很快收敛到局部最优解。这通常与高斯分布的期望最大化算法相似，这两种算法都采用迭代求精的方法。此外，它们都使用聚类中心来对数据进行建模 k-means算法的提出与发展詹姆斯·麦奎恩（James MacQueen）1967年第一次使用这个术语“k-means”，虽然这个想法可以追溯到1957年的雨果·斯坦豪斯（Hugo Steinhaus）。标准算法首先由Stuart Lloyd在1957年提出，作为脉冲编码调制技术，尽管直到1982年才发布在贝尔实验室以外。1965年，E. W. Forgy发表了基本相同的方法，这就是为什么它有时被称为Lloyd-Forgy。 k-means算法的优势适应问题 k-means算法优点 是解决聚类问题的一种经典算法，简单、快速。对处理大数据集，该算法是相对可伸缩和高效率的。因为它的复杂度是$O(nkt)$, 其中, n 是所有对象的数目, k 是簇的数目, t 是迭代的次数。通常$k\\ll n$且$t\\ll n$ 。当结果簇是密集的，而簇与簇之间区别明显时, 它的效果较好。 k-means算法缺点 在簇的平均值被定义的情况下才能使用，这对于处理符号属性的数据不适用。必须事先给出k（要生成的簇的数目），而且对初值敏感，对于不同的初始值，可能会导致不同结果。它对于“躁声”和孤立点数据是敏感的，少量的该类数据能够对平均值产生极大的影响。 k-means算法的思想介绍(1) 选定某种距离作为数据样本件的相似性度量 由于k-means聚类算法不适合处理离散型数据，因此在计算个样本距离时，可以根据实际需要选择欧氏距离、曼哈顿距离或者明可夫斯基距离中的一种来作为算法的相似性度量。 假设给定的数据集$X=\\{x_m|m=1,2,…,total\\}$,$X$中的样本用d个属性$A_1,A_2,…,A_d$来表示，并且d个描述属性都是连续型数据。数据样本$x_i=(x_{i1},x_{i2},x_{id}),x_j=(x_{j1},x_{j2}…,x_{jd})$，其中，$x_{i1},x_{i2},x_{id}$和$x_{j1},x_{j2}…,x_{jd}$分别是样本$x_i$和$x_j$对应的d个描述属性$A_1,A_2,…,A_d$的具体取值。样本$x_i$和$x_j$之间的相似度通常用他们之间的距离d($x_i,x_j$)来表示，距离越小，样本$x_i$和$x_j$越相似，差异度越小。距离越大，样本$x_i$和$x_j$越不相似，差异越大。欧氏距离公式如下： d(x_i,x_j)=\\sqrt{\\sum_{k=1}^d(x_{ik}-x_{jk})^2}曼哈顿距离如下： d(x_i,x_j)=\\sum_{k=1}^d|x_{ik}-x_{jk}|明可夫斯基距离如下： d(x_i,x_j)=\\sqrt[p]{\\sum_{k=1}^d|x_{ik}-x_{jk}|^p}当$p=1$时，明氏距离即为曼哈顿距离，当$p=2$时，明氏距离即为欧氏距离，当$p=\\infty$时，明氏距离即为切比雪夫距离。 (2) 选择评价聚类性能的准则函数 k-means聚类算法使用误差平方和准则函数来评价聚类性能。给定数据集X，其中只包含描述属性，不包含类别属性。假设X包含k个聚类子集$X_1,X2,…,X_k$,各个聚类子集中的样本量分别为$n_1,n_2,…,n_k$，各个聚类子集均值代表点分别为$m_1,m_2,…,m_k$，则误差平方和准则函数公式为： E=\\sum_{i=1}^k \\sum_{p\\in X_i}(p-m_i)^2(3) 相似度的计算根据一个簇中对象的平均值来进行 将所有对象随机分配到k个非空的簇中。 计算每个簇的平均值，并用该平均值代表相应的簇。 根据每个对象与各个簇中心的距离，分配给最近的簇。 然后转2，重新计算每个簇的均值。这个过程不断重复知道满足某个准则函数为止。 k-means实现流程：分步骤写k-means算法2个核心问题，一是度量记录之间的相关性的计算公式，此处采用欧氏距离。一是更新簇内质心的方法，此处用平均值法，即means。此时的输入数据为簇的数目k和包含n个对象的数据库——通常在软件中用数据框或者矩阵表示。输出k个簇，使平方误差准则最小。下面为实现k-means聚类的Python代码 # (1)选择初始簇中心。 # (2)对剩余的每个对象，根据其与各个簇中心的距离，将它赋给最近的簇。 # (3)计算新的簇中心。 # (4)重复(2)和(3)，直至准则函数不再明显变小为止。 from numpy import * #定义加载数据的函数。如果数据以文本形式存储在磁盘内，可以用此函数读取 def loadDataSet(fileName) dataMat = [] fr = open(fileName) for line in fr.readlines() curLine = line.strip().split('\\t') fltLine = list(map(float,curLine)) dataMat.append(list(fltLine)) return dataMat #该函数计算两个向量的距离，即欧氏距离 def distEclud(vecA,vecB) return sqrt(sum(power(vecA - vecB,2))) #曼哈顿距离 def Manhattan(vecA,vecB) return sum(abs(vecA-vecB)) #明考夫斯基距离 def MinKowski(vecA,vecB,p) return power(sum(power(abs(vecA-vecB),p)),1/p) #此处为构造质心，而不是从数据集中随机选择k个样本点作为质心， #也是比较合理的方法 def randCent(dataSet,k) n = shape(dataSet)[1] #n为dataSet的列数 centroids = mat(zeros((k,n))) #构造k行n列的矩阵，就是k个质心的坐标 for j in range(n) # minJ = min(dataSet[,j]) #该列数据的最小值 maxJ = max(dataSet[,j]) #该列数据的最大值 rangeJ = float(maxJ-minJ) #全距 #随机生成k个数值，介于minJ和maxJ之间，填充J列 centroids[,j] = minJ + rangeJ * random.rand(k,1) return centroids #定义kMeans函数 def kMeans(dataSet,k,distMeas=distEclud,createCent=randCent) m = shape(dataSet)[0] #m为原始数据的行数 #clusterAssment包含两列，一列记录簇索引值，一列存储误差 clusterAssment = mat(zeros((m,2))) centroids = createCent(dataSet,k) #构造初始的质心 clusterChanged = True #控制变量 while clusterChanged #当控制变量为真时，执行下述循环 clusterChanged = False for i in range(m) minDist = inf #首先令最小值为无穷大 minIndex = -1 #令最小索引为-1 for j in range(k) #下面是一个嵌套循环 #计算点数据点I和簇中心点J的距离 distJI=distMeas(centroids[j,],dataSet[i,]) if distJI < minDist minDist = distJI minIndex = j if clusterAssment[i,0] != minIndex clusterChanged = True clusterAssment[i,] = minIndex,minDist**2 print(centroids) #更新质心的位置 for cent in range(k) #mat.A意味着将矩阵转换为数组，即matrix-->array ptsInClust = dataSet[nonzero(clusterAssment[,0].A==cent)[0]] centroids[cent,] = mean(ptsInClust,axis=0) return centroids,clusterAssment k-means与EM算法EM是机器学习十大算法之一，是一种好理解，但是推导过程比较复杂的方法。下面将原英文版的EM算法介绍翻译一遍，在翻译的过程中也加深一点自己的理解。 Jensen不等式假设$f$是一个定义域为实数的函数。回忆一下，如果对于所有的$x \\in R,f’’(x) \\geq 0$，则$f$是一个凸函数。 如果$f$的输入值是一个向量，则当$x$的海塞矩阵(hessian矩阵$H$)是半正定矩阵时，$f$是凸函数。如果对于所有的$x$，$f’’(x)&gt;0$恒成立，那么我们说$f$是严格凸函数（如果$x$是一个向量，则当$H$是严格半正定矩阵时，写做$H&gt;0$，则$f$是严格凸函数）。设$f$是一个凸函数，并且$X$是一个随机变量，则： E[f(X)] \\geq f(EX).当且仅当$x=$常数时，$E[f(X)]=f(EX)$$.$进一步说，如果$f$是一个严格凸函数，则当P\\{X=E(X)\\}=1时，满足$E[f(X)] = f(EX).$ 由于我们在写某一随机变量的期望时，习惯上是不写方括号的，因此在上述式子中，$f(EX)=f(E[X])$为了解释上述理论，可以用图1帮助理解。 如图1所示，实黑线表示凸函数$f$，$X$是一个随机变量，取值为$a$和$b$的概率均为0.5。因此，$a$和$b$的均值$E(X)$在二者之间。 与此同时，我们在y轴上可以看见$f(x)$,$f(b)$和$f(EX)$的值，并且，$E[f(X)]$$f(a)$和$f(b)$之间。从这个例子可以看出，因为$f$是一个凸函数，所以肯定满足$E[f(X)]\\ge f(EX)$。一般来说，很多人会忘记上式的不等号方向，那么，记住这个图，就很易能够想起来上面的公式了。 注意，如果$f$是一个(严格的)凹函数($f’’(x)\\le 0$或者$H\\le0$)，Jensen不等式仍然成立，只是方向反过来而已，即$E[f(X)]\\le f(EX).$} EM算法假设我们有包含m个独立样本的训练集$\\{x^{(1)},x^{(2)},…,x^{(m)}\\}$,基于该样本和模型$p(x,z)$估计待估参数$\\theta$,似然函数如下： \\ell (\\theta)=\\sum _{i=1}^mlogp(x;\\theta)=\\sum _{i=1}^mlog\\sum _zp(x,z;\\theta).但是，明确找出$\\theta$的最大似然估计值恐怕非常困难。因为在这里，$z^{(i)}$是隐含随机变量，通常情况下如果已知$z^{(i)}$，那么求上述最大似然估计值会比较容易。 在这种情况下，EM算法给出了一种有效的求最大似然估计值的方法。直接求$\\ell (\\theta)$的最大似然估计值也许会很困难，我们的策略是在$\\ell (\\theta)$下面构造一个下界(E-步骤)，然后优化这个下界，使其不断逼近求$\\ell (\\theta)$的最大似然估计值(M-步骤)。 对于每一个$i$，令$Q_i$是$z^{(i)}$上关于某一分布的概率，($\\sum _zQ_i(z)=1,Q_i(z)\\ge 0$).}得到下面的式子.如果z是连续型随机变量，那么$Q_i$就是密度函数，$Q_i$在z上的总和就是$Q_i$在z上的积分： \\sum _i logp(x^{(i)};\\theta)=\\sum _ilog\\sum _{z^{(i)}} p(x^{(i)},z^{(i)};\\theta) =\\sum _i log \\sum _{z^{(i)}}Q_i(z^{(i)})\\frac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})} \\ge \\sum _i \\sum _{z^{(i)}}Q_i(z^{(i)})log\\frac{p(x^{(i)},z^{(i)};\\theta)}{Q_i(z^{(i)})} 最后一步应用了Jensen不等式。特别地，在这里$f(x)=logx$是一个凹函数，因为在实数范围内 f''(x)=-1/x^2 iris_test iris_test_cl iris_test_cl K-means clustering with 3 clusters of sizes 50, 62, 38 Cluster means Sepal.Length Sepal.Width Petal.Length Petal.Width 1 5.006000 3.428000 1.462000 0.246000 2 5.901613 2.748387 4.393548 1.433871 3 6.850000 3.073684 5.742105 2.071053 Clustering vector [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [39] 1 1 1 1 1 1 1 1 1 1 1 1 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [77] 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3 3 3 2 3 3 3 3 3 3 2 [115] 2 3 3 3 3 2 3 2 3 2 3 3 2 2 3 3 3 3 3 2 3 3 3 3 2 3 3 3 2 3 3 3 2 3 3 2 Within cluster sum of squares by cluster [1] 15.15100 39.82097 23.87947 (between_SS / total_SS = 88.4 %) Available components [1] \"cluster\" \"centers\" \"totss\" \"withinss\" \"tot.withinss\" [6] \"betweenss\" \"size\" \"iter\" \"ifault\" > table(iris_test_cl$cluster,iris$Species) setosa versicolor virginica 1 50 0 0 2 0 48 14 3 0 2 36 根据输出结果可以看出，setosa类分类正确率为100\\%，versicolor类有48个分类正确，有两个错误分到了virginica中，viginica有36个分类分类正确，有14个错误分到了versicolor中。总体来说，聚类的正确率为89.3% k-means算法的改进k-mode算法k-mode算法实现了对离散型数据的快速聚类，保留了k-means算法的效率的同时，将k-means的应用范围扩大到了离散型数据。 k-mode算法是按照k-means算法的核心内容进行修改，针对分类属性的度量和更新质心的问题而改进。具体如下： 度量记录之间的相关性D的计算公式是比较两记录之间，属性相同为0，不同为1，并把所有的值相加。因此D越大，就说明两个记录之间的不相关性越强，也可以理解为距离越大，与欧氏距离代表的意义是一样的。 更新modes，使用每个簇的每个属性出现频率最大的那个属性值作为代表簇的属性值。 k-prototype算法k-prototype算法可以对离散型和数值型两种混合的数据进行聚类，在k-prototype中定义了一个对数值型和离散型属性都计算的相异性度量标准。 k-prototype是结合k-means和k-mode算法，针对混合属性的，解决两个核心问题如下： 度量具有混合属性的方法是，数值型属性采用k-means方法得到P1，分类属性采用k-mode方法得到P2，那么D=P1+a*P2，a是权重，如果觉得分类属性重要，则增加权重的值。当a=0时，只有数值型属性。 更新一个簇的中心的方法是结合k-means和k-mode的方法。 k-中心点算法k-中心点算法是针对k-means算法对于孤立点敏感所提出的改进方法。为了解决上述问题，不采用簇中的平均值作为参照点，可以选用簇中位置最中间的对象，即中心点作为参照点。这样划分方法仍然基于最小化所有对象与其参照点的相异度之和的原则来执行的。对于算法的实现来说，就是在更新质心的时候，不是计算所有点的平均值，而是中位数来代表其中心。然后进行迭代，直到质心不再变化为止。 Enhanced k-means下面讨论一下k-means聚类的收敛性。 任意生成k个初始类中心$\\mu_1,\\mu_2,…,\\mu_k \\in R$. 重复如下步骤直到收敛： 对于每一个i，令 c^{(i)}=arg \\ {min_j ||x^{(i)}-\\mu_j||^2} 对于每一个j，令 \\mu_j=\\frac{\\sum _{i=1}^m1\\{c^{(i)}=j\\}x^{(i)}} {\\sum _{i=1}^m1\\{c^{(i)}=j\\}}那么，k-means算法是否能够保证一定收敛呢？答案是肯定的。定义畸变函数 J(c,\\mu)=\\sum _{i=1}^n||x^{(i)}-\\mu _c^{(i)}||^2k-means算法的目的就是使J降至最小。在内循环中，固定$\\mu$,可以通过调整$c$来使J减小；同样的，固定$c$，调整每个类的质心$\\mu$也可以使J减小。当J减到最小时，$\\mu$和$c$也同时收敛。 但是，需要注意的是，J函数是一个非凸函数，所以J不能保证收敛到全局最优，而可以保证收敛到局部最优。通常，k-means达到局部最优的结果差不多是全局最优。但是如果担心陷入了很严重的局部最优，可以多运行几次k-means算法（(给出不同的初始类中心)，选出使J达到最小值的那个模型的$\\mu$和$c$。 为了克服k-means上述缺点，可以在k-mean的结果上对其进行改进(enhanced k-means)。该算法步骤如下： 根据k-means算法得出k个类和相应的类中心。 计算每一个样本点与所有类中心的欧氏距离。 假设$x_i$在第$r$个类中，$n_r$表示第$r$个类包含的样本点数目，$d_{ir}^2$表示$x_i$与第$r$个类的中心点的欧氏距离。如果存在类$s$，使得 \\frac{n_r}{n_r-1}d_{ir}^2>\\frac{n_s}{n_s+1}d_{is}^2,则将$x_i$移到类$s$中。 如果有多个类满足上述不等式，则将$x_i$移动到使得$\\frac{n_s}{n_s+1}d_{ir}^2$最小的那个类中。 重复步骤2$\\sim$4，直到没有变化为止。 二分K-均值算法为了克服k-means算法收敛于局部最小值的问题，有人提出了使用另一个称为二分-K均值(bisecting K-means)的算法。该算法首先将所有的点作为一个簇，然后将该簇一分为二。之后选择其中一个簇继续划分，选择哪一个簇进行划分取决于对其划分是否可以最大程度降低SSE的值。上述基于SSE的划分过程不断重复，直到得到用户指定的簇数目为止。 二分K-均值算法的步骤如下： 将所有的点看成一个簇 当簇数目小于k时，对于每一个簇 计算总误差 在给定的簇上面进行K-means聚类(k=2) 计算将该簇一分为二后的总误差 选择使得误差最小的那个簇进行划分操作。 另一种方法是选择SSE最大的那个簇进行划分。该算法的R代码如下。 #计算误差平方和SSE，group接受一个数值型矩阵，centroid为其均值向量 SSE iris_test_bik dim(iris_test_bik) [1] 150 5 > table(iris_test_bik$cluster,iris$Species) setosa versicolor virginica 1 50 3 0 3 0 9 50 4 0 38 0 由上述最终的聚类结果显示，聚类正确率约为92\\%，比单纯使用k-means算法正确率高。 Enhanced k-means算法更加稳健的原因k-means算法具有明显的两个缺点，即容易陷入局部最优和出现“超级类”。局部最优比较好理解，“超级类”是指聚类结果中有一个包含许多样本点的大类，其他的类包含的样本点都较少。enhanced k-means同时考虑了样本点与聚类中心的距离和类中样本点的个数，保留了k-means聚类算法的优点，同时又克服了出现”超级类”的问题。 Enhanced k-means举例及R实现k-中心点算法可以使用R中cluster包中的pam函数(pam的全称为Partitioning Around Medoids，即围绕中心点分割)，同样以iris数据集为例，说明pam函数的使用。 > library(cluster) > pam.cl table(pam.cl$clustering,iris$Species) setosa versicolor virginica 1 50 0 0 2 0 48 14 3 0 2 36 > par(mfrow=c(1,2)) > plot(pam.cl) 图2将原始数据进行了降维，提取了两个主成分，并且这两个主成分能够解释原始数据方差的95.81\\%，说明降维效果很好。将这两个成分绘制成散点图，就得到了图2.结合table函数的输出结果和图2可知，setosa类与其他两个类的界限比较明显，而其他两个类之间具有交叉。 聚类指标评价多种聚类结果的比较聚类性能的度量大致有两类，一类是将聚类结果与某个“参考模型”进行比较，称为“外部指标”；另一类是直接考察聚类结果而不利用任何参考模型，称为“内部指标”。 外部指标对数据集$D=\\{x_1,x_2,…,x_m\\}$,假定通过聚类给出的簇划分为 C=\\{C_1,C_2,...,C_k\\},参考模型给出的簇划分为 C^+=\\{C_1^+,C_2^+,...,C_s^+\\}相应地，令 $\\lambda$ 与 $\\lambda^+$ 分别表示与 $C$ 和 $C^+$ 对应的簇标记向量，我们将样本两两配对考虑，定义 a=|SS|,SS=\\{(x_i,x_j)|\\lambda _i=\\lambda _j,\\lambda _i^+=\\lambda _j^+,i","link":"/2018/04/04/kmeans聚类/"}],"tags":[{"name":"ETL","slug":"ETL","link":"/tags/ETL/"},{"name":"数据仓库","slug":"数据仓库","link":"/tags/数据仓库/"},{"name":"爬虫","slug":"爬虫","link":"/tags/爬虫/"},{"name":"MapReduce","slug":"MapReduce","link":"/tags/MapReduce/"},{"name":"线性回归","slug":"线性回归","link":"/tags/线性回归/"},{"name":"机器学习","slug":"机器学习","link":"/tags/机器学习/"},{"name":"排序","slug":"排序","link":"/tags/排序/"},{"name":"数据结构","slug":"数据结构","link":"/tags/数据结构/"},{"name":"感知机","slug":"感知机","link":"/tags/感知机/"},{"name":"神经网络","slug":"神经网络","link":"/tags/神经网络/"},{"name":"深度学习","slug":"深度学习","link":"/tags/深度学习/"},{"name":"降维","slug":"降维","link":"/tags/降维/"},{"name":"矩阵分解","slug":"矩阵分解","link":"/tags/矩阵分解/"},{"name":"R","slug":"R","link":"/tags/R/"},{"name":"Lubridate","slug":"Lubridate","link":"/tags/Lubridate/"},{"name":"岭回归","slug":"岭回归","link":"/tags/岭回归/"},{"name":"逐步回归","slug":"逐步回归","link":"/tags/逐步回归/"},{"name":"优化算法","slug":"优化算法","link":"/tags/优化算法/"},{"name":"梯度下降","slug":"梯度下降","link":"/tags/梯度下降/"},{"name":"自然语言处理","slug":"自然语言处理","link":"/tags/自然语言处理/"},{"name":"kmeans","slug":"kmeans","link":"/tags/kmeans/"},{"name":"clustering","slug":"clustering","link":"/tags/clustering/"}],"categories":[{"name":"大数据","slug":"大数据","link":"/categories/大数据/"},{"name":"NLP","slug":"NLP","link":"/categories/NLP/"}]}