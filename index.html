<!DOCTYPE html>
<html  lang="zh">
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
<title>Young&#39;s Blog</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />



    <meta name="description" content="每个人都是平凡的，同时也是与众不同的。">
<meta property="og:type" content="website">
<meta property="og:title" content="Young&#39;s Blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Young&#39;s Blog">
<meta property="og:description" content="每个人都是平凡的，同时也是与众不同的。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://yoursite.com/images/og_image.png">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Young&#39;s Blog">
<meta name="twitter:description" content="每个人都是平凡的，同时也是与众不同的。">
<meta name="twitter:image" content="http://yoursite.com/images/og_image.png">







<link rel="icon" href="/images/favicon.svg">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css">


    
    
    
    <style>body>.footer,body>.navbar,body>.section{opacity:0}</style>
    

    
    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">
    

    
    

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">


    
    
    
    

<link rel="stylesheet" href="/css/back-to-top.css">


    
    
    
    

    
    
<link rel="stylesheet" href="/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>

    
    
    

    
    
    


<link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>
<body class="is-3-column">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/">
            
                <img src="/images/logo.svg" alt="Young&#39;s Blog" height="28">
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item is-active"
                href="/">主页</a>
                
                <a class="navbar-item"
                href="/archives">归档</a>
                
                <a class="navbar-item"
                href="/categories">分类</a>
                
                <a class="navbar-item"
                href="/tags">标签</a>
                
                <a class="navbar-item"
                href="/about">关于</a>
                
            </div>
            
            <div class="navbar-end">
                
                
                
                <a class="navbar-item search" title="搜索" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-8-tablet is-8-desktop is-6-widescreen has-order-2 column-main">
    <div class="card">
    
    <div class="card-image">
        <a href="/2018/07/02/R语言简明教程/" class="image is-7by1">
            <img class="thumbnail" src="/thumbnails/Rlogo.png" alt="R语言简明教程">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2018-07-02T03:43:55.696Z">2018-07-02</time>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    几秒 读完 (大约 13 个字)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2018/07/02/R语言简明教程/">R语言简明教程</a>
            
        </h1>
        <div class="content">
            <h1 id="R语言介绍"><a href="#R语言介绍" class="headerlink" title="R语言介绍"></a>R语言介绍</h1><p>教程测试</p>
<h1 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h1><p># </p>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-image">
        <a href="/2018/05/09/排序算法/" class="image is-7by1">
            <img class="thumbnail" src="/thumbnails/sort.jpg" alt="排序算法">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2018-05-08T16:00:00.000Z">2018-05-09</time>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    3 分钟 读完 (大约 376 个字)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2018/05/09/排序算法/">排序算法</a>
            
        </h1>
        <div class="content">
            <h2 id="排序算法简介"><a href="#排序算法简介" class="headerlink" title="排序算法简介"></a>排序算法简介</h2><h2 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h2><h2 id="所有算法程序"><a href="#所有算法程序" class="headerlink" title="所有算法程序"></a>所有算法程序</h2><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#基础数据结果为python中的list</span></span><br><span class="line">testArr = [<span class="hljs-number">6</span>,<span class="hljs-number">2</span>,<span class="hljs-number">7</span>,<span class="hljs-number">3</span>,<span class="hljs-number">8</span>,<span class="hljs-number">9</span>]</span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">inverse</span><span class="hljs-params">(array)</span>:</span></span><br><span class="line">    lst = array.copy()</span><br><span class="line">    length = len(array)</span><br><span class="line">    mid = int((length - <span class="hljs-number">1</span>) / <span class="hljs-number">2</span>)</span><br><span class="line">    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(mid+<span class="hljs-number">1</span>):</span><br><span class="line">        temp = lst[i]</span><br><span class="line">        lst[i] = lst[length-i<span class="hljs-number">-1</span>]</span><br><span class="line">        lst[length-i<span class="hljs-number">-1</span>] = temp</span><br><span class="line">    <span class="hljs-keyword">return</span> lst</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#冒泡排序</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bubbleSort</span><span class="hljs-params">(array,desc = False)</span>:</span></span><br><span class="line">    lst = array.copy()</span><br><span class="line">    length = len(array)</span><br><span class="line">    <span class="hljs-comment">#默认升序,降序只需改变符号</span></span><br><span class="line">    <span class="hljs-keyword">if</span> length == <span class="hljs-number">1</span>:</span><br><span class="line">        <span class="hljs-keyword">return</span> lst</span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(length<span class="hljs-number">-1</span>):</span><br><span class="line">            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(i+<span class="hljs-number">1</span>,length):</span><br><span class="line">                <span class="hljs-keyword">if</span> lst[i] &gt; lst[j]:</span><br><span class="line">                    temp = lst[i]</span><br><span class="line">                    lst[i] = lst[j]</span><br><span class="line">                    lst[j] = temp</span><br><span class="line">    <span class="hljs-keyword">return</span> lst</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#快速排序</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">shortQuickSort</span><span class="hljs-params">(array,left,right)</span>:</span></span><br><span class="line">    i = left</span><br><span class="line">    j = right</span><br><span class="line">    <span class="hljs-keyword">if</span> i &gt;= j:</span><br><span class="line">        <span class="hljs-keyword">return</span> array</span><br><span class="line">    key = array[i]</span><br><span class="line">    <span class="hljs-keyword">while</span> i &lt; j:</span><br><span class="line">        <span class="hljs-keyword">while</span> i &lt; j <span class="hljs-keyword">and</span> key &lt;= array[j]:</span><br><span class="line">            j -= <span class="hljs-number">1</span></span><br><span class="line">        array[i] = array[j]</span><br><span class="line">        <span class="hljs-keyword">while</span> i &lt; j <span class="hljs-keyword">and</span> key &gt;= array[i]:</span><br><span class="line">            i += <span class="hljs-number">1</span></span><br><span class="line">        array[j] = array[i]</span><br><span class="line">    array[i] = key</span><br><span class="line">    print(array)</span><br><span class="line">    shortQuickSort(array,left,i<span class="hljs-number">-1</span>)</span><br><span class="line">    shortQuickSort(array,j+<span class="hljs-number">1</span>,right)</span><br><span class="line">    <span class="hljs-keyword">return</span> array</span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">longQuickSort</span><span class="hljs-params">(array)</span>:</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">simInsertSort</span><span class="hljs-params">(array)</span>:</span></span><br><span class="line">    lst = array.copy()</span><br><span class="line">    <span class="hljs-keyword">if</span> len(lst) == <span class="hljs-number">0</span>:</span><br><span class="line">        <span class="hljs-keyword">return</span> lst</span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,len(lst)):</span><br><span class="line">            current = lst[i]</span><br><span class="line">            k = i</span><br><span class="line">            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(i<span class="hljs-number">-1</span>,<span class="hljs-number">-1</span>,<span class="hljs-number">-1</span>):</span><br><span class="line">                <span class="hljs-keyword">if</span> current &lt; lst[j]:</span><br><span class="line">                    lst[j+<span class="hljs-number">1</span>] = lst[j]</span><br><span class="line">                    k = j</span><br><span class="line">            lst[k] = current</span><br><span class="line">        <span class="hljs-keyword">return</span> lst</span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">shellSort</span><span class="hljs-params">()</span>:</span></span><br><span class="line">    <span class="hljs-keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">simSelectSort</span><span class="hljs-params">(array)</span>:</span></span><br><span class="line">    lst = array.copy()</span><br><span class="line">    <span class="hljs-keyword">if</span> len(lst) == <span class="hljs-number">1</span>:</span><br><span class="line">        <span class="hljs-keyword">return</span> lst</span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(lst)<span class="hljs-number">-1</span>):</span><br><span class="line">            minVal = lst[i]</span><br><span class="line">            k = i</span><br><span class="line">            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(i+<span class="hljs-number">1</span>,len(lst)):</span><br><span class="line">                <span class="hljs-keyword">if</span> lst[j] &lt; minVal:</span><br><span class="line">                    minVal = lst[j]</span><br><span class="line">                    k = j</span><br><span class="line">            temp = lst[i]</span><br><span class="line">            lst[i] = lst[k]</span><br><span class="line">            lst[k] = temp</span><br><span class="line">    <span class="hljs-keyword">return</span> lst</span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">heapSort</span><span class="hljs-params">()</span>:</span></span><br><span class="line">    <span class="hljs-keyword">pass</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mergerSort</span><span class="hljs-params">()</span>:</span></span><br><span class="line">    <span class="hljs-keyword">pass</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">radixSort</span><span class="hljs-params">()</span>:</span></span><br><span class="line">    <span class="hljs-keyword">pass</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bucketSort</span><span class="hljs-params">()</span>:</span></span><br><span class="line">    <span class="hljs-keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:</span><br><span class="line">    simSelectSort(testArr)</span><br><span class="line">    </span><br><span class="line">start = time.time()</span><br><span class="line">b = simInsertSort(a)</span><br><span class="line">time.time() - start</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fab</span><span class="hljs-params">(n)</span>:</span></span><br><span class="line">    <span class="hljs-keyword">if</span> n == <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> n == <span class="hljs-number">2</span>:</span><br><span class="line">        <span class="hljs-keyword">return</span> n</span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        <span class="hljs-keyword">return</span> fab(n<span class="hljs-number">-1</span>) + fab(n<span class="hljs-number">-2</span>)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#任何递归都可以通过循环实现，因此，快速排序也使用循环</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fab2</span><span class="hljs-params">(n)</span>:</span></span><br><span class="line">    <span class="hljs-keyword">if</span> n == <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> n ==<span class="hljs-number">2</span> :</span><br><span class="line">        <span class="hljs-keyword">return</span> n</span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        a = <span class="hljs-number">1</span></span><br><span class="line">        b = <span class="hljs-number">2</span></span><br><span class="line">        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">2</span>,n):</span><br><span class="line">            temp = b</span><br><span class="line">            b = a + b</span><br><span class="line">            a = temp</span><br><span class="line">        <span class="hljs-keyword">return</span> b</span><br></pre></td></tr></table></figure>


        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-image">
        <a href="/2018/05/05/梯度下降/" class="image is-7by1">
            <img class="thumbnail" src="/thumbnails/gradient.jpg" alt="线性回归与梯度下降算法">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2018-05-04T16:00:00.000Z">2018-05-05</time>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    32 分钟 读完 (大约 4805 个字)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2018/05/05/梯度下降/">线性回归与梯度下降算法</a>
            
        </h1>
        <div class="content">
            <blockquote>
<p>【摘要】</p>
<p>本文以线性回归为例，讲解了批量梯度下降、随机梯度下降、小批量梯度下降、冲量梯度下降等算法，由浅入深，并结合精心设计的例子，使读者最快掌握这种最常用的优化方法。每一种优化方法，笔者都基于R语言给出了相应的代码，供读者参考，</p>
</blockquote>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>假如我们有以下身高和体重的数据，我们希望用身高来预测体重。如果你学过统计，那么很自然地就能想到建立一个线性回归模型：</p>
<p>$$y=a+bx$$</p>
<p>其中$a$是截距，$b$是斜率，$y$是体重，$x$是身高。</p>
<p><img src="/picture/1525406306009-1525855946681.png" alt="1525406306009"></p>
<p><img src="/picture/1525406028458.png" alt="1525406028458"></p>
<p>我们将身高与体重的关系在Excel里面用折线图表示，并且添加了线性的趋势线。蓝色的线条是真实数据，红色的实线是模型给出的预测值。蓝色线条与红色线条之间的距离绝对值是预测误差。所以，我们要找到最优的$a$和$b$来拟合这条直线，使得我们模型的总误差最小。</p>
<p>$$Error = \frac{1}{2}(Actual\ weight - Predicted\ weight)^2=\frac{1}{2}(Y-Ypred)^2$$</p>
<p>我们使用均方误差来表示模型的误差，由于$Ypred = a + bx$，因此，模型的均方误差可以表示为</p>
<p>$$SSE = \sum \frac{1}{2}(Y-a-bx)^2$$</p>
<p>也就是说，$SSE$是关于$a$和$b$的函数，我们只需要不断调整$a$和$b$，使$SSE$降到最低就可以了。这个时候，我们就可以利用梯度下降算法，来求解$a$和$b$的值。</p>
<p>梯度下降的计算过程如下：</p>
<blockquote>
<p>step 1:随机初始化权重$a$和$b$，计算出误差$SSE$</p>
<p>step 2:计算梯度。    $a$和$b$的轻微变化都会导致$SSE$的变化，因此，我们只需要找到能使$SSE$减小的$a$和$b$的变化方向就可以了。这个方向，一般就是由梯度决定的。</p>
<p>step 3:调整权重值，使得$SSE$不断接近最小值。</p>
<p>step 4:使用新的权重去做预测，并且计算出新的$SSE$。</p>
<p>step 5:重复step2-step3，直到权重不再显著变化为止。</p>
</blockquote>
<p>我们在Excel中进行上述步骤。为了计算能够快一点，我们首先对数据进行Min-Max标准化。得到如下数据：</p>
<p><img src="/picture/1525407652713.png" alt="1525407652713"></p>
<p>step1:随机选取一组权重(此处我们设置a=0,b=1),我们计算出预测值和误差：</p>
<p><img src="/picture/1525408865090.png" alt="1525408865090"></p>
<p>step2:计算梯度</p>
<p>$$\frac{\partial SSE}{\partial a} = \sum-(Y-a-bx)=\sum-(Y-Ypred)$$</p>
<p>$$\frac{\partial SSE}{\partial b}=\sum-(Y-a-bx)x=\sum-(Y-Ypred)x$$</p>
<p>$\frac{\partial SSE}{\partial a}$和$\frac{\partial SSE}{\partial b}$就是梯度，他们决定了$a$和$b$的移动方向和距离。    </p>
<p>step3: 调整权重值，使得$SSE$不断接近最小值。</p>
<p>调整规则为:</p>
<p>$$a_{new} = a_{old} - \eta \nabla a = a_{old} - \eta \cdot \partial SSE/\partial a$$</p>
<p>$$b_{new} = b_{old} - \eta \nabla b = b_{old} - \eta \cdot \partial SSE / \partial b$$</p>
<p>其中，$\eta$是一个被我们称之为学习率(learning rate)的东西，一般设置为0.01或者你希望的任何比较小的数值。</p>
<p>本文选择0.01作为学习率。</p>
<p>$$a_{new} = 0 - 0.01 \times 1.925 = -0.01925$$</p>
<p>$$b_{new} = 1 - 0.01 \times 1.117 = 0.98883$$</p>
<p>step4:使用新的权重去做预测，并且计算出新的$SSE$。</p>
<p><img src="/picture/1525410198759.png" alt="1525410198759"></p>
<p>可以看出，SSE从0.155降低到0.111，说明系数有改善。</p>
<p>step 5:重复step2-step3，直到权重不再显著变化为止。</p>
<p>我们知道，一元线性回归的系数可以用公式计算，我们用R的lm()函数来计算权重，结果为</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">lm(y~x,dat)</span><br><span class="line"></span><br><span class="line">Call:</span><br><span class="line">lm(formula = y ~ x, data = dat)</span><br><span class="line"></span><br><span class="line">Coefficients:</span><br><span class="line">(Intercept)            x  </span><br><span class="line">-<span class="hljs-number">0.1167</span>       <span class="hljs-number">0.9777</span></span><br></pre></td></tr></table></figure>



<p>然后，我在R里面写了一个梯度下降的函数，当精度调到0.0000001的时候，与lm的结果已经很接近了。</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">gradientDescent &lt;- <span class="hljs-keyword">function</span>(dat,start = c(<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),learning_rate = <span class="hljs-number">0.01</span>,tol = <span class="hljs-number">0.001</span>)</span><br><span class="line">&#123;</span><br><span class="line">	a = start[<span class="hljs-number">1</span>]</span><br><span class="line">	b = start[<span class="hljs-number">2</span>]</span><br><span class="line">	x = dat[,<span class="hljs-number">1</span>]</span><br><span class="line">	y = dat[,<span class="hljs-number">2</span>]</span><br><span class="line">	iters = <span class="hljs-number">0</span></span><br><span class="line">	<span class="hljs-keyword">while</span>(<span class="hljs-literal">TRUE</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		Ypred = a + b * x</span><br><span class="line">		old_a = a</span><br><span class="line">		old_b = b</span><br><span class="line">		a = a + learning_rate * sum(y - Ypred)</span><br><span class="line">		b = b + learning_rate * sum((y-Ypred) * x)</span><br><span class="line">		iters = iters + <span class="hljs-number">1</span></span><br><span class="line">		<span class="hljs-keyword">if</span>(abs(a-old_a) &lt;= tol &amp; abs(b-old_b) &lt;= <span class="hljs-number">0.01</span>)</span><br><span class="line">			<span class="hljs-keyword">break</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	list(weights = c(a,b),iters = iters)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">gradientDescent(dat,tol=<span class="hljs-number">0.0000001</span>)</span><br><span class="line">$weights</span><br><span class="line">[<span class="hljs-number">1</span>] -<span class="hljs-number">0.1167315</span>  <span class="hljs-number">0.9776839</span></span><br><span class="line"></span><br><span class="line">$iters <span class="hljs-comment">#迭代了975次</span></span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">975</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>我们常说的梯度，其实是指向量，其方向与切线方向相同。</p>
<p>利用梯度下降法进行权重更新的公式为:</p>
<p>$$weight_{new} = weight_{old} - \eta \cdot \nabla$$</p>
<p>其中，那个倒三角形就是梯度的意思。我们在高中数学学过，切线方向是函数变化速度最快的方向，</p>
</blockquote>
<h2 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h2><p>梯度下降算法，又可以称为Batch-Gradient-Gescent,即批量梯度下降算法。从上面的例子可以看出，批量梯度下降算法，每次更新系数都需要所有的样本参与计算，当样本规模达到一定数量以后，这个更新速度会非常慢。另外，还有可能导致内存溢出。</p>
<p>为了克服批量梯度下降的这个缺点，有人提出了随机梯度下降(Stochastic Gradient Descent)算法，即每次更新系数只需要一个样本参与计算，因此既可以减少迭代次数，节省计算时间，又可以防止内存溢出。</p>
<p>对于上述问题，随机梯度下降的算法过程如下：</p>
<blockquote>
<p>for every $Y_i$:</p>
<p>$Ypred = a + bx$</p>
<p>$a = a + \eta (Y-Ypred)$</p>
<p>$b = b+\eta(Y-Ypred)\cdot x$</p>
</blockquote>
<p>随机梯度下降算法适用于大数量的计算，对于小数据量不一定准确。为了检验随机梯度下降算法，我们构造了一个有10000个样本的数据，同样是计算一元线性回归的系数。</p>
<p>随机梯度下降的函数如下：</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">stochasticGradientDescent &lt;- <span class="hljs-keyword">function</span>(dat,start = c(<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),learning_rate = <span class="hljs-number">0.01</span>,tol = <span class="hljs-number">0.000001</span>,iteratons = <span class="hljs-number">100</span>)</span><br><span class="line">&#123;</span><br><span class="line">	<span class="hljs-comment">#start:初始参数</span></span><br><span class="line">	<span class="hljs-comment">#learning_rate:学习率</span></span><br><span class="line">	<span class="hljs-comment">#tol:精度</span></span><br><span class="line">	<span class="hljs-comment">#iterations:迭代次数</span></span><br><span class="line">	</span><br><span class="line">	dat = as.matrix(dat)</span><br><span class="line">	a = start[<span class="hljs-number">1</span>]</span><br><span class="line">	b = start[<span class="hljs-number">2</span>]</span><br><span class="line">	x = dat[,<span class="hljs-number">1</span>]</span><br><span class="line">	y = dat[,<span class="hljs-number">2</span>]</span><br><span class="line">	iters = <span class="hljs-number">0</span></span><br><span class="line">	<span class="hljs-keyword">while</span>(iters &lt; iteratons)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="hljs-comment">#重排，即将样本的顺序打乱</span></span><br><span class="line">		index = sample(length(x))</span><br><span class="line">		old_a = a</span><br><span class="line">		old_b = b</span><br><span class="line">		<span class="hljs-keyword">for</span>(i <span class="hljs-keyword">in</span> index)</span><br><span class="line">		&#123;</span><br><span class="line">			</span><br><span class="line">			Ypred = a + b * x[i]</span><br><span class="line">			a = a + learning_rate * (y[i] - Ypred)</span><br><span class="line">			b = b + learning_rate * (y[i]-Ypred) * x[i]</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="hljs-keyword">if</span>(abs(a-old_a) &lt;= tol &amp; abs(b-old_b) &lt;= tol)</span><br><span class="line">		 	<span class="hljs-keyword">break</span>;</span><br><span class="line">		learning_rate = learning_rate / (<span class="hljs-number">1</span> + <span class="hljs-number">0.01</span> * iters) <span class="hljs-comment">#自适应学习率</span></span><br><span class="line">		iters = iters + <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">if</span>(iters &gt; iterations)</span><br><span class="line">            <span class="hljs-keyword">break</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	list(weights = c(a,b),iters = iters)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>然后我们构造一个相对大的样本用来检验算法：</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">set.seed(<span class="hljs-number">100</span>)</span><br><span class="line">x &lt;- seq(<span class="hljs-number">1</span>,<span class="hljs-number">10</span>,length.out = <span class="hljs-number">10000</span>)</span><br><span class="line">y &lt;- <span class="hljs-number">2</span> * x + rnorm(<span class="hljs-number">10000</span>) * <span class="hljs-number">10</span> + <span class="hljs-number">2</span></span><br><span class="line">bigdata &lt;- data.frame(x ,y )</span><br><span class="line">plot(x,y)</span><br><span class="line">cor(x,y)</span><br></pre></td></tr></table></figure>

<p><img src="/picture/sgd.png" alt="sgd"></p>
<p>回归结果：</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lm(y~x,data = bigdata )</span><br><span class="line">Call:</span><br><span class="line">lm(formula = y ~ x, data = bigdata)</span><br><span class="line"></span><br><span class="line">Coefficients:</span><br><span class="line">(Intercept)            x  </span><br><span class="line">      <span class="hljs-number">2.004</span>        <span class="hljs-number">2.006</span></span><br></pre></td></tr></table></figure>

<p>随机梯度下降的结果：</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">stochasticGradientDescent(bigdata,learning_rate = <span class="hljs-number">0.001</span>,tol=<span class="hljs-number">0.000000001</span>)</span><br><span class="line">$weights</span><br><span class="line"><span class="hljs-number">2.01138749995603</span> <span class="hljs-number">2.00584502615877</span></span><br><span class="line">$iters</span><br><span class="line"><span class="hljs-number">69</span></span><br></pre></td></tr></table></figure>

<p>批量梯度下降的结果：</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">batchGradientDescent(bigdata,learning_rate = <span class="hljs-number">0.000001</span>,tol = <span class="hljs-number">0.000000001</span>)</span><br><span class="line">$weights</span><br><span class="line"><span class="hljs-number">2.00385275101478</span> <span class="hljs-number">2.00634924457312</span></span><br><span class="line">$iters</span><br><span class="line"><span class="hljs-number">8345</span></span><br></pre></td></tr></table></figure>

<p>可以看到，在同样的精度要求下，随机梯度下降进行59次迭代以后即收敛，而批量梯度下降则需要迭代8345次。</p>
<p>但是随机梯度下降也有一个缺点，即参数更新频率太快，有可能出现目标函数值在最优质附近的震荡现象，因为高频率的参数更新导致了高方差。 同时也可以看出，在相同精度要求下，随机梯度下降计算出来的系数与精确值离差较大，而批量随机下降则更接近精确值。</p>
<h2 id="Mini-batch-Gradient-Descent"><a href="#Mini-batch-Gradient-Descent" class="headerlink" title="Mini-batch Gradient Descent"></a>Mini-batch Gradient Descent</h2><p>小批量梯度下降(Mini-batch Gradient Descent)是介于上述两种方法之间的优化方法，即在更新参数时，只使用一部分样本（一般256以下）来更新参数，这样既可以保证训练过程更稳定，又可以利用批量训练方法中的矩阵计算的优势。</p>
<p>具体更新哪些样本，通常是随机确定的，下面，我们定义一下小批量梯度下降的函数，用来求解上述bigdata的系数。</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">miniBatchGradientDescent &lt;- <span class="hljs-keyword">function</span>(dat,start = c(<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),learning_rate = <span class="hljs-number">0.01</span>,tol = <span class="hljs-number">0.001</span>,batchSize = <span class="hljs-number">256</span>,iterations = <span class="hljs-number">10000</span>)</span><br><span class="line">&#123;</span><br><span class="line">    a = start[<span class="hljs-number">1</span>]</span><br><span class="line">    b = start[<span class="hljs-number">2</span>]</span><br><span class="line">    iters = <span class="hljs-number">0</span></span><br><span class="line">    len = length(y)</span><br><span class="line">    <span class="hljs-keyword">while</span>(<span class="hljs-literal">TRUE</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        mini_index = sample(len,batchSize,replace = <span class="hljs-literal">FALSE</span>)</span><br><span class="line">        x = dat[mini_index,<span class="hljs-number">1</span>]</span><br><span class="line">        y = dat[mini_index,<span class="hljs-number">2</span>]</span><br><span class="line">        Ypred = a + b * x</span><br><span class="line">        error = y - Ypred</span><br><span class="line">        old_a = a</span><br><span class="line">        old_b = b</span><br><span class="line">        a = a + learning_rate * sum(error)</span><br><span class="line">        b = b + learning_rate * sum((error) * x)</span><br><span class="line">        start = rbind(start,c(a,b))</span><br><span class="line">        iters = iters + <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">if</span>(abs(a-old_a) &lt;= tol &amp; abs(b-old_b) &lt;= tol)</span><br><span class="line">            <span class="hljs-keyword">break</span></span><br><span class="line">        <span class="hljs-keyword">if</span>(iters &gt;= iterations)</span><br><span class="line">            <span class="hljs-keyword">break</span></span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    list(weights = c(a,b),iters = iters,coes = start)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">miniBatchGradientDescent(bigdata,learning_rate = <span class="hljs-number">0.0001</span>,tol = <span class="hljs-number">0.00001</span>,batchSize = <span class="hljs-number">100</span>)</span><br><span class="line">$weights</span><br><span class="line"><span class="hljs-number">2.02646349019186</span> <span class="hljs-number">2.0439693915315</span></span><br><span class="line">$iters</span><br><span class="line"><span class="hljs-number">920064</span></span><br></pre></td></tr></table></figure>

<p>小梯度批量梯度下降收敛时需要迭代92万次，这显然有点多了。一般来说，当数据量非常大时，小批量梯度下降比较有效，否则计算结果很有可能出现偏移。</p>
<blockquote>
<p>先mark，偏移的原因待考究。</p>
</blockquote>
<h2 id="Momentum-optimization"><a href="#Momentum-optimization" class="headerlink" title="Momentum optimization"></a>Momentum optimization</h2><p>考虑这样一种情形，小球从山顶往下滚动，一开始很顺利，可是在接近最低点的时候，小球陷入了一段狭长的浅山谷。由于小球一开始并不是直接沿着山谷的方向滚下，因此小球会在这个浅浅的山谷中不断震荡——不断冲上墙壁，接着又从墙壁上滚下来。这种情况并不是我们想看到的，因为这增加了迭代时间。冲量(Momentnum)的引入，使得我们的目标更新的更快了，冲量的更新方式有以下两种，两种方式之间并无太大差异。</p>
<blockquote>
<p>第一种：</p>
<p>$Z^{k+1}=\beta Z_k + \nabla$</p>
<p>$w^{k+1} = w_k - \alpha Z^{k+1}$</p>
<p>其中，$Z$是一个与$w$方向相同的向量，</p>
</blockquote>
<blockquote>
<p>第二种：</p>
<p>$Z^{k+1}=\beta Z^k + \alpha \nabla$</p>
<p>$w^{k+1} = w^k - Z^{k+1}$</p>
</blockquote>
<p><img src="/picture/1525839842032.png" alt="1525839842032"></p>
<p>两者的差别仅仅在于$Z^{k+1}$的系数不同。</p>
<p>通常，这里的学习率要比随机梯度下降小一点，因为随机梯度下降的梯度大一点。$\beta$的取值决定了前一次的梯度有多少被纳入了本次的更新。一般来说，稳定前将$\beta$设置为0.5，稳定后可以设置为0.9或更高。</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#适用于求解一元或多元线性回归的回归系数，返回结果包括截距</span></span><br><span class="line">momentumGradientDescent &lt;- <span class="hljs-keyword">function</span>(dat,beta = <span class="hljs-number">0.9</span>,z = <span class="hljs-number">0</span>,start = rep(<span class="hljs-number">0</span>,dim(dat)[<span class="hljs-number">2</span>]),alpha = <span class="hljs-number">0.001</span>,tol=<span class="hljs-number">0.0000001</span>,iterations = <span class="hljs-number">100</span>)</span><br><span class="line">&#123;</span><br><span class="line">    dataSet = cbind(<span class="hljs-number">1</span>,dat)  <span class="hljs-comment">#将第一列加上1</span></span><br><span class="line">    cols = dim(dataSet)[<span class="hljs-number">2</span>] <span class="hljs-comment">#列数</span></span><br><span class="line">    x = as.matrix(dataSet[,<span class="hljs-number">1</span>:(cols - <span class="hljs-number">1</span>)])  <span class="hljs-comment">#自变量矩阵</span></span><br><span class="line">    y = as.matrix(dataSet[,cols],ncol = <span class="hljs-number">1</span>)  <span class="hljs-comment">#因变量，矩阵</span></span><br><span class="line">    w = as.matrix(start,ncol = <span class="hljs-number">1</span>)  <span class="hljs-comment">#权重矩阵</span></span><br><span class="line">    iters = <span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">while</span>(<span class="hljs-literal">TRUE</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        old_w = w</span><br><span class="line">        old_z = z</span><br><span class="line">        Ypred = x %*% w</span><br><span class="line">        error = y - Ypred</span><br><span class="line">        grad = - t(x) %*% error</span><br><span class="line">        z = beta * old_z + grad</span><br><span class="line">        w = old_w - alpha * z</span><br><span class="line">        <span class="hljs-keyword">if</span>(sum(abs(w-old_w)) &lt; tol)</span><br><span class="line">            <span class="hljs-keyword">break</span>;</span><br><span class="line">        iters = iters + <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">if</span>(is.integer(iterations))</span><br><span class="line">            <span class="hljs-keyword">if</span>(iters &gt;= iterations)</span><br><span class="line">                <span class="hljs-keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    list(weights = as.vector(w),iters = iters)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>利用冲量梯度下降求bigdata的系数：</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">momentumGradientDescent(bigdata,alpha=<span class="hljs-number">0.00001</span>)</span><br><span class="line">$weights</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">2.003851</span> <span class="hljs-number">2.006354</span></span><br><span class="line"></span><br><span class="line">$iters</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">248</span></span><br></pre></td></tr></table></figure>

<p>可以看到，迭代次数明显减少，并且系数与精确值更加接近了。</p>
<h2 id="Nesterov-Accelerated-Gradient"><a href="#Nesterov-Accelerated-Gradient" class="headerlink" title="Nesterov Accelerated Gradient"></a>Nesterov Accelerated Gradient</h2><p>然而，让一个小球盲目地沿着斜坡滚下山是不理想的。我们需要一个更聪明的球，它知道下一步要往哪里去，因此在斜坡有上升的时候，它能够自主调整方向。</p>
<p>Nesterov Accelerated Gradient 是基于冲量梯度下降算法进行改进的一种算法，也是梯度下降算法的变种。</p>
<p>在上一种算法中，我们使用了冲量$\beta Z^k$来调整我们的参数改变量，将上述第二种更新方法改写一下，得到如下式子：</p>
<blockquote>
<p>$Z^{k+1} = \beta Z^k + \alpha \nabla$</p>
<p>$w^{k+1} = w^k - Z^{k+1} = w^k - \beta Z_k - \alpha \nabla $</p>
</blockquote>
<p>$w^k - \beta Z_k $给出了下一个参数位置的近似，指明了参数该如何变化。现在，我们可以不用当前的参数，而是用未来的参数的近似位置来更新我们的参数，即</p>
<blockquote>
<p>$Z^{k+1} = \beta Z^k + \alpha \nabla _wJ(w - \beta Z^k)$</p>
<p>$w^{k+1} = w^k - Z^{k+1}$</p>
</blockquote>
<p>这里，我们仍然设置$\beta$的值在0.9附近。如下图所示，冲量梯度下降先计算当前的梯度（短的蓝色向量），然后根据累积的冲量向前跨越一大步（长的蓝色向量）。NAG首先根据之前的累积梯度向前迈一大步（棕色向量），然后对梯度进行修正（红色向量）。这种利用近似未来参数来更新参数的方法，可以防止梯度更新太快，并且增加了响应能力。</p>
<p><img src="/picture/nesterov_update_vector.png" alt="nesterov_update_vector"> </p>
<blockquote>
<p>假设我们的权重矩阵（系数矩阵）为$w$，自变量$x$，因变量$Y$，则损失函数为：</p>
<p>$$J(w) = 1/2 \sum _{i=1} ^n (Y_i - w’x_i)^2$$</p>
<p>则</p>
<p>$$\nabla J(w) = \frac{\partial J(w)}{\partial w} = - \sum_{i=1}^n(Y_i - w’x_i)x_i’$$</p>
<p>那么</p>
<p>$$\nabla J(w - \beta Z^{k}) = - \sum _{i=1}^n(Y_i - (w’-\beta Z^k)’x_i)x_i’$$</p>
</blockquote>
<p>根据上述思想，编写的NAG代码如下 ：</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#适用于求解一元或多元线性回归的回归系数，返回结果包括截距</span></span><br><span class="line">NAG &lt;- <span class="hljs-keyword">function</span>(dat,beta = <span class="hljs-number">0.9</span>,z = <span class="hljs-number">0</span>,start = rep(<span class="hljs-number">0</span>,dim(dat)[<span class="hljs-number">2</span>]),alpha = <span class="hljs-number">0.001</span>,tol=<span class="hljs-number">0.0000001</span>,iterations = <span class="hljs-number">100</span>)</span><br><span class="line">&#123;</span><br><span class="line">    dataSet = cbind(<span class="hljs-number">1</span>,dat)  <span class="hljs-comment">#将第一列加上1</span></span><br><span class="line">    cols = dim(dataSet)[<span class="hljs-number">2</span>] <span class="hljs-comment">#列数</span></span><br><span class="line">    x = as.matrix(dataSet[,<span class="hljs-number">1</span>:(cols - <span class="hljs-number">1</span>)])  <span class="hljs-comment">#自变量矩阵</span></span><br><span class="line">    y = dataSet[,cols]  <span class="hljs-comment">#因变量，矩阵</span></span><br><span class="line">    w = start  <span class="hljs-comment">#权重矩阵</span></span><br><span class="line">    iters = <span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">while</span>(<span class="hljs-literal">TRUE</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        old_w = w</span><br><span class="line">        old_z = z</span><br><span class="line">        <span class="hljs-comment">#中间过程用mid1 mid2代替</span></span><br><span class="line">        mid1 = apply((old_w - beta * old_z) * t(x),<span class="hljs-number">2</span>,sum)</span><br><span class="line">        mid2 = y - mid1</span><br><span class="line">        grad = - apply(mid2 * x,<span class="hljs-number">2</span>,sum)</span><br><span class="line">        z = beta * old_z + alpha * grad</span><br><span class="line">        w = old_w - z</span><br><span class="line">        <span class="hljs-keyword">if</span>(sum(abs(w-old_w)) &lt; tol)</span><br><span class="line">            <span class="hljs-keyword">break</span>;</span><br><span class="line">        iters = iters + <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">if</span>(is.integer(iterations))</span><br><span class="line">            <span class="hljs-keyword">if</span>(iters &gt;= iterations)</span><br><span class="line">                <span class="hljs-keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    list(weights = as.vector(w),iters = iters)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>用NAG来求bigdata的系数：</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">NAG(bigdata,alpha=<span class="hljs-number">0.000001</span>)</span><br><span class="line">$weights</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">2.003849</span> <span class="hljs-number">2.006350</span></span><br><span class="line"></span><br><span class="line">$iters</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">598</span></span><br></pre></td></tr></table></figure>

<h2 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h2><p>尽管我们可以根据损失函数的梯度来加快更新参数，我们也希望能够根据参数的重要性来决定其更新的幅度。</p>
<p>AdaGrad是一种基于梯度算法的优化算法,它只做了一件事:根据参数来自适应调整学习率。对于不常出现的参数进行较大的更新，对于经常出现的参数进行较少的更新，因此，这种方法非常适合处理稀疏数据。</p>
<p>之前，我们对每一个参数更新所使用的学习率都是一样的，而AdaGrad在每一步都使用不同的学习率对不同的参数进行更新。我们先写出AdaGrad的单个参数的更新方法，然后将其向量化。长话短说，假设$g_{t,i}$表示损失函数对于参数$\theta_i$的梯度：</p>
<p>在步骤$t$:</p>
<p>$$g_{t,i} = \nabla _\theta J(\theta_t,i)$$</p>
<p>那么，对于步骤$t$，使用随机梯度下将对$\theta_i$进行更新的公式为：</p>
<p>$$\theta_{t+1,i} = \theta_{t,i} - \eta_i \cdot g_{t,i}$$</p>
<p>在上述更新过程中，AdaGrad在每一步都对参数$\theta_i$对应的学习率$\eta_i$进行调整，调整的方法基于过去的所有梯度：</p>
<p>$$\theta_{t+1,i} = \theta_{t,i} - \frac{\eta_i}{\sqrt{G_{t,ii} + \epsilon}} \cdot g_{t,i}$$</p>
<p>$G_{t}\in R^{d\times d}$是一个对角矩阵，第$i$个对角元素是历史上损失函数对$\theta_i$的所有梯度的平方和，$\epsilon$是一个平滑参数，防止分母为0，通常取$10^{-8}$。有趣的是，如果不加开方，算法表现极差。</p>
<p>因为$G_t$包含了过去所有参数梯度的平方和，因此我们可以将其向量化：</p>
<p>$$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}}\odot g_t$$</p>
<p>AdaGrad的一个最大的好处是不用手动调整学习率的大小，通常设置默认值为0.01，然后顺其自然就好了。</p>
<p>AdaGrad的一个较大的缺点是，分母是不断增大的，当迭代次数不断增加时，分母会逐渐趋于无穷大，学习率进而趋于无穷小，此时，算法将变得不再有效。</p>
<p>我们延用上述符号来写出AdaGrad的函数。</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#SGD为基础</span></span><br><span class="line">AdaGrad &lt;- <span class="hljs-keyword">function</span>(dat,theta,learning_rate = <span class="hljs-number">0.01</span>,start = rep(<span class="hljs-number">0</span>,dim(dat)[<span class="hljs-number">2</span>]),tol = <span class="hljs-number">0.000001</span>,iterations = <span class="hljs-number">1000</span>)</span><br><span class="line">&#123;</span><br><span class="line">    dataSet = cbind(<span class="hljs-number">1</span>,dat) <span class="hljs-comment">#将自变量进行增广，第一列全为1</span></span><br><span class="line">    cols = dim(dataSet)[<span class="hljs-number">2</span>] <span class="hljs-comment">#列数</span></span><br><span class="line">    rows = dim(dataSet)[<span class="hljs-number">1</span>] <span class="hljs-comment">#行数</span></span><br><span class="line">    x = as.matrix(dataSet[,<span class="hljs-number">1</span>:(cols - <span class="hljs-number">1</span>)])  <span class="hljs-comment">#自变量矩阵</span></span><br><span class="line">    y = dataSet[,cols]  <span class="hljs-comment">#因变量，矩阵</span></span><br><span class="line">    g = <span class="hljs-number">0</span></span><br><span class="line">    theta = start  <span class="hljs-comment">#权重矩阵</span></span><br><span class="line">    iters = <span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">while</span>(<span class="hljs-literal">TRUE</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        old_theta = theta</span><br><span class="line">        index = sample(rows)</span><br><span class="line">        <span class="hljs-keyword">for</span>(i <span class="hljs-keyword">in</span> index)</span><br><span class="line">        &#123;</span><br><span class="line">            grad = - (y[i] - sum(theta * x[i,])) * x[i,]</span><br><span class="line">            g = g + grad * grad</span><br><span class="line">            theta = theta - learning_rate / sqrt(g + <span class="hljs-number">1e-8</span>) * grad</span><br><span class="line">        &#125;</span><br><span class="line">        iters = iters + <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">if</span>(is.integer(iterations))</span><br><span class="line">            <span class="hljs-keyword">if</span>(iters &gt; iterations)</span><br><span class="line">                <span class="hljs-keyword">break</span>;</span><br><span class="line">        <span class="hljs-keyword">if</span>(sum(abs(old_theta - theta)) &lt; tol)</span><br><span class="line">            <span class="hljs-keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    list(weights = as.vector(theta),iters = iters)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>AdaGrad的计算结果如下：</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">AdaGrad(bigdata)</span><br><span class="line">$weights</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">2.008219</span> <span class="hljs-number">2.005682</span></span><br><span class="line"></span><br><span class="line">$iters</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">6579</span></span><br></pre></td></tr></table></figure>

<h2 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h2><p>AdaDelta是在AdaGrad的基础上发展而来的，目的是解决AdaGrad算法中学习率的单调减少问题。AdaDelta不再采用累积梯度平方和的方法来调整学习率，而是根据一些固定的$w$的大小来限制过去累积梯度的窗口。</p>
<p>AdaDelta不再无效率地存储历史梯度的平方和，而将历史梯度平方和定义为衰减均值(decaying average)。第$t$步的移动平均值$E[g^2]_t$仅仅取决于过去的平均值和当前梯度（有点类似于momentum）：</p>
<p>$$E[g^2]<em>t = \gamma E[g^2]</em>{t-1} + (1-\gamma)g_t^2$$</p>
<p>同样的，我们把$\gamma$设置在0.9附近。为清楚起见，我们重新写下更新规则：</p>
<blockquote>
<p>$$\Delta \theta <em>t = - \eta \cdot g</em>{t,i}$$</p>
<p>$$\theta _{t+1} = \theta_t + \Delta \theta _t$$</p>
</blockquote>
<p>根据AdaGrad我们推出AdaDelta的参数更新公式：</p>
<blockquote>
<p>$$\Delta \theta _t = - \frac{\eta}{\sqrt{G_t + \epsilon}} \odot g_t$$</p>
</blockquote>
<p>我们只需把$G_t$替换$E[g^2]_t$就行了：</p>
<blockquote>
<p>$$\theta_t = - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}}$$</p>
</blockquote>
<p>因为分母刚好是梯度的均方根，我们将其简写为：</p>
<blockquote>
<p>$$\Delta \theta_t = -\frac{\eta}{RMS[g]_t}g_t$$</p>
</blockquote>
<p>作者们发现上述更新中的单位不一致（可以理解为量纲不一致），因此，他们定义了另外一个指数衰减均值，这次不用梯度的平方了，而是用参数的平方来进行更新：</p>
<p>$$E[\Delta \theta^2]<em>t = \gamma E[\Delta \theta^2]</em>{t-1} + (1-\gamma)\Delta\theta_t^2$$</p>
<p>参数更新的均方误差即：</p>
<p>$$RMS[\Delta\theta]_t = \sqrt{E[\Delta\theta^2]_t + \epsilon}$$</p>
<p>因为$RMS[\theta]<em>t$是未知的，我们可以用之前所有的更新过的参数的RMS来代替。将之前的学习率$\eta$换成$RMS[\Delta\theta]</em>{t-1}$，那么，我们得出AdaDelta的更新规则：</p>
<p>$$\Delta\theta_t = -\frac{RMS[\Delta\theta]_{t-1}}{RMS[g]_t}g_t$$</p>
<p>$$\theta_{t+1} = \theta_t + \Delta\theta_t$$</p>
<p>AdaDelta甚至不需要初始化学习率，因为在更新规则中已经不见它的身影了。</p>
<p>根据上述思路，我们写出AdaDelta的函数:</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">RMS &lt;- <span class="hljs-keyword">function</span>(x)</span><br><span class="line">&#123;</span><br><span class="line">    sqrt(mean(x^<span class="hljs-number">2</span>) - mean(x)^<span class="hljs-number">2</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">AdaDelta &lt;- <span class="hljs-keyword">function</span>()</span><br></pre></td></tr></table></figure>

<h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p><strong>上述所有改进方法均可以运用于批量梯度下降、小批量梯度下降和随机梯度下降！</strong></p>
<p>[1]<a href="http://netedu.xauat.edu.cn/jpkc/netedu/jpkc/gdsx/homepage/5jxsd/51/513/5308/530807.htm" target="_blank" rel="noopener">http://netedu.xauat.edu.cn/jpkc/netedu/jpkc/gdsx/homepage/5jxsd/51/513/5308/530807.htm</a></p>
<p>[2]<a href="https://www.kdnuggets.com/2017/04/simple-understand-gradient-descent-algorithm.html" target="_blank" rel="noopener">https://www.kdnuggets.com/2017/04/simple-understand-gradient-descent-algorithm.html</a></p>
<p>[3]<a href="https://distill.pub/2017/momentum/" target="_blank" rel="noopener">https://distill.pub/2017/momentum/</a></p>
<p>[4]<a href="http://ruder.io/optimizing-gradient-descent/index.html" target="_blank" rel="noopener">http://ruder.io/optimizing-gradient-descent/index.html</a></p>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-image">
        <a href="/2018/05/04/牛顿法/" class="image is-7by1">
            <img class="thumbnail" src="/thumbnails/optimize.jpg" alt="平方根与牛顿法">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2018-05-04T02:50:15.447Z">2018-05-04</time>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    几秒 读完 (大约 0 个字)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2018/05/04/牛顿法/">平方根与牛顿法</a>
            
        </h1>
        <div class="content">
            
        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-image">
        <a href="/2018/05/03/逻辑回归/" class="image is-7by1">
            <img class="thumbnail" src="/thumbnails/logistics.jpg" alt="逻辑回归">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2018-05-02T16:00:00.000Z">2018-05-03</time>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    24 分钟 读完 (大约 3638 个字)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2018/05/03/逻辑回归/">逻辑回归</a>
            
        </h1>
        <div class="content">
            <h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>逻辑回归常用来处理分类问题，最常用来处理二分类问题。</p>
<p>生活中经常遇到具有两种结果的情况（冬天的北京会下雪，或者不会下雪；暗恋的对象也喜欢我，或者不喜欢我；今年的期末考试会挂科，或者不会挂科……）。对于这些二分类结果，我们通常会有一些输入变量，或者是连续性，或者是离散型。那么，我们怎样来对这些数据建立模型并且进行分析呢？</p>
<p>我们可以尝试构建一种规则来根据输入变量猜测二分输出变量，这在统计机器学上被称为分类。然而，简单的给一个回答“是”或者“不是”显得太过粗鲁，尤其是当我们没有完美的规则的时候。总之呢，我们不希望给出的结果就是武断的“是”或“否”，我们希望能有一个概率来表示我们的结果。</p>
<p>一个很好的想法就是，在给定输入$X$的情况下，我们能够知道Y的条件概率$Pr(Y|X)$。一旦给出了这个概率，我们就能够知道我们预测结果的准确性。</p>
<p>让我们把其中一个类称为1，另一个类称为0。（具体哪一个是1，哪一个是0都无所谓）。$Y$变成了一个指示变量，现在，你要让自己相信，$Pr(Y=1)=EY$，类似的，$Pr(Y=1|X=x)=E[Y|X=x]$。</p>
<blockquote>
<p>假设$Y$有10个观测值，分别为 0 0 0 1 1 0 1 0 0 1.即6个0,4个1.那么，$Pr(Y=1)=\frac{count(1)}{count(n)}=\frac{4}{10}=0.4$，同时，$EY=\frac{sum(Y)}{count(n)}=\frac{4}{10}=0.4$</p>
</blockquote>
<p>换句话说，条件概率是就是指示变量（即$Y$)的条件期望。这对我们有帮助，因为从这个角度上，我们知道所有关于条件期望的估计。我们要做的最直接的事情是挑选出我们喜欢的平滑器，并估计指示变量的回归函数，这就是条件概率函数的估计。</p>
<p>有两个理由让我们放弃陷入上述想法。</p>
<ol>
<li>概率必须介于0和1之间，但是我们在上面估计出来的平滑函数的输出结果却不能保证如此，即使我们的指示变量$y_i$不是0就是1；</li>
<li>另一种情况是，我们可以更好地利用这个事实，即我们试图通过更显式地模拟概率来估计概率。</li>
</ol>
<p>假设$Pr(Y=1|X=x)=p(x;\theta)$,$p$是参数为$\theta$的函数。进一步，假设我们的所有观测都是相互独立的，那么条件似然函数可以写成：</p>
<p>$$\prod _{i=1}^nPr(Y=y_i|X=x_i)=\prod _{i=1}^np(x_i;\theta)^{y_i}(1-p(x_i;\theta))^{1-y_i}$$</p>
<p>回忆一下，对于一系列的伯努利试验$y_1,y_2,\cdots,y_n$，如果成功的概率都是常数$p$，那么似然概率为：</p>
<p>$$\prod _{i=1}^n p^{y_i}(1-p)^{1-y_i}$$</p>
<p>我们知道，当$p=\hat{p}=\frac{1}{n}\sum _{i=1}^ny_i$时，似然概率取得最大值。如果每一个试验都有对应的成功概率$p_i$，那么似然概率就变成了</p>
<p>$$\prod _{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i}$$</p>
<p>不添加任何约束的通过最大化似然函数来估计上述模型是没有意义的。当$\hat{p_i}=1$的时候，$y_i=1$，当$\hat{p_i}=0$的时候，$y_i=0$。我们学不到任何东西。如果我们假设所有的$p_i$不是任意的数字，而是相互连接在一起的，这些约束给我们提供了一个很重要的参数，我们可以通过这个约束来求得似然函数的最大值。对于我们正在讨论的这种模型，约束条件就是$p_i=p(x_i;\theta)$，当$x_i$相同的时候，$p_i$也必须相同。因为我们假设的$p$是未知的，因此似然函数是参数为$\theta$的函数，我们可以通过估计$\theta$来最大化似然函数。</p>
<h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p>总结一下：我们有一个二分输出变量$Y$，我们希望构造一个关于$x$的函数来计算$Y$的条件概率$Pr（{Y=1|X=x}）$，所有的未知参数都可以通过最大似然法来估计。到目前为止，你不会惊讶于发现统计学家们通过问自己“我们如何使用线性回归来解决这个问题”。</p>
<ul>
<li>最简单的一个想法就是令$p(x)$为线性函数。无论$x$在什么位置，$x$每增加一个单位，$p$的变化量是一样的。由于线性函数不能保证预测结果位于0和1之间，因此从概念上线性函数就不适合。另外，在很多情况下，根据我们的经验可知，当$p$很大的时候，对于$p$的相同的变化量，$x$的变化量将会大于$p$在1/2附近的变化量。线性函数做不到这样。</li>
<li>另一个最直观的想法是令$log\ p(x)$为$x$的线性函数。但是对数函数只在一个方向上是无界的，因此也不符合要求。</li>
<li>最后，我们选择了$p$经过logit变换以后的函数$ln\frac{p}{1-p}$。这个函数就很好啊，满足了我们的所有需求。</li>
</ul>
<p>最后一个选择就是我们所说的逻辑回归。一般的，逻辑回归的模型可以表示为如下形式：</p>
<p>$$ln\frac{p(x)}{1-p(x)}=\beta_0+x\beta$$</p>
<p>根据上式，解出$p$</p>
<p>$$p(x;b,w)=\frac{e^{\beta_0+x\beta}}{1+e^{\beta_0+x\beta}}=\frac{1}{1+e^{-(\beta_0+x\beta)}}$$</p>
<p>为了最小化错分率，当$p\ge 0.5$的时候，我们预测$Y=1$，否则$Y=0$。这意味着当$\beta_0+x\beta$非负的时候，预测结果为1，否则为预测结果为0.因此，逻辑回归为我么提供了一个线性分类器。决策边界是$\beta_0+x\beta=0$，当$x$是一维的时候，决策边界是一个点，当$x$是二维的时候，决策边界是一条直线，以此类推。空间中某个点到决策边界的距离为$\beta_0/||\beta||+x\cdot\beta/||\beta||$.逻辑回归不仅告诉我们两个类的决策边界，还以一种独特的方式根据样本点到决策边界的距离给出该点分属于某类的概率。当$||\beta||$越大的时候，概率取极端值（0或1）的速度就越快。上述说明使得逻辑回归不仅仅是一个分类器，它能做出更简健壮、更详细的预测，并能以一种不同的方式进行拟合;但那些强有力的预测可能是错误的。</p>
<h2 id="似然函数和逻辑回归"><a href="#似然函数和逻辑回归" class="headerlink" title="似然函数和逻辑回归"></a>似然函数和逻辑回归</h2><p>因为逻辑回归的预测结果是概率，而不是类别，因此我们可以用似然函数来拟合模型。对于每一个样本点，我们有一个特征向量$x_i$，这个向量的维度就是特征的个数。同时还有一个观测类别$y_i$。当$y_i=1$的时候，该类的概率为$p$，否则为$1-p$。因此，似然函数为：</p>
<p>$$L(\beta_0,\beta) = \prod _{i=1}^np(x_i)^{y_i}(1-p(x_i))^{1-y_i}$$</p>
<p>对数似然函数为：</p>
<p>$$\begin{aligned} \ell(\beta_0,\beta) &amp;= \sum_{i=1}^ny_iln(p(x_i))+(1-y_i)ln(1-p(x_i)) \&amp;=\sum_{i=1}^n(y_i(ln\frac{p(x_i)}{1-p(x_i)})+ln(1-p(x_i))) \&amp;=\sum_{i=1}^ny_i(\beta_0+x_i\beta)-ln(1+e^{\beta_0+x_i\beta}) \end{aligned}$$</p>
<p> 为了表示方便，统一将$\beta_0,\beta$表示成$\beta$,则$\ell$对$\beta$的一阶导数为： </p>
<p>$$\begin{aligned} \frac{\partial \ell}{\partial \beta} &amp;=\sum _{i=1}^n[y_i-\frac{e^{\beta^Tx_i}}{1+\beta^Tx_i}]x_i\&amp; = \sum _{i=1}^n(y_i-p_i)x_i\end{aligned}$$</p>
<h2 id="多分类逻辑回归"><a href="#多分类逻辑回归" class="headerlink" title="多分类逻辑回归"></a>多分类逻辑回归</h2><p>如果$Y$有多个类别，我们仍然可以使用逻辑回归。假如有$k$个类别，分别是$0,1,\cdots,k-1$，对于每一个类$k$，其都有对应的$\beta_0$和$\beta$，每个类对应的概率为:</p>
<p>$$P(Y=c|X=x)=\frac{e^{\beta_0^c+x\beta^c}}{\sum e^{\beta_0^c+x\beta^c}}$$</p>
<p>观察上式可以发现，二分类逻辑回归求是多分类逻辑回归的特例.</p>
<blockquote>
<p>在这里，读者可能比较好奇，根据上式，二分类逻辑回归的分母中的1是怎么来的。其实，无论有多少个类，我们总是将第一类的系数设置为0，那么类别为0的那部分在分母中对应的就是1.这样做对模型的通用性没有任何影响。</p>
<p>有读者可能会问，为什么偏要把第一个类的系数设置为0，而不是其他的类。事实上，你可以设置任何一个类的系数为0，并且最终计算出来的结果都是一样的。所以，按照惯例，我们都是把第一个类的系数设置为0.</p>
</blockquote>
<h2 id="牛顿法求解参数"><a href="#牛顿法求解参数" class="headerlink" title="牛顿法求解参数"></a>牛顿法求解参数</h2><p>为了求出待估参数$\beta$，我们利用Newton-Raphson算法。首先对对数似然函数求二阶偏导： </p>
<p>$$\frac{\partial ^2\ell (\beta)}{\partial \beta \partial \beta^T}=-\sum_{i=1}^nx_ix_i^Tp_i(1-p_i)$$ </p>
<p>注意，上面的$x_i$是个向量，也就是上面所说的特征向量，维度为特征个数加一。即假设原始数据为$n\times m$矩阵，其中n表示观测数，m表示特征数。则$x_i$的长度为m+1。根据上述说明，上面的二阶偏导实际上是一个$(m+1)\times (m+1)$的矩阵。</p>
<p>如果给定一个$\hat{\beta}^{old}$，则一步牛顿迭代为（梯度下降）：</p>
<p>$$\hat{\beta}^{new}=\hat{\beta}^{old}-(\frac{\partial ^2\ell (\beta)}{\partial \beta \partial \beta^T})^{-1} \cdot\frac{\partial \ell({\beta})}{\partial \beta}$$</p>
<p>将上述式子表示成矩阵的形式就是：</p>
<p>$$\frac{\partial \ell(\beta)}{\partial \beta}=X^T(y-p)$$</p>
<p>$$\frac{\partial ^2\ell (\beta)}{\partial \beta \partial \beta^T}=-X^TWX$$</p>
<p>其中，$X$为原始自变量矩阵，$y$为类别向量，$p$为预测概率向量，$W$是一个$n\times n$对角矩阵，第$i$个元素取值为$p(x_i,\hat{\beta}^{old})(1-p(x_i,\hat{\beta}^{old}))$.</p>
<p>联立上述两个式子，可以得出参数的迭代公式：</p>
<p>$$\begin{aligned}\hat{\beta}^{new} &amp;=\hat{\beta}^{old}+(X^TWX)^{-1}X^T(y-p) \ &amp;=(X^TWX)^{-1}X^TW(X\hat{\beta^{old}}+W^{-1}(y-p)) \ &amp;=(X^TWX)^{-1}X^TWz \end{aligned}$$</p>
<p>其中，$z=X\hat{\beta^{old}}+W^{-1}(y-p)$.</p>
<p>实际上，$X^TWX$是一个黑塞矩阵</p>
<p>$$H(\beta) = \frac{\partial ^2\ell (\beta)}{\partial \beta \partial \beta^T}$$</p>
<p>即目标函数对$\beta$的二阶偏导，那么，上述迭代公式也可以写作：</p>
<p>$$\begin{aligned} \hat{\beta}^{new} &amp;=\hat{\beta}^{old} - H(\beta)^{-1}\frac{\partial \ell(\beta)}{\beta} \ &amp;= \hat{\beta}^{old} - H(\beta)^{-1}X^T(y-p) \end{aligned} $$ </p>
<p>上述是我们熟悉的牛顿迭代公式。</p>
<p>矩阵相乘的计算不算复杂，但是当数据量上升以后，黑塞矩阵的求逆就非常复杂了，因此衍生出许多拟牛顿算法，本节不讨论优化算法。</p>
<p>很明显，本例的目标函数就是对数似然函数$\ell(\beta)$，也就是求其最大值。然而，很多同学已经习惯了牛顿法求最小值，因此，为了大家看着方便，下面介绍梯度下降法求解逻辑回归。</p>
<p>只需要在上述似然函数前面加一个负号，本例就变成了一个梯度下降的问题了。为了形式上好看，还可以在前面对数似然函数求一个均值，即除以样本量。</p>
<p>假设$J(\beta)$是我们的目标函数，则</p>
<p>$$<br>\begin{aligned}<br>J(\beta) &amp;= -\frac{1}{n}\ell(\beta) \<br>&amp;=-\frac{1}{n}\sum_{i=1}^ny_iln(p(x_i))+(1-y_i)ln(1-p(x_i))<br>\end{aligned}<br>$$</p>
<p>此时我们的梯度公式就变成了：</p>
<p>$$\frac{\partial J(\beta)}{\partial \beta}=-\frac{1}{n}\sum _{i=1}^n(y_i-p_i)x_i=\frac{1}{n}\sum _{i=1}^n(p_i-y_i)x_i$$</p>
<p>我们的二阶偏导数就变成了</p>
<p>$$\frac{\partial ^2J(\beta)}{\partial \beta\partial\beta^T} =\frac{1}{n} \sum_{i=1}^nx_ix_i^Tp_i(1-p_i)$$</p>
<p>那么，此时为了求得我们的回归系数，即求使得$J(\beta)$最小的系数。牛顿迭代公式就变成了：</p>
<p>$$\hat{\beta}^{new} = \hat{\beta}^{old}-H^{-1}\nabla=\hat{\beta}^{old}-\frac{1}{n}(X^T\cdot diag(p)\cdot diag(1-p) \cdot X)^{-1}\cdot \frac{1}{n}X^T(p-y)$$</p>
<p>按照上述思路，编程实现逻辑回归求解是比较简单的。</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#迭代函数</span></span><br><span class="line"><span class="hljs-keyword">from</span> numpy <span class="hljs-keyword">import</span> *</span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span><span class="hljs-params">(x)</span>:</span></span><br><span class="line">    <span class="hljs-keyword">return</span> <span class="hljs-number">1.0</span>/(<span class="hljs-number">1.0</span>+exp(-x))</span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">LogitReg</span><span class="hljs-params">(x,y,tol = <span class="hljs-number">0.001</span>,maxiter = <span class="hljs-number">1000</span>)</span>:</span></span><br><span class="line">    samples,features = x.shape  <span class="hljs-comment">#分别表示观测样本数量和特征数量</span></span><br><span class="line">    features += <span class="hljs-number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment">#全部转换为矩阵</span></span><br><span class="line">    xdata = array(ones((samples,features)))</span><br><span class="line">    xdata[:,<span class="hljs-number">0</span>] = <span class="hljs-number">1</span></span><br><span class="line">    xdata[:,<span class="hljs-number">1</span>:] = x</span><br><span class="line">    xdata = mat(xdata)  <span class="hljs-comment">#sample行，features列的输入</span></span><br><span class="line">    </span><br><span class="line">    y = mat(y.reshape(samples,<span class="hljs-number">1</span>))  <span class="hljs-comment">#label，一个长度为samples的向量</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment">#首先初始化beta，令所有的系数为1,生成一个长度为features的列向量</span></span><br><span class="line">    beta = mat(zeros((features,<span class="hljs-number">1</span>)))</span><br><span class="line">    </span><br><span class="line">    iternum = <span class="hljs-number">0</span> <span class="hljs-comment">#迭代计数器</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment">#计算初始损失</span></span><br><span class="line">    </span><br><span class="line">    loss0 = float(<span class="hljs-string">'inf'</span>)</span><br><span class="line">    J = []</span><br><span class="line">    <span class="hljs-keyword">while</span> iternum &lt; maxiter:</span><br><span class="line">        <span class="hljs-keyword">try</span>:</span><br><span class="line">            p = sigmoid(xdata*beta) <span class="hljs-comment">#计算似然概率</span></span><br><span class="line">            nabla = <span class="hljs-number">1.0</span>/samples*xdata.T*(p-y)   <span class="hljs-comment">#计算梯度</span></span><br><span class="line">            H = <span class="hljs-number">1.0</span>/samples*xdata.T*diag(p.getA1())* diag((<span class="hljs-number">1</span>-p).getA1())*xdata  <span class="hljs-comment">#计算黑塞矩阵</span></span><br><span class="line">            </span><br><span class="line">            loss = <span class="hljs-number">1.0</span>/samples*sum(-y.getA1()*log(p.getA1())-(<span class="hljs-number">1</span>-y).getA1()*log((<span class="hljs-number">1</span>-p).getA1())) <span class="hljs-comment">#计算损失</span></span><br><span class="line">            J.append(loss)</span><br><span class="line">            beta =beta -  H.I * nabla  <span class="hljs-comment">#更新参数</span></span><br><span class="line">            iternum += <span class="hljs-number">1</span> <span class="hljs-comment">#迭代器加一</span></span><br><span class="line">            <span class="hljs-keyword">if</span> loss0 - loss &lt; tol:</span><br><span class="line">                <span class="hljs-keyword">break</span></span><br><span class="line">            loss0 = loss</span><br><span class="line">        <span class="hljs-keyword">except</span>:</span><br><span class="line">            H = H + <span class="hljs-number">0.0001</span></span><br><span class="line">            <span class="hljs-comment">#通常当黑塞矩阵奇异的时候，将矩阵加上一个很小的常数。</span></span><br><span class="line">            <span class="hljs-keyword">break</span></span><br><span class="line">        </span><br><span class="line">    <span class="hljs-keyword">return</span> beta,J</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#预测函数</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predictLR</span><span class="hljs-params">(data,beta)</span>:</span></span><br><span class="line">    data = array(data)</span><br><span class="line">    <span class="hljs-keyword">if</span> len(data.shape) == <span class="hljs-number">1</span>:</span><br><span class="line">        length = len(data)</span><br><span class="line">        newdata = tile(<span class="hljs-number">0</span>,length+<span class="hljs-number">1</span>)</span><br><span class="line">        newdata[<span class="hljs-number">0</span>] = <span class="hljs-number">1</span></span><br><span class="line">        newdata[<span class="hljs-number">1</span>:] = data</span><br><span class="line">        newdata = mat(newdata)</span><br><span class="line">        <span class="hljs-keyword">pass</span></span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        shape = data.shape</span><br><span class="line">        newdata = zeros((shape[<span class="hljs-number">0</span>],shape[<span class="hljs-number">1</span>]+<span class="hljs-number">1</span>))</span><br><span class="line">        newdata[:,<span class="hljs-number">0</span>] = <span class="hljs-number">1</span></span><br><span class="line">        newdata[:,<span class="hljs-number">1</span>:] = data</span><br><span class="line">        newdata = mat(newdata)</span><br><span class="line">    <span class="hljs-keyword">return</span> sigmoid(newdata*beta)</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="hljs-string">'df.csv'</span>,header=<span class="hljs-keyword">None</span>)</span><br><span class="line">df = array(df)</span><br><span class="line">df.shape</span><br><span class="line">xdata = df[:,:<span class="hljs-number">3</span>]</span><br><span class="line">ydata = df[:,<span class="hljs-number">3</span>]</span><br><span class="line"></span><br><span class="line">beta,J = LogitReg(xdata,ydata) <span class="hljs-comment">#拟合</span></span><br><span class="line"></span><br><span class="line">testdata = xdata[<span class="hljs-number">1</span>:<span class="hljs-number">10</span>,]</span><br><span class="line"></span><br><span class="line">predictLR(testdata,beta)</span><br><span class="line"></span><br><span class="line">matrix([[ <span class="hljs-number">0.4959212</span> ],</span><br><span class="line">        [ <span class="hljs-number">0.44642627</span>],</span><br><span class="line">        [ <span class="hljs-number">0.47419207</span>],</span><br><span class="line">        [ <span class="hljs-number">0.42209742</span>],</span><br><span class="line">        [ <span class="hljs-number">0.41802565</span>],</span><br><span class="line">        [ <span class="hljs-number">0.51283217</span>],</span><br><span class="line">        [ <span class="hljs-number">0.44833226</span>],</span><br><span class="line">        [ <span class="hljs-number">0.41252982</span>],</span><br><span class="line">        [ <span class="hljs-number">0.47853786</span>]])</span><br></pre></td></tr></table></figure>

<blockquote>
<p><a href="http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf" target="_blank" rel="noopener">http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf</a></p>
<p>[1]周志华.机器学习[M].清华大学出版社,2016.</p>
<p>[2]李航著.统计学习方法[M].清华大学出版社,2012.</p>
</blockquote>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-image">
        <a href="/2018/05/02/囚徒困境/" class="image is-7by1">
            <img class="thumbnail" src="/thumbnails/qiutu.jpg" alt="囚徒困境">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2018-05-01T16:00:00.000Z">2018-05-02</time>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    3 分钟 读完 (大约 487 个字)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2018/05/02/囚徒困境/">囚徒困境</a>
            
        </h1>
        <div class="content">
            <p>2018年4月27日18时10分许，米脂县第三中学学生放学途中遭犯罪嫌疑人袭击,造成19名学生受伤，其中7人死亡。</p>
<p>有同学会说，嫌犯只有一个人，却导致了7人死亡的惨剧，如果大家一起上前反抗，肯定能阻止这次犯罪，将伤害降到最低。</p>
<p>这让我想起了囚徒困境，因为人都是理智的，都会做出对自有利的选择，所以没有人上前反抗，大家只顾着跑，即使有人受了伤害。</p>
<p>为此我们做出如下假设：</p>
<ol>
<li>A和B同时反抗，则歹徒被制服，无人伤亡；</li>
<li>A上前反抗，B逃跑，则A死亡，B安全；</li>
<li>A和B同时逃跑，则A和B有可能死亡。</li>
</ol>
<p>绘制出如下二维表：</p>
<p><img src="/picture/PrisonerDilemma.png" alt="囚徒困境"></p>
<p>对于甲，有两种选择，即</p>
<p>1.上前阻止</p>
<ul>
<li>如果乙上前阻止，则甲安全，乙安全</li>
<li>如果乙逃跑，则甲死亡，乙安全</li>
</ul>
<p>2.逃跑</p>
<ul>
<li>如果乙上前阻止，则甲安全，乙死亡</li>
<li>如果乙逃跑，则甲可能死亡，乙可能死亡</li>
</ul>
<p>即对于甲来说，如果其上前阻止，那么乙的后果都会比甲要好，即乙都是安全的，那么甲不可能选择上前阻止。因此甲肯定选择逃跑。</p>
<p>而如果甲选择了逃跑，乙这时只有两种方案：</p>
<ol>
<li>上前阻止，则甲安全，乙死亡</li>
<li>逃跑，则甲可能死亡，乙可能死亡  </li>
</ol>
<p>以上两个方案中，对乙最有利的是方案2，即乙肯定也会选择逃跑。</p>
<p>分析到此结束，不难理解，所有人都只顾着才跑，而没有人上前反抗了。</p>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-image">
        <a href="/2018/04/27/新浪爬虫/" class="image is-7by1">
            <img class="thumbnail" src="/thumbnails/sina.jpg" alt="新浪新闻爬虫">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2018-04-26T16:00:00.000Z">2018-04-27</time>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    4 分钟 读完 (大约 669 个字)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2018/04/27/新浪爬虫/">新浪新闻爬虫</a>
            
        </h1>
        <div class="content">
            <h2 id="获取标题列表"><a href="#获取标题列表" class="headerlink" title="获取标题列表"></a>获取标题列表</h2><p>新浪新闻总是不断更新的，因此有可能每次查询的返回新闻数量都是不一样的，所以我们最好不要通过设置固定的条数取爬取。</p>
<p><img src="/picture/sina_scrapy.png" alt="sina_scrapy"></p>
<p>从上图可以看出，只要有“下一页”存在，我们就可以从下一页获取链接，从而开始下一页的爬取。</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> requests</span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="hljs-keyword">import</span> re</span><br><span class="line"><span class="hljs-keyword">import</span> time</span><br><span class="line"><span class="hljs-keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#第一页的URL</span></span><br><span class="line">startURL = <span class="hljs-string">'http://search.sina.com.cn/?c=news&amp;q=%D0%F0%C0%FB%D1%C7+%BF%D5%CF%AE&amp;range=all&amp;time=2018&amp;stime=&amp;etime=&amp;num=20'</span></span><br><span class="line"></span><br><span class="line">url = startURL</span><br><span class="line">allNewsURL = []</span><br><span class="line">errorPages = []</span><br><span class="line">count = <span class="hljs-number">1</span></span><br><span class="line"><span class="hljs-keyword">while</span> url:</span><br><span class="line">    <span class="hljs-keyword">try</span>:</span><br><span class="line">        res = requests.get(url)</span><br><span class="line">        print(<span class="hljs-string">'已经爬取了第'</span>,count,<span class="hljs-string">'页！'</span>)</span><br><span class="line">        print(<span class="hljs-string">'URL:'</span>,url)</span><br><span class="line">        soup = BeautifulSoup(res.text,<span class="hljs-string">'html.parser'</span>)</span><br><span class="line">        <span class="hljs-keyword">for</span> news <span class="hljs-keyword">in</span> soup.findAll(<span class="hljs-string">'h2'</span>):</span><br><span class="line">            allNewsURL.append(news.find(<span class="hljs-string">'a'</span>).get(<span class="hljs-string">'href'</span>))</span><br><span class="line">        pages = soup.find(<span class="hljs-string">'div'</span>,&#123;<span class="hljs-string">'id'</span>:<span class="hljs-string">"_function_code_page"</span>&#125;).findAll(<span class="hljs-string">'a'</span>)</span><br><span class="line">        <span class="hljs-keyword">if</span> pages[<span class="hljs-number">-1</span>].get(<span class="hljs-string">'title'</span>) == <span class="hljs-string">'下一页'</span>:</span><br><span class="line">            url = <span class="hljs-string">'http://search.sina.com.cn'</span> + pages[<span class="hljs-number">-1</span>].get(<span class="hljs-string">'href'</span>)</span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            <span class="hljs-keyword">break</span></span><br><span class="line">        time.sleep(np.random.rand() + <span class="hljs-number">2</span>)</span><br><span class="line">        count += <span class="hljs-number">1</span></span><br><span class="line">    <span class="hljs-keyword">except</span> AttributeError:</span><br><span class="line">        print(<span class="hljs-string">'ip被封！'</span>)</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">with</span> open(<span class="hljs-string">'xuliyakongxi.csv'</span>,<span class="hljs-string">'a'</span>) <span class="hljs-keyword">as</span> f:</span><br><span class="line">    <span class="hljs-keyword">for</span> url <span class="hljs-keyword">in</span> allNewsURL:</span><br><span class="line">        f.write(url+<span class="hljs-string">'\n'</span>)</span><br></pre></td></tr></table></figure>

<h2 id="获取文章详情"><a href="#获取文章详情" class="headerlink" title="获取文章详情"></a>获取文章详情</h2><p>获取文章详情需要注意以下两点：</p>
<ul>
<li>不同的新闻来源具有不同的channel。</li>
<li>不是所有的新闻都具有关键词、源链接等信息，因此需要进行异常处理。</li>
</ul>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line">URLs = []</span><br><span class="line"><span class="hljs-keyword">with</span> open(<span class="hljs-string">'xuliyakongxi.csv'</span>,<span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> f:</span><br><span class="line">    <span class="hljs-keyword">for</span> url <span class="hljs-keyword">in</span> f.readlines():</span><br><span class="line">        url = url.strip(<span class="hljs-string">'\n'</span>)</span><br><span class="line">        URLs.append(url)</span><br><span class="line"></span><br><span class="line">allNews = []</span><br><span class="line">count = <span class="hljs-number">0</span></span><br><span class="line">comments = []</span><br><span class="line">notMatch= []</span><br><span class="line"><span class="hljs-keyword">for</span> url <span class="hljs-keyword">in</span> URLs:</span><br><span class="line">    <span class="hljs-keyword">try</span>:</span><br><span class="line">        currentNews = []</span><br><span class="line">        request = requests.get(url,timeout=<span class="hljs-number">4</span>)</span><br><span class="line">        request.encoding = <span class="hljs-string">'utf-8'</span></span><br><span class="line">        soup = BeautifulSoup(request.text,<span class="hljs-string">'html.parser'</span>)</span><br><span class="line"></span><br><span class="line">        newsid = re.search(<span class="hljs-string">'doc-i(.+).shtml'</span>,url).group(<span class="hljs-number">1</span>) <span class="hljs-comment">#新闻ID</span></span><br><span class="line">        </span><br><span class="line">        allChannels = [<span class="hljs-string">'gz'</span>,<span class="hljs-string">'shc'</span>,<span class="hljs-string">'gn'</span>,<span class="hljs-string">'gj'</span>,<span class="hljs-string">'jc'</span>,<span class="hljs-string">'cj'</span>,<span class="hljs-string">'kj'</span>,<span class="hljs-string">'sh'</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment">#不同来源的新闻对应不同的channel，此处将所有的channel进行遍历，并加入了异常处理</span></span><br><span class="line">        <span class="hljs-keyword">for</span> channel <span class="hljs-keyword">in</span> allChannels:</span><br><span class="line">            <span class="hljs-keyword">try</span>:</span><br><span class="line">                comm = <span class="hljs-string">'http://comment5.news.sina.com.cn/page/info?version=1&amp;format=json&amp;channel='</span>+channel+<span class="hljs-string">'&amp;newsid=comos-'</span>+ newsid</span><br><span class="line">                res_comm = requests.get(comm)</span><br><span class="line">                commentsDesc = json.loads(res_comm.text)</span><br><span class="line">                engageNum = commentsDesc[<span class="hljs-string">'result'</span>][<span class="hljs-string">'count'</span>][<span class="hljs-string">'total'</span>] <span class="hljs-comment">#参与</span></span><br><span class="line">                commentsNum = commentsDesc[<span class="hljs-string">'result'</span>][<span class="hljs-string">'count'</span>][<span class="hljs-string">'show'</span>] <span class="hljs-comment">#评论</span></span><br><span class="line">                comments.append(commentsDesc[<span class="hljs-string">'result'</span>][<span class="hljs-string">'count'</span>])</span><br><span class="line">            <span class="hljs-keyword">except</span>:</span><br><span class="line">                <span class="hljs-keyword">if</span> channel == <span class="hljs-string">'sh'</span>:</span><br><span class="line">                    print(<span class="hljs-string">'没有匹配'</span>)</span><br><span class="line">                    engageNum = <span class="hljs-keyword">None</span></span><br><span class="line">                    commentsNum = <span class="hljs-keyword">None</span></span><br><span class="line">                    notMatch.append(comm)</span><br><span class="line">                <span class="hljs-keyword">else</span>:</span><br><span class="line">                    <span class="hljs-keyword">continue</span></span><br><span class="line">            <span class="hljs-keyword">else</span>:</span><br><span class="line">                <span class="hljs-keyword">break</span>;</span><br><span class="line">        </span><br><span class="line">        titleAndKeywords = soup.find(<span class="hljs-string">'title'</span>).text </span><br><span class="line">        tk = titleAndKeywords.split(<span class="hljs-string">'|'</span>)</span><br><span class="line">        title = tk[<span class="hljs-number">0</span>] <span class="hljs-comment">#标题</span></span><br><span class="line">        <span class="hljs-keyword">if</span> len(tk) &gt; <span class="hljs-number">2</span>:</span><br><span class="line">            keywords = tk[<span class="hljs-number">1</span>:(len(tk)<span class="hljs-number">-1</span>)]</span><br><span class="line">            keywords.append(tk[<span class="hljs-number">-1</span>].split(<span class="hljs-string">'_'</span>)[<span class="hljs-number">0</span>]) <span class="hljs-comment">#关键词</span></span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            keywords = tk[<span class="hljs-number">-1</span>].split(<span class="hljs-string">'_'</span>)[<span class="hljs-number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        paras = soup.find(<span class="hljs-string">'div'</span>,&#123;<span class="hljs-string">'class'</span>:<span class="hljs-string">'article'</span>&#125;).text <span class="hljs-comment">#正文，存在脏数据</span></span><br><span class="line">        </span><br><span class="line">        <span class="hljs-keyword">try</span>:</span><br><span class="line">            date = soup.find(<span class="hljs-string">'span'</span>,&#123;<span class="hljs-string">'class'</span>:<span class="hljs-string">'date'</span>&#125;).text <span class="hljs-comment">#日期</span></span><br><span class="line">        <span class="hljs-keyword">except</span>:</span><br><span class="line">            date = <span class="hljs-keyword">None</span></span><br><span class="line">            </span><br><span class="line">        <span class="hljs-keyword">try</span>:</span><br><span class="line">            sourceUrl = soup.find(<span class="hljs-string">'a'</span>,&#123;<span class="hljs-string">'class'</span>:<span class="hljs-string">'source'</span>&#125;).get(<span class="hljs-string">'href'</span>) <span class="hljs-comment">#源链接</span></span><br><span class="line">            source = soup.find(<span class="hljs-string">'a'</span>,&#123;<span class="hljs-string">'class'</span>:<span class="hljs-string">'source'</span>&#125;).text <span class="hljs-comment">#来源</span></span><br><span class="line">        <span class="hljs-keyword">except</span>:</span><br><span class="line">            sourceUrl = <span class="hljs-keyword">None</span></span><br><span class="line">            source = <span class="hljs-keyword">None</span></span><br><span class="line">            </span><br><span class="line">        currentNews.append(url) <span class="hljs-comment">#url</span></span><br><span class="line">        currentNews.append(title)  <span class="hljs-comment">#标题</span></span><br><span class="line">        currentNews.append(keywords) <span class="hljs-comment">#关键词</span></span><br><span class="line">        currentNews.append(paras) <span class="hljs-comment">#正文</span></span><br><span class="line">        currentNews.append(date) <span class="hljs-comment">#日期</span></span><br><span class="line">        currentNews.append(sourceUrl) <span class="hljs-comment">#源链接</span></span><br><span class="line">        currentNews.append(source) <span class="hljs-comment">#来源</span></span><br><span class="line">        currentNews.append(engageNum) <span class="hljs-comment">#参与人数</span></span><br><span class="line">        currentNews.append(commentsNum) <span class="hljs-comment">#评论人数</span></span><br><span class="line"></span><br><span class="line">        allNews.append(currentNews)</span><br><span class="line">        count += <span class="hljs-number">1</span></span><br><span class="line"></span><br><span class="line">        print(<span class="hljs-string">'已经爬取'</span>,str(count),<span class="hljs-string">'个页面.'</span>)</span><br><span class="line">        print(<span class="hljs-string">'comment URL:'</span>,comm)</span><br><span class="line">        time.sleep(<span class="hljs-number">1</span>+abs(np.random.rand()))</span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">if</span> count % <span class="hljs-number">20</span> == <span class="hljs-number">0</span>:</span><br><span class="line">            time.sleep(<span class="hljs-number">5</span>)</span><br><span class="line"><span class="hljs-comment">#         if count &gt; 36:</span></span><br><span class="line"><span class="hljs-comment">#             break;</span></span><br><span class="line">    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:</span><br><span class="line">        print(e)</span><br><span class="line">        <span class="hljs-keyword">continue</span></span><br></pre></td></tr></table></figure>


        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-image">
        <a href="/2018/04/26/岭回归/" class="image is-7by1">
            <img class="thumbnail" src="/thumbnails/ridge.jpg" alt="改进的删一法交叉验证与逐步回归比较">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2018-04-25T16:00:00.000Z">2018-04-26</time>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    23 分钟 读完 (大约 3444 个字)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2018/04/26/岭回归/">改进的删一法交叉验证与逐步回归比较</a>
            
        </h1>
        <div class="content">
            <h2 id="定义基本函数"><a href="#定义基本函数" class="headerlink" title="定义基本函数"></a>定义基本函数</h2><p>下面主要定义cholesky分解函数，以及求解线性方程组的两个函数和标准化函数。</p>
<p>代码分割线</p>
<hr>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="hljs-comment">## mchol函数将对称方阵分解为一个下三角矩阵乘以该矩阵转置的形式,</span></span><br><span class="line"><span class="hljs-comment">#函数返回值为下三角矩阵</span></span><br><span class="line"><span class="hljs-comment">#输入：欲分解的矩阵x</span></span><br><span class="line"><span class="hljs-comment">#输出：cholesky分解所得矩阵L</span></span><br><span class="line">mchol &lt;- <span class="hljs-keyword">function</span>(x)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="hljs-comment">#求矩阵x的行列数,m为行数,n为列数</span></span><br><span class="line">  mn &lt;- dim(x)</span><br><span class="line">  m &lt;- mn[<span class="hljs-number">1</span>]</span><br><span class="line">  n &lt;- mn[<span class="hljs-number">2</span>]</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#检验x是否为方阵</span></span><br><span class="line">  <span class="hljs-keyword">if</span>(m != n) </span><br><span class="line">  &#123;</span><br><span class="line">    <span class="hljs-keyword">return</span> (<span class="hljs-string">"Wrong dimensions of matrix!"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#检验x是否为对称矩阵</span></span><br><span class="line">  <span class="hljs-keyword">if</span>(sum(t(x) != x) &gt; <span class="hljs-number">0</span>) </span><br><span class="line">  &#123;</span><br><span class="line">    <span class="hljs-keyword">return</span> (<span class="hljs-string">"Input matrix is not symmetrical!"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#L为与x行列数相等的零矩阵，用于存放分解所得下三角矩阵</span></span><br><span class="line">  L &lt;- matrix(<span class="hljs-number">0</span>, m, m)</span><br><span class="line"></span><br><span class="line">  <span class="hljs-comment">#循环每进行一次,求解一列矩阵L的元素</span></span><br><span class="line">  <span class="hljs-comment">#矩阵x第i列和第i行之前的元素不再使用，相当于矩阵x减少一个维数，</span></span><br><span class="line">  <span class="hljs-comment">#故下述将循环所至第i列记为当前矩阵x和矩阵L的第一列</span></span><br><span class="line">  <span class="hljs-keyword">for</span>(i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:m)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="hljs-comment">#L的主对角线上第一个元素为x的主对角线上第一个元素开方</span></span><br><span class="line">    L[i,i] &lt;- sqrt(x[i,i])</span><br><span class="line">    <span class="hljs-keyword">if</span>(i &lt; m)</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="hljs-comment">#求当前矩阵L的第一列除第一个元素外的其他元素</span></span><br><span class="line">      L[(i+<span class="hljs-number">1</span>):m,i] &lt;- x[(i+<span class="hljs-number">1</span>):m,i]/L[i,i]</span><br><span class="line">      </span><br><span class="line">      <span class="hljs-comment">#矩阵L第一列（除第一个元素）乘以它的转置得到TLM用于更新矩阵x，效果同TLM%*%TLM</span></span><br><span class="line">      TLV &lt;- L[(i+<span class="hljs-number">1</span>):m,i] <span class="hljs-comment">#记录已求出第一列除第一个元素外剩下元素</span></span><br><span class="line">      TLM &lt;- matrix(TLV, m-i, m-i) <span class="hljs-comment">#TLV按列复制成矩阵</span></span><br><span class="line">      TLM &lt;- sweep(TLM, <span class="hljs-number">2</span>, TLV, <span class="hljs-string">"*"</span>)  </span><br><span class="line">      <span class="hljs-comment">#sweep(x， MARGIN， STATS， FUN=”-“， …) 对矩阵进行运算</span></span><br><span class="line">      <span class="hljs-comment">#MARGIN为1，表示行的方向上进行运算，</span></span><br><span class="line">      <span class="hljs-comment">#为2表示列的方向上运算(是指将参数从列的方向移下去算)</span></span><br><span class="line">      <span class="hljs-comment">#STATS是运算的参数，FUN为运算函数，默认是减法</span></span><br><span class="line">      </span><br><span class="line">      <span class="hljs-comment">#减少一个维数的矩阵x更新为原来对应位置上的元素减去TLM，为下一次循环做准备</span></span><br><span class="line">      x[(i+<span class="hljs-number">1</span>):m,(i+<span class="hljs-number">1</span>):m] &lt;- x[(i+<span class="hljs-number">1</span>):m,(i+<span class="hljs-number">1</span>):m] - TLM</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="hljs-comment">#矩阵的返回值为我们要求的下三角矩阵L</span></span><br><span class="line">  L  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">##mforwardsolve函数求解线性方程租Lx=b，其中L为下三角矩阵</span></span><br><span class="line"><span class="hljs-comment">#输入：下三角矩阵L，向量b</span></span><br><span class="line"><span class="hljs-comment">#输出：线性方程组的解x</span></span><br><span class="line">mforwardsolve &lt;- <span class="hljs-keyword">function</span>(L, b)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="hljs-comment">#求L的行列数,m为L的行数,n为L的列数</span></span><br><span class="line">  mn &lt;- dim(L)</span><br><span class="line">  m &lt;- mn[<span class="hljs-number">1</span>]</span><br><span class="line">  n &lt;- mn[<span class="hljs-number">2</span>]</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#判断L是否为方阵</span></span><br><span class="line">  <span class="hljs-keyword">if</span>(m != n) </span><br><span class="line">  &#123;</span><br><span class="line">    <span class="hljs-keyword">return</span> (<span class="hljs-string">"Wrong dimensions of matrix L!"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#判断L是否为下三角矩阵</span></span><br><span class="line">  <span class="hljs-keyword">for</span> (i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:(m-<span class="hljs-number">1</span>))</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="hljs-keyword">if</span>(sum(L[i,(i+<span class="hljs-number">1</span>):m] != <span class="hljs-number">0</span>) &gt; <span class="hljs-number">0</span>)<span class="hljs-comment">#逐行判断上三角是否全为0元素</span></span><br><span class="line">    &#123;</span><br><span class="line">      <span class="hljs-keyword">return</span> (<span class="hljs-string">"Matrix L must be a lower triangular matrix!"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#判断L的行数与b的长度是否相等</span></span><br><span class="line">  <span class="hljs-keyword">if</span>(m != length(b))</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="hljs-keyword">return</span> (<span class="hljs-string">"Wrong dimensions of matrix L or vector b!"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#0向量记录求解结果</span></span><br><span class="line">  x=rep(<span class="hljs-number">0</span>, m)</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#循环每进行一次,求解一个x中的元素，</span></span><br><span class="line">  <span class="hljs-comment">#看作矩阵L向量x向量b的维数减一</span></span><br><span class="line">  <span class="hljs-comment">#故下述将矩阵L的第i列记为当前矩阵L第一列，</span></span><br><span class="line">  <span class="hljs-comment">#将向量x向量b的第i个元素记为当前向量第一个元素</span></span><br><span class="line">  <span class="hljs-keyword">for</span>(i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:m)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="hljs-comment">#求当前循环中x的第一个元素</span></span><br><span class="line">    x[i] &lt;- b[i] / L[i,i]</span><br><span class="line">    <span class="hljs-comment">#降维后的b向量为原来位置上的元素减去当前矩阵L的第一列的乘积</span></span><br><span class="line">    <span class="hljs-keyword">if</span>(i &lt; m) </span><br><span class="line">    &#123;</span><br><span class="line">      b[(i+<span class="hljs-number">1</span>):m] &lt;- b[(i+<span class="hljs-number">1</span>):m] - x[i]*L[(i+<span class="hljs-number">1</span>):m,i]</span><br><span class="line">    &#125;      </span><br><span class="line">  &#125;</span><br><span class="line">  <span class="hljs-comment">#函数返回的x向量即为线性方程组的解</span></span><br><span class="line">  x  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">##mbacksolve函数求解线性方程租Lx=b，其中L为上三角矩阵</span></span><br><span class="line"><span class="hljs-comment">#输入：上三角矩阵L，向量b</span></span><br><span class="line"><span class="hljs-comment">#输出：线性方程组的解x</span></span><br><span class="line">mbacksolve &lt;- <span class="hljs-keyword">function</span>(L, b)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="hljs-comment">#求L的行列数,m为L的行数,n为L的列数</span></span><br><span class="line">  mn &lt;-dim(L)</span><br><span class="line">  m &lt;- mn[<span class="hljs-number">1</span>]</span><br><span class="line">  n &lt;- mn[<span class="hljs-number">2</span>]</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#判断L是否为方阵</span></span><br><span class="line">  <span class="hljs-keyword">if</span>(m != n)</span><br><span class="line">  &#123;  </span><br><span class="line">    <span class="hljs-keyword">return</span> (<span class="hljs-string">"Wrong dimensions of matrix L!"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#判断L是否为上三角矩阵</span></span><br><span class="line">  <span class="hljs-keyword">for</span> (i <span class="hljs-keyword">in</span> <span class="hljs-number">2</span>:m)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="hljs-keyword">if</span>(sum(L[i,<span class="hljs-number">1</span>:(i-<span class="hljs-number">1</span>)] != <span class="hljs-number">0</span>) &gt; <span class="hljs-number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="hljs-keyword">return</span> (<span class="hljs-string">"Matrix L must be a upper triangular matrix!"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#判断L的行数与b的列数是否相等</span></span><br><span class="line">  <span class="hljs-keyword">if</span>(m != length(b))</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="hljs-keyword">return</span> (<span class="hljs-string">"Wrong dimensions of matrix L or vector b!"</span>)</span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  x &lt;- rep(<span class="hljs-number">0</span>, m)</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#循环每进行一次,求解一个x中的元素，</span></span><br><span class="line">  <span class="hljs-comment">#看作矩阵L向量x向量b的维数减一</span></span><br><span class="line">  <span class="hljs-comment">#故下述将矩阵L的第i列记为当前矩阵L最后一列，</span></span><br><span class="line">  <span class="hljs-comment">#将向量x向量b的第i个元素记为当前向量最后一个元素</span></span><br><span class="line">  <span class="hljs-keyword">for</span>(i <span class="hljs-keyword">in</span> m:<span class="hljs-number">1</span>)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="hljs-comment">#求当前循环中x的最后一个元素</span></span><br><span class="line">    x[i] &lt;- b[i] / L[i,i]</span><br><span class="line">    <span class="hljs-comment">#降维后的向量b为原来位置上的元素减去刚才求出的</span></span><br><span class="line">    <span class="hljs-comment">#x元素与当前上三角矩阵L最后一列（除最后一个元素）的乘积</span></span><br><span class="line">    <span class="hljs-keyword">if</span>(i &gt; <span class="hljs-number">1</span>) </span><br><span class="line">    &#123;</span><br><span class="line">      b[(i-<span class="hljs-number">1</span>):<span class="hljs-number">1</span>] &lt;- b[(i-<span class="hljs-number">1</span>):<span class="hljs-number">1</span>] - x[i]*L[(i-<span class="hljs-number">1</span>):<span class="hljs-number">1</span>,i]</span><br><span class="line">    &#125;      </span><br><span class="line">  &#125;</span><br><span class="line">  <span class="hljs-comment">#函数返回值x向量即为线性方程组的解</span></span><br><span class="line">  x  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">##ridgereg函数用于实现岭回归参数beta的估计，</span></span><br><span class="line"><span class="hljs-comment">#参数x和y分别为回归方程的自变量和因变量,</span></span><br><span class="line"><span class="hljs-comment">#lambda为L2正则项的调节参数</span></span><br><span class="line"><span class="hljs-comment">#此函数求解线性方程租</span></span><br><span class="line"><span class="hljs-comment">#(t(x)%*%x+lambada)%*%beta=t(x)%*%y,</span></span><br><span class="line"><span class="hljs-comment">#将t(x)%*%x+lambada进行cholesky分解为R%*%t(R),</span></span><br><span class="line"><span class="hljs-comment">#forwardsolve求解L%*%d=t(x)%*%y,</span></span><br><span class="line"><span class="hljs-comment">#其中d=t(R)%*%beta,</span></span><br><span class="line"><span class="hljs-comment">#backsolve求解t(R)%*%beta=d,即得参数beta的估计值 </span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#输入：自变量x，因变量y，调节参数lambda</span></span><br><span class="line"><span class="hljs-comment">#输出：回归系数beta的估计值</span></span><br><span class="line">ridgereg &lt;- <span class="hljs-keyword">function</span>(lambda, x, y)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="hljs-comment">#y=data[,m]; x=data[,-m]</span></span><br><span class="line">  <span class="hljs-comment">#n为自变量矩阵行数,即n个样本,p为自变量矩阵列数,即p个参数</span></span><br><span class="line">  np &lt;- dim(x)</span><br><span class="line">  n &lt;- np[<span class="hljs-number">1</span>]</span><br><span class="line">  p &lt;- np[<span class="hljs-number">2</span>]</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#将自变量矩阵增加一列全1元素,以便于截距项的计算</span></span><br><span class="line">  x &lt;- as.matrix(cbind(rep(<span class="hljs-number">1</span>, n),x))</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#利用cholesky分解求取回归方程的参数beta的估计值  </span></span><br><span class="line">  V &lt;- t(x)%*%x + diag(c(<span class="hljs-number">0</span>, rep(lambda, p)))              </span><br><span class="line">  <span class="hljs-comment">#t(x)%*%x+lambda作为线性方程组的系数矩阵V</span></span><br><span class="line">  U &lt;- as.vector(t(x)%*%y)                           </span><br><span class="line">  R &lt;- mchol(V)                                           </span><br><span class="line">  <span class="hljs-comment">#调用mchol函数将系数矩阵V进行cholesky分解,V=R%*%t(R)</span></span><br><span class="line">  M &lt;- mforwardsolve(R, U)                                </span><br><span class="line">  <span class="hljs-comment">#使用前代法求解R%*%M=t(x)%*%y,其中M=t(R)%*%beta</span></span><br><span class="line">  mbacksolve(t(R), M)                                    </span><br><span class="line">  <span class="hljs-comment">#使用回代法求解t(R)%*%beta=M,即可得beta的估计值</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">##pred函数的参数b为参数向量,x为自变量,返回值为因变量的预测值</span></span><br><span class="line"><span class="hljs-comment">#输入：回归系数b向量,数据nx</span></span><br><span class="line"><span class="hljs-comment">#输出：因变量y的预测值</span></span><br><span class="line">pred &lt;- <span class="hljs-keyword">function</span>(b, nx)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="hljs-comment">#nx=prostate[1:2,1:8]</span></span><br><span class="line">  b &lt;- as.vector(b)</span><br><span class="line">  p &lt;- length(b) - <span class="hljs-number">1</span></span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#将数据矩阵nx重新排列，每一行为一个样品，</span></span><br><span class="line">  <span class="hljs-comment">#重排矩阵的原因是下面例子中调用的数据原结构为dataframe</span></span><br><span class="line">  nx &lt;- as.matrix(nx, ncol &lt;- p)</span><br><span class="line">  n &lt;- dim(nx)[<span class="hljs-number">1</span>]</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#计算预测值</span></span><br><span class="line">  apply(t(nx)*b[<span class="hljs-number">2</span>:(p+<span class="hljs-number">1</span>)], <span class="hljs-number">2</span>, sum) + b[<span class="hljs-number">1</span>]  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">##mridge函数用于实现删去一个样品的岭回归</span></span><br><span class="line">mridge=<span class="hljs-keyword">function</span>(i,lambda,x,y) </span><br><span class="line">&#123;</span><br><span class="line">  ridgereg(lambda,x[-i,],y[-i])</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">##cvridgeregerr函数用交叉验证实现岭回归，</span></span><br><span class="line"><span class="hljs-comment">#参数依次为调节参数lambda,自变量x(数据矩阵),因变量y,返回值为测试均方误差   </span></span><br><span class="line"><span class="hljs-comment">#输入：超参数lambda,自变量x(数据矩阵),因变量y</span></span><br><span class="line"><span class="hljs-comment">#输出：删一交叉验证岭回归测试均方误差</span></span><br><span class="line">cvridgeregerr&lt;-<span class="hljs-keyword">function</span>(lambda,x,y)</span><br><span class="line">&#123;  </span><br><span class="line">  <span class="hljs-comment">#lambda=1</span></span><br><span class="line">  np&lt;-dim(x)</span><br><span class="line">  n&lt;-np[<span class="hljs-number">1</span>]</span><br><span class="line">  p&lt;-np[<span class="hljs-number">2</span>]</span><br><span class="line">  <span class="hljs-comment">#矩阵中的元素作为第一个参数输入mridge，表示去掉的数据编号，</span></span><br><span class="line">  <span class="hljs-comment">#结果第i行为删去第i个样本的岭回归系数估计值</span></span><br><span class="line">  coe&lt;-t(apply(as.matrix(<span class="hljs-number">1</span>:n,ncol=<span class="hljs-number">1</span>),<span class="hljs-number">1</span>,mridge,lambda,x,y))</span><br><span class="line">  <span class="hljs-comment">#coe第i行和数据矩阵第i个样本做点对点相乘，对行求和，计算测试均方误差</span></span><br><span class="line">  mean((apply(coe*cbind(<span class="hljs-number">1</span>,x),<span class="hljs-number">1</span>,sum)-y)^<span class="hljs-number">2</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">##ridgeregerr函数用于计算训练均方误差  </span></span><br><span class="line"><span class="hljs-comment">#输入：岭回归超参数lambda，数据矩阵x，因变量y</span></span><br><span class="line"><span class="hljs-comment">#输出：训练均方误差</span></span><br><span class="line">ridgeregerr=<span class="hljs-keyword">function</span>(lambda,x,y)</span><br><span class="line">&#123;</span><br><span class="line">  mean((pred(ridgereg(lambda,x,y),x)-y)^<span class="hljs-number">2</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#矩阵标准化，即先减去列均值，再除以列标准差</span></span><br><span class="line">mystandard = <span class="hljs-keyword">function</span>(x)</span><br><span class="line">&#123;</span><br><span class="line">  mx = apply(x, <span class="hljs-number">2</span>, mean)</span><br><span class="line">  sdx = apply(x, <span class="hljs-number">2</span>, sd)</span><br><span class="line">  t = sweep(x, <span class="hljs-number">2</span>, mx, <span class="hljs-string">'-'</span>)</span><br><span class="line">  sweep(t, <span class="hljs-number">2</span>, sdx, <span class="hljs-string">'/'</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">##在不同的lambda下,比较训练均方误差和测试均方误差，</span></span><br><span class="line"><span class="hljs-comment">#以选取合适的调节参数lambda</span></span><br><span class="line"><span class="hljs-comment">#----------------选取lambda---------------</span></span><br><span class="line"><span class="hljs-keyword">library</span>(ElemStatLearn)</span><br><span class="line">x &lt;- as.matrix(prostate[ ,<span class="hljs-number">1</span>:<span class="hljs-number">8</span>])</span><br><span class="line">x = mystandard(x)  <span class="hljs-comment">#标准化x</span></span><br><span class="line">y &lt;- as.vector(prostate[ ,<span class="hljs-number">9</span>])</span><br><span class="line">LAM &lt;- seq(<span class="hljs-number">0.001</span>, <span class="hljs-number">10</span>, len=<span class="hljs-number">50</span>)</span><br><span class="line"><span class="hljs-comment">#计算岭回归50个模型的训练均方误差，将结果从list展开成向量</span></span><br><span class="line">err &lt;- unlist(lapply(LAM, ridgeregerr, x, y))</span><br><span class="line"><span class="hljs-comment">#计算岭回归50个模型的测试均方误差，将结果从list展开成向量</span></span><br><span class="line">pe &lt;- unlist(lapply(LAM, cvridgeregerr, x, y))</span><br><span class="line">x &lt;- rep(<span class="hljs-number">1</span>:<span class="hljs-number">50</span>, <span class="hljs-number">2</span>)</span><br><span class="line">plot(pe)</span><br></pre></td></tr></table></figure>

<p><img src="/picture/output_10_0.png" alt="png"></p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#取交叉验证中使测试均方误差最小的lambda</span></span><br><span class="line">which.min(pe)  <span class="hljs-comment">#30</span></span><br><span class="line">lam=LAM[which.min(pe)]  <span class="hljs-comment">#5.918776</span></span><br></pre></td></tr></table></figure>

<p>30</p>
<p>5.91877551020408</p>
<h2 id="改进删一法交叉验证"><a href="#改进删一法交叉验证" class="headerlink" title="改进删一法交叉验证"></a>改进删一法交叉验证</h2><p>设
$$<br>X = \left [<br>\begin{matrix}<br>x_1 \<br>x_2 \<br>\cdots \<br>x_n<br>\end{matrix}<br>\right ]<br>$$</p>
<p>则，<br>$$<br>X’ = [x_1’,x_2’,…,x_n’]<br>$$</p>
<p>那么，<br>$$<br>X’X = x_1’x_1+x_2’x_2+\cdots+x_n’x_n<br>$$</p>
<p>删一法每次删除一个样本，又上述$x_i$对应于一个样本$i$，因此，删除一个样本后的$X’X$变成了$X’X-x_i’x_i$。</p>
<p>同理，<br>$$<br>y’X = y’<br>\left [<br>\begin{matrix}<br>x_1 \<br>x_2 \<br>\cdots \<br>x_n<br>\end{matrix}<br>\right ]<br>=y_1x_1+y_2x_2+\cdots+y_nx_n<br>$$</p>
<p>那么，删除一个样本$i$，则$y’X$变成了$y’X-y_ix_i$</p>
<p>根据上述思路，可以将删一法交叉验证算法进行改进，避免了每次都要计算矩阵相乘，大大降低了计算压力。</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">## 改进的删一法</span></span><br><span class="line"><span class="hljs-comment">##mridge_aug函数用于实现删去一个样品的岭回归</span></span><br><span class="line">mridge_aug=<span class="hljs-keyword">function</span>(i,lambda,x,y,xx,xy) </span><br><span class="line">&#123;</span><br><span class="line">  xx =xx-x[i,]%*%t(x[i,])</span><br><span class="line">  xy =xy-y[i]*x[i,]</span><br><span class="line">  xvar=apply(x,<span class="hljs-number">2</span>,var)  <span class="hljs-comment">#各变量方差</span></span><br><span class="line">  <span class="hljs-comment">#利用cholesky分解求取回归方程的参数beta的估计值  </span></span><br><span class="line">  V &lt;- xx + diag(lambda*xvar)  </span><br><span class="line">  <span class="hljs-comment">#t(x)%*%x+lambda*xvar作为线性方程组的系数矩阵V</span></span><br><span class="line">  R &lt;- mchol(V)   </span><br><span class="line">  <span class="hljs-comment">#调用mchol函数将系数矩阵V进行cholesky分解,V=R%*%t(R)</span></span><br><span class="line">  M &lt;- mforwardsolve(R, xy) </span><br><span class="line">  <span class="hljs-comment">#使用前代法求解R%*%M=t(x)%*%y,其中M=t(R)%*%beta</span></span><br><span class="line">  beta=mbacksolve(t(R), M)   </span><br><span class="line">  <span class="hljs-comment">#使用回代法求解t(R)%*%beta=M,即可得beta的估计值</span></span><br><span class="line">  <span class="hljs-keyword">return</span> (beta)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#验证上述函数功能</span></span><br><span class="line">x &lt;- as.matrix(prostate[ ,<span class="hljs-number">1</span>:<span class="hljs-number">8</span>])</span><br><span class="line">x &lt;- as.matrix(cbind(<span class="hljs-number">1</span>,x))</span><br><span class="line">y &lt;- as.vector(prostate[ ,<span class="hljs-number">9</span>])</span><br><span class="line">xx=t(x)%*%x</span><br><span class="line">xy=t(y)%*%x</span><br><span class="line">mridge_aug(<span class="hljs-number">8</span>,<span class="hljs-number">0</span>,x,y,xx,xy)</span><br></pre></td></tr></table></figure>

<ol class="list-inline">
    <li>0.491888211499562</li>
    <li>0.570089057625066</li>
    <li>0.601708515193068</li>
    <li>-0.0231551000547407</li>
    <li>0.112972308293185</li>
    <li>0.773102592770376</li>
    <li>-0.11433738535188</li>
    <li>0.031977556499466</li>
    <li>0.0045456886523957</li>
</ol>




<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#改进的删一法交叉验证,计算测试均方误差</span></span><br><span class="line">cvridgeregerr_aug &lt;- <span class="hljs-keyword">function</span>(lambda,x,y)</span><br><span class="line">&#123;</span><br><span class="line">  np&lt;-dim(x)</span><br><span class="line">  n&lt;-np[<span class="hljs-number">1</span>]</span><br><span class="line">  <span class="hljs-comment">#列表中的元素作为第一个参数输入mridge，示去掉的数据编号，</span></span><br><span class="line">  <span class="hljs-comment">#结果第i行为删去第i个样本的岭回归系数估计值</span></span><br><span class="line">  coe&lt;-t(apply(as.matrix(<span class="hljs-number">1</span>:n,ncol=<span class="hljs-number">1</span>),<span class="hljs-number">1</span>,mridge_aug,lambda,x,y,xx,xy))</span><br><span class="line">  <span class="hljs-comment">#coe第i行和数据矩阵第i个样本做点对点相乘，对行求和，</span></span><br><span class="line">  <span class="hljs-comment">#计算测试均方误差</span></span><br><span class="line">  mean((apply(coe*x,<span class="hljs-number">1</span>,sum)-y)^<span class="hljs-number">2</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#验证上述函数功能</span></span><br><span class="line">x &lt;- as.matrix(prostate[ ,<span class="hljs-number">1</span>:<span class="hljs-number">8</span>])</span><br><span class="line">x &lt;- mystandard(x)</span><br><span class="line">y &lt;- as.vector(prostate[ ,<span class="hljs-number">9</span>])</span><br><span class="line">y &lt;- (y-mean(y)) / sd(y)</span><br><span class="line">xx=t(x)%*%x</span><br><span class="line">xy=t(y)%*%x</span><br><span class="line">cvridgeregerr_aug(<span class="hljs-number">0.3</span>,x,y)</span><br></pre></td></tr></table></figure>

<p>0.396699322945445</p>
<h2 id="利用标准化的X和Y训练模型"><a href="#利用标准化的X和Y训练模型" class="headerlink" title="利用标准化的X和Y训练模型"></a>利用标准化的X和Y训练模型</h2><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">LAM = seq(<span class="hljs-number">0.001</span>,<span class="hljs-number">10</span>,length.out = <span class="hljs-number">50</span>)</span><br><span class="line">x &lt;- as.matrix(prostate[ ,<span class="hljs-number">1</span>:<span class="hljs-number">8</span>])</span><br><span class="line">x &lt;- mystandard(x) <span class="hljs-comment">#y也进行标准化，因此X不用增广为(1,X)</span></span><br><span class="line">y &lt;- as.vector(prostate[ ,<span class="hljs-number">9</span>])</span><br><span class="line">y &lt;- (y-mean(y)) / sd(y) <span class="hljs-comment">#对y进行标准化</span></span><br><span class="line">xx=t(x)%*%x</span><br><span class="line">xy=t(y)%*%x</span><br><span class="line"><span class="hljs-comment">#计算岭回归50个模型的测试均方误差，将结果从list展开成向量</span></span><br><span class="line">pe1 &lt;- unlist(lapply(LAM, cvridgeregerr_aug, x, y))</span><br></pre></td></tr></table></figure>

<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">plot(pe1) <span class="hljs-comment">#</span></span><br><span class="line">lam1 =  LAM[which.min(pe1)]</span><br></pre></td></tr></table></figure>

<p><img src="/picture/output_26_0.png" alt="png"></p>
<h2 id="利用未标准化的X和Y训练模型"><a href="#利用未标准化的X和Y训练模型" class="headerlink" title="利用未标准化的X和Y训练模型"></a>利用未标准化的X和Y训练模型</h2><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x &lt;- as.matrix(prostate[ ,<span class="hljs-number">1</span>:<span class="hljs-number">8</span>])</span><br><span class="line">x &lt;- cbind(<span class="hljs-number">1</span>,x) <span class="hljs-comment">#X为未标准化</span></span><br><span class="line">y &lt;- as.vector(prostate[ ,<span class="hljs-number">9</span>]) <span class="hljs-comment">#y未标准化</span></span><br><span class="line">xx=t(x)%*%x</span><br><span class="line">xy=t(y)%*%x</span><br><span class="line"><span class="hljs-comment">#计算岭回归50个模型的测试均方误差，将结果从list展开成向量</span></span><br><span class="line">pe2 &lt;- unlist(lapply(LAM, cvridgeregerr_aug, x, y))</span><br></pre></td></tr></table></figure>

<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot(pe2) <span class="hljs-comment">#</span></span><br><span class="line">lam2 =  LAM[which.min(pe2)]</span><br></pre></td></tr></table></figure>

<p><img src="/picture/output_29_0.png" alt="png"></p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lam1;lam2</span><br></pre></td></tr></table></figure>

<p>5.91877551020408</p>
<p>5.91877551020408</p>
<p><strong>可以看到，无论是用标准化的X和y还是未标准化的X和y来进行训练，最佳的lambda结果是一样的。</strong></p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#用本文自定义的函数计算回归系数</span></span><br><span class="line">x &lt;- as.matrix(prostate[ ,<span class="hljs-number">1</span>:<span class="hljs-number">8</span>])</span><br><span class="line">y &lt;- as.vector(prostate[ ,<span class="hljs-number">9</span>]) <span class="hljs-comment">#y未标准化</span></span><br><span class="line">ridgereg(lambda = lam1,x,y)</span><br></pre></td></tr></table></figure>

<ol class="list-inline">
    <li>0.857190872862373</li>
    <li>0.54969271971446</li>
    <li>0.450878931201676</li>
    <li>-0.0172129105085714</li>
    <li>0.103056884295319</li>
    <li>0.466532216724395</li>
    <li>-0.0285916273005211</li>
    <li>0.0157990230188132</li>
    <li>0.00488299779401389</li>
</ol>




<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#用mda内置函数实现岭回归(输出结果中缺少截距项)</span></span><br><span class="line"><span class="hljs-keyword">library</span>(mda)</span><br><span class="line">ridge1 &lt;- gen.ridge(prostate[ ,<span class="hljs-number">1</span>:<span class="hljs-number">8</span>], prostate[ ,<span class="hljs-number">9</span>], </span><br><span class="line">                    drop &lt;- <span class="hljs-literal">FALSE</span>, lambda =lam)  </span><br><span class="line">ridge1$coe</span><br></pre></td></tr></table></figure>

<pre><code>Loading required package: class
Loaded mda 0.4-10</code></pre><table>
<tbody>
    <tr><td> 0.549692720</td></tr>
    <tr><td> 0.450878931</td></tr>
    <tr><td>-0.017212911</td></tr>
    <tr><td> 0.103056884</td></tr>
    <tr><td> 0.466532217</td></tr>
    <tr><td>-0.028591627</td></tr>
    <tr><td> 0.015799023</td></tr>
    <tr><td> 0.004882998</td></tr>
</tbody>
</table>




<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">min(pe1);min(pe2)  <span class="hljs-comment">#用标准化的X和Y来进行训练，可将预测误差降低到0.39</span></span><br></pre></td></tr></table></figure>

<p>0.393552554589219</p>
<p>0.536324873140668</p>
<p>对于标准化的岭回归模型，有：<br>$$\frac{y-\bar{y}}{sd_y}=\sum \frac{x_i-\bar{x_i}}{ {sd_x}_i} \beta_i<br>$$</p>
<p>即：<br>$$y = \bar{y} + sd_y \sum \frac{x_i-\bar{x_i}}{ {sd_x}_i} \beta_i = \sum x_i \frac{\beta_i sd_y}{ {sd_x}_i} + \bar{y} - \sum \frac{\bar{x_i}\beta_i sd_y}{ {sd_x}_i}$$</p>
<p>令 
$$\beta’_1 = \frac{\beta_i sd_y}{ {sd_x}_i}, \beta’_0 = \bar{y} - \sum \frac{\bar{x_i}\beta_i sd_y}{ {sd_x}_i}$$</p>
<p>当遇到一个新样本时，可以利用上面变换得到的回归系数进行预测。</p>
<h2 id="逐步回归结果"><a href="#逐步回归结果" class="headerlink" title="逐步回归结果"></a>逐步回归结果</h2><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data &lt;- prostate[,<span class="hljs-number">1</span>:<span class="hljs-number">9</span>]</span><br><span class="line">lm.data &lt;- lm(lpsa~.,data = data)</span><br><span class="line">lm.step &lt;- step(lm.data)</span><br></pre></td></tr></table></figure>

<pre><code>Start:  AIC=-60.78
lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + 
    pgg45

          Df Sum of Sq    RSS     AIC
- gleason  1    0.0491 43.108 -62.668
- pgg45    1    0.5102 43.569 -61.636
- lcp      1    0.6814 43.740 -61.256
&lt;none&gt;                 43.058 -60.779
- lbph     1    1.3646 44.423 -59.753
- age      1    1.7981 44.857 -58.810
- lweight  1    4.6907 47.749 -52.749
- svi      1    4.8803 47.939 -52.364
- lcavol   1   20.1994 63.258 -25.467

Step:  AIC=-62.67
lpsa ~ lcavol + lweight + age + lbph + svi + lcp + pgg45

          Df Sum of Sq    RSS     AIC
- lcp      1    0.6684 43.776 -63.176
&lt;none&gt;                 43.108 -62.668
- pgg45    1    1.1987 44.306 -62.008
- lbph     1    1.3844 44.492 -61.602
- age      1    1.7579 44.865 -60.791
- lweight  1    4.6429 47.751 -54.746
- svi      1    4.8333 47.941 -54.360
- lcavol   1   21.3191 64.427 -25.691

Step:  AIC=-63.18
lpsa ~ lcavol + lweight + age + lbph + svi + pgg45

          Df Sum of Sq    RSS     AIC
- pgg45    1    0.6607 44.437 -63.723
&lt;none&gt;                 43.776 -63.176
- lbph     1    1.3329 45.109 -62.266
- age      1    1.4878 45.264 -61.934
- svi      1    4.1766 47.953 -56.336
- lweight  1    4.6553 48.431 -55.373
- lcavol   1   22.7555 66.531 -24.572

Step:  AIC=-63.72
lpsa ~ lcavol + lweight + age + lbph + svi

          Df Sum of Sq    RSS     AIC
&lt;none&gt;                 44.437 -63.723
- age      1    1.1588 45.595 -63.226
- lbph     1    1.5087 45.945 -62.484
- lweight  1    4.3140 48.751 -56.735
- svi      1    5.8509 50.288 -53.724
- lcavol   1   25.9427 70.379 -21.119</code></pre><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lm.step$coefficients</span><br></pre></td></tr></table></figure>

<dl class="dl-horizontal">
    <dt>(Intercept)</dt>
        <dd>0.494729262182627</dd>
    <dt>lcavol</dt>
        <dd>0.543997856944351</dd>
    <dt>lweight</dt>
        <dd>0.58821270309519</dd>
    <dt>age</dt>
        <dd>-0.016444846497545</dd>
    <dt>lbph</dt>
        <dd>0.101223333723462</dd>
    <dt>svi</dt>
        <dd>0.714903976347167</dd>
</dl>



<p>根据AIC信息准则，最终的模型为“lpsa ~ lcavol + lweight + age + lbph + svi”，系数如上述结果所示。</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lm.err = mean((apply(t(t(cbind(<span class="hljs-number">1</span>,data[,c(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>)]))</span><br><span class="line">              *lm.step$coefficients),<span class="hljs-number">1</span>,sum)-data[,<span class="hljs-number">9</span>])^<span class="hljs-number">2</span>)</span><br><span class="line">lm.err <span class="hljs-comment">#逐步回归的预测误差</span></span><br></pre></td></tr></table></figure>

<p>0.458110121687733</p>
<p>可以看到，逐步回归的预测误差为0.458，略高于岭回归。</p>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-image">
        <a href="/2018/04/13/神经网络/" class="image is-7by1">
            <img class="thumbnail" src="/thumbnails/net.jpg" alt="神经网络和深度学习">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2018-04-12T16:00:00.000Z">2018-04-13</time>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    几秒 读完 (大约 29 个字)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2018/04/13/神经网络/">神经网络和深度学习</a>
            
        </h1>
        <div class="content">
            <h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><h2 id="前向神经网络"><a href="#前向神经网络" class="headerlink" title="前向神经网络"></a>前向神经网络</h2><h3 id="S型神经元"><a href="#S型神经元" class="headerlink" title="S型神经元"></a>S型神经元</h3><h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><h3 id="反向神经网络"><a href="#反向神经网络" class="headerlink" title="反向神经网络"></a>反向神经网络</h3><h2 id="编程实现"><a href="#编程实现" class="headerlink" title="编程实现"></a>编程实现</h2>
        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-image">
        <a href="/2018/04/04/集成学习/" class="image is-7by1">
            <img class="thumbnail" src="/thumbnails/ensemble.jpg" alt="集成学习">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2018-04-03T17:15:49.176Z">2018-04-04</time>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    31 分钟 读完 (大约 4590 个字)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2018/04/04/集成学习/">集成学习</a>
            
        </h1>
        <div class="content">
            <h2 id="个体与集成"><a href="#个体与集成" class="headerlink" title="个体与集成"></a>个体与集成</h2><blockquote>
<p>Hoeffding不等式：给定m个取值在[0,1]区间的独立随机变量$x_1,x_2,\cdots,x_n$，对任意$\epsilon &gt; 0$有如下等式成立：</p>
<p>$$P(|\frac{1}{m}\sum _{1=1}^mx_i-\frac{1}{m}E(x_i)|\ge \epsilon) \le 2e^{-2m\epsilon^2}$$</p>
</blockquote>
<h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p>提升（boosting）方法是一种常用的机器学习方法，应用广泛且有效。在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。</p>
<h3 id="提升方法的基本思路"><a href="#提升方法的基本思路" class="headerlink" title="提升方法的基本思路"></a>提升方法的基本思路</h3><p>提升方法基于这样一种思想：对于一个复杂的任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好。实际上，就是“三个臭皮匠赛过诸葛亮”的道理。</p>
<p>历史上，Kearns和Valiant首先提出了“强可学习（strong learnable）”和“弱可学习（weakly learnable）”的概念。指出：在概率近似正确学习的框架中，一个概念（一个类），如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的；一个概念，如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。非常有趣的是Schapire后来证明强可学习与弱可学习是等价的，也就是说，在PAC学习的框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。</p>
<p>这样一来，问题便成为，在学习中，如果已经发现了“弱学习算法”，那么能否将它提升（boost）为“强学习算法”。大家知道，发现弱学习算法比发现强学习算法容易得多。那么如何具体实施提升，便成为开发提升方法时所要解决的问题。关于提升方法的研究很多，有很多算法被提出。最具有代表性的是AdaBoost算法。</p>
<p>对于分类问题而言，给定一个训练样本集，求比较粗糙的分类规则（弱分类器）要比求精确的分类规则（强分类器）容易的多。提升方法就是从弱学习算法出发，反复学习，得到一系列弱分类器，然后组合这些弱分类器，构成一个强分类器。大多数的提升方法都是改变训练数据的概率分布（训练数据的权值分布），针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。</p>
<p>这样，对提升方法来说，有两个问题需要回答：一是在每一轮如何改变数据的权值或概率分布；二是如何将弱分类器组合为一个强分类器。关于第1个问题，AdaBoost的做法是，提高那些被前一轮弱分类器分错的样本权值，而降低那些被正确分类样本的权值。这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注。于是，分类问题被一系列弱分类器“分而治之”。至于第2个问题，即弱分类器的组合，AdaBoost采取加权多数表决的方法。具体地，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。</p>
<p>AdaBoost的巧妙之处在于它将这些想法自然且有效地实现在一种算法里。</p>
<h3 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h3><p>现在叙述AdaBoost算法。假设给定一个二分类的训练数据集</p>
<p>$$T={(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)}$$</p>
<p>其中，每个样本点由实例与标记组成。实例$x_i\in R^n$,标记$y_i\in {-1,1}$。AdaBoost利用以下算法，从训练数据中学习一系列弱分类器或基本分类器，并将这些弱分类器线性组合为一个强分类器。</p>
<p>输入：训练数据集$T={(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)}$,其中$x_i\in R^n$,标记$y_i\in {-1,1}$；弱分类算法。</p>
<p>输出：最终分类器$G(x)$。</p>
<ol>
<li>初始化训练数据的权值分布，</li>
</ol>
<p>$$D_1=(w_{11},\cdots,w_{1i},\cdots,w_{1N}),w_{1i}=\frac{1}{N},i=1,2,\cdots,N$$</p>
<ol start="2">
<li>对$m=1,2,\cdots,M$  </li>
</ol>
<ul>
<li>使用具有权值分布$D_m$的训练数据集学习，得到基本分类器</li>
</ul>
<blockquote>
<p>$$G_m(x):R^n \rightarrow {-1,1}$$</p>
</blockquote>
<ul>
<li>计算$G_m(x)$在训练数据上的分类误差率</li>
</ul>
<blockquote>
<p>$$e_m=\sum <em>{i=1}^NP(G_m(x)\ne y_i)=\sum _{i=1}^Nw</em>{mi}I(G_m(x)\ne y_i) $$</p>
</blockquote>
<ul>
<li>计算$G_m(x)$的系数</li>
</ul>
<blockquote>
<p>$$\alpha _m = \frac{1}{2} ln \frac{1-e_m}{e_m}$$ </p>
</blockquote>
<ul>
<li>更新训练数据集的权值分布</li>
</ul>
<blockquote>
<p>$$D_{m+1} = (w_{m+1,1},\cdots,w_{m+1,i},\cdots,w_{m+1,N})$$ </p>
</blockquote>
<blockquote>
<p>$$w_{m+1,i} = \frac{w_{mi}}{Z_m}e^{-\alpha _m y_i G_m(x_i)},i=1,2,\cdots,N$$ </p>
</blockquote>
<p>这里，$Z_m$是规范化因子</p>
<p>$$Z_m = \sum _{i=1}^N e^{-\alpha _m y_i G_m(x_i)}$$ </p>
<p>它使$D_{m+1}$成为一个概率分布。</p>
<ol start="3">
<li>构建基本分类器的线性组合</li>
</ol>
<p>$$f(x) = \sum _{m=1}^M \alpha_m G_m(x)$$ </p>
<p>得到最终的分类器</p>
<p>$$G(X) = sign(f(x)) = sign(\sum _{m=1}^M \alpha_m G_m(x))$$ </p>
<p>对AdaBoost算法作如下说明：</p>
<p>步骤1 假设数据集具有均匀的权值分布，即每个训练样本在基本分类器中作用相同，这一假设保证第1步能够在原始数据集上学习基本分类器$G_1(x)$。</p>
<p>步骤2 AdaBoost反复学习基本分类器，在每一轮$m=1,2,\cdots,M$中顺次地执行下列操作：</p>
<p>（a）使用当前分布$D_m$加权的训练数据集，学习基本分类器$G_m(x)$。</p>
<p>（b）计算基本分类器$G_m(x)$在加权训练数据集上的分类误差率：</p>
<p>$$e_m = \sum <em>{i=1}^N P(G_m(x_i)\ne y_i) = \sum _{G_m(x_i)\ne y_i} w</em>{mi}$$  </p>
<p>这里，$w_{mi}$表示第$m$轮中第$i$个实例的权值，$\sum <em>{i=1}^N w</em>{mi}=1$。这表明，$G_m(x)$在加权的训练数据上的分类误差率是被$G_m(x)$误分类样本的权值之和，由此可以看出数据权值分布$D_m$与基本分类器$G_m(x)$的分类误差率的关系。</p>
<p>（c）计算基本分类器$G_m(x)$的系数$\alpha _m$，$\alpha _m$表示$G_m(x)$在最终分类器中的重要性。由(2)可知，当$e_m\le \frac{1}{2}$时，$\alpha_m \ge 0$，并且$\alpha_m$随着$e_m$的减小而增大，所以分类误差率越小的基本分类器在最终分类器中的所用越大。</p>
<p>（d）更新训练数据的权值分布为下一轮作准备，式(4)可以写成：</p>
<p>$$ w_{m+1,i}=\left{\begin{aligned}\frac{w_{mi}}{Z_m}e^{-\alpha_m} &amp;  &amp; G_m(x_i)=y_i \\frac{w_{mi}}{Z_m}e^{\alpha_m} &amp;  &amp; G_m(x_i) \ne y_i \end{aligned}\right.$$</p>
<p>由此可知，被基本分类器$G_m(x)$误分类样本的权值得以扩大，而被正确分类样本的权值却得以缩小。两相比较，由式(2)知误分类样本的权值被放大$e^{2\alpha_m}=\frac{1-e_m}{e_m}$倍。因此，误分类样本在下一轮学习中起更大的作用。不改变所给的训练数据，而不断改变训练数据的权值分布，使得训练数据在基本分类器的学习中起到不同的作用，这是AdaBoost的一个特点。</p>
<p>步骤3 线性组合$f(x)$实现$M$个基本分类器的加权表决。系数$\alpha_m$表示了基本分类器$G_m(x)$的重要性，这里，所有$\alpha_m$之和并不为1.$f(x)$的符号决定实例$x$的分类，$f(x)$的绝对值表示分类的确信度。利用基本分类器的线性组合构建最终分类器是AdaBoost的另一个特点。</p>
<h3 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h3><h3 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h3><p>XGBoost是”极端梯度上升”(Extreme Gradient Boosting)的简称,从技术上说，XGBoost是Extreme Gradient Boosting的缩写。它的流行源于在著名的Kaggle数据科学竞赛上被称为”奥托分类”的挑战。它可以处理多种目标函数，包括回归，分类和排序，是一个较为全面的分类器。<br>由于其他许多分类器，不管是强分类器或是集成分类器，在预测性能上的强大但是相对缓慢的实现，如上一章的Adaboost集成算法，不管是在在运行时间上还是在内存占有上开销都很大。XGBoost成为很多比赛的理想选择。XGBoost包还添加了做交叉验证和发现关键变量的额外功能。在优化模型时，这个算法使用了好几个参数。所以为了提高模型的表现，参数的调整十分必要。本节将讨论这些因素。</p>
<h4 id="XGBoost优势"><a href="#XGBoost优势" class="headerlink" title="XGBoost优势"></a>XGBoost优势</h4><p>XGBoost算法总结起来大致其有三个优点：高效、准确度、模型的交互性。</p>
<ul>
<li>正则化：标准GBDT提升树算法的实现没有像XGBoost这样的正则化步骤。正则化用于控制模型的复杂度，对减少过拟合也是有帮助的。XGBoost也正是以“正则化提升”技术而闻名。</li>
<li>并行处理：XGBoost可以实现并行处理，相比GBM有了速度的飞跃。不过，需要注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。XGBoost的并行是在特征粒度上的。决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点）。因此XGBoost在R重定义了一个自己数据矩阵类DMatrix。XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复利用索引地使用这个结构，获得每个节点的梯度，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</li>
<li>高度灵活性：XGBoost允许用户定义自定义优化目标和评价标准，它对模型增加了一个全新的维度，所以我们的处理不会受到任何限制。</li>
<li>缺失值处理：XGBoost内置处理缺失值的规则。 用户需要提供一个和其它样本不同的值，然后把它作为一个参数传进去，以此来作为缺失值的取值。XGBoost在不同节点遇到缺失值时采用不同的处理方法，并且会学习未来遇到缺失值时的处理方法。</li>
<li>剪枝：当分裂时遇到一个负损失时，传统GBDT会停止分裂。因此传统GBDT实际上是一个贪心算法。XGBoost会一直分裂到指定的最大深度(max_depth)，然后回过头来剪枝。如果某个节点之后不再有正值，它会去除这个分裂。这种做法的优点，当一个负损失（如-2）后面有个正损失（如+10）的时候，就显现出来了。GBM会在-2处停下来，因为它遇到了一个负值。但是XGBoost会继续分裂，然后发现这两个分裂综合起来会得到+8，因此会保留这两个分裂。</li>
<li>内置交叉验证：XGBoost允许在每一轮boosting迭代中使用交叉验证。因此，可以方便地获得最优boosting迭代次数。而传统的GBDT使用网格搜索，只能检测有限个值。</li>
</ul>
<h4 id="XGBoost算法推导"><a href="#XGBoost算法推导" class="headerlink" title="XGBoost算法推导"></a>XGBoost算法推导</h4><p>XGBoost 在函数空间中用牛顿法进行优化。首先，boosting是一种加法模型。XGBoost同样属于GBDT梯度提升法，模型的基分类器都包含有树，对于给定的数据集D={($x_i​$,$y_i​$)}，XGBoost进行additive learning，学习K棵树，采用以下函数对样本进行预测。</p>
<p>$$\widehat{y}=\phi(x_i)=\sum_{k=1}^{K}f_{k}(x_i)\qquad f_k\in F$$</p>
<p>这里F是函数空间，$f(x)$是回归树CART。</p>
<p>$$F={f(x)=w_{q(x)}}(q:R^m\to T,w\in R)$$</p>
<p>$q(x)$标识将样本x分到了某个叶子节点上，$w$是叶子节点的分数，（leaf score），所以$w_q(x)$ 表示回归树对样本的预测值。<br>回归树的预测输出是实数分数，可以用于回归，分类，排序等任务中，对于回归问题，可以直接作为目标值，对于分类问题，需要映射成概率，比如采用逻辑函数，然后可以控制阈值，进行两种分类错误的把控。<br>XGBoost对传统的提升树算法Adaboost等的改进，在于在参数空间的目标函数中加入了正则化项，来惩罚模型的复杂程度，进而控制过拟合。和Adaboost一样都是通过最小化损失函数求解最优模型，并加入了阈值，如下公式所示：</p>
<p>$$L(\phi)=\sum_{i}l(\widehat{y}<em>l,y_i)+\sum</em>{k}\Omega(f_k)$$</p>
<p>误差函数可以是square loss，logloss等，也可以自己定义损失函数，只要能够求出目标函数的梯度和Hessian矩阵，用户就可以自定义训练模型时的目标函数 这也正是XGBoost的优势之一，可以通过研究目的的不同自己定义损失函数，<br>在公式（3-3）中，相比于原始的GBDT，XGBoost的目标函数多了正则项，是学习出来的模型更加不容易过拟合。衡量树的复杂程度主要与树的深度，内部节点的个数，叶子节点的个数，叶子节点的权重有关，因此XGBoost对这些参量进行了约束。得出了正则项为：</p>
<p>$$\Omega(f)=\gamma T+\frac{1}{2}\rho|w|^2$$</p>
<p>正则项对每棵树的复杂程度都应进行惩罚，对每个节点进行了复杂度的惩罚，从另一种角度来说也就进行了自动的剪枝。<br>另外，还可以选择使用线性模型替代树模型，从而得到带$L1+L2$惩罚的线性回归正则项可以是$L1$正则，$L2$正则 。<br>第$t$次迭代后，模型的预测等于前$t-1$次的模型预测加上第t棵树的预测。将目标函数在前$t-1$次的模型$y_i^{t-1}$处进行泰勒展开，并将常数项去掉即得到：</p>
<p>$$L^{(t)}=\sum_{i=1}^{n}[g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]+\Omega(f_t)$$</p>
<p>公式中，$g_i=\delta_{\widehat{y}^{(t-1)}}l(y_i,\widehat{y}^{(t-1)})\qquad h_i=\delta^2_{\widehat{y}^{(t-1)}}l(y_i,\widehat{y}^{(t-1)})$<br>把$f_t$, $\Omega(f_t)$写成树结构的形式，得到：</p>
<p>$$L^{(t)}=\sum_{i=1}^{n}[g_iw_{q(x_i)}+\frac{1}{2}h_iw^2_{q(x_i)}]+\gamma T+\rho\frac{1}{2}\sum_{j=i}^{T}w_j^2$$</p>
<p>则目标函数可以写成按叶节点累加的形式：</p>
<p>$$L^{(t)}=\sum_{j=1}^{T}[G_iw_j+\frac{1}{2}(H_j+\rho)w_j^2]+\gamma T$$</p>
<p>如果确定了树的结构（即$q(x)$确定），为了使目标函数最小，可以令其导数为0，解得每个叶节点的最优预测分数为：</p>
<p>$$W_j^*=-\frac{G_j}{H_j+\rho}$$</p>
<p>$$\widehat{L}^*=-\frac{1}{2}\sum_{j=1}^{T}\frac{G_j^2}{H_j+\rho}+\gamma T$$</p>
<p>公式（3-9）的负部衡量了每个叶子节点对总体损失的的贡献，我们希望损失越小越好，则公式的（3-9）的负部值越大越好。<br>因此，对一个叶子节点进行分裂，分裂前后的增益定义为：</p>
<p>$$Gain=\frac{G_L^2}{H_L+\rho}+\frac{G_R^2}{H_R+\rho}-\frac{(G_L+G_R)^2}{H_L+H_R+\rho}-\gamma$$</p>
<p>Gain的值越大，分裂后L减小越多。所以当对一个叶节点分割时，计算所有候选特征值所对应的gain，选取gain最大的进行分割。但由于精确遍历所有可能的分割点是效率很低的，所以，实际上XGBoost采用的是对于每个特征，不是简单地按照样本个数进行分位，而是以二阶导数值作为权重，进行分位点的选择，以此减少计算复杂度。在学习每棵树前，提出候选切分点，这也是XGBoost可以实现并行的原因之一，可以提前切割分位点。<br> 最后，XGBoost算法还借鉴了bagging的bootstrap自助法行抽样，还借鉴了随机森林的列抽样，即特征抽样。这样减少过拟合同时还降低了计算复杂度。</p>
<h2 id="Bagging-amp-随机森林"><a href="#Bagging-amp-随机森林" class="headerlink" title="Bagging &amp; 随机森林"></a>Bagging &amp; 随机森林</h2>
        </div>
        
        
        
    </div>
</div>









    
<div class="card card-transparent">
    <nav class="pagination is-centered" role="navigation" aria-label="pagination">
        <div class="pagination-previous is-invisible is-hidden-mobile">
            <a class="is-flex-grow has-text-black-ter" href="/page/0/">上一页</a>
        </div>
        <div class="pagination-next">
            <a class="is-flex-grow has-text-black-ter" href="/page/2/">下一页</a>
        </div>
        <ul class="pagination-list is-hidden-mobile">
            
            <li><a class="pagination-link is-current" href="/">1</a></li>
            
            <li><a class="pagination-link has-text-black-ter" href="/page/2/">2</a></li>
            
            <li><a class="pagination-link has-text-black-ter" href="/page/3/">3</a></li>
            
        </ul>
    </nav>
</div>
</div>
                




<div class="column is-4-tablet is-4-desktop is-3-widescreen  has-order-1 column-left ">
    
        
<div class="card widget">
    <div class="card-content">
        <nav class="level">
            <div class="level-item has-text-centered">
                <div>
                    
                        <img class="image is-128x128 has-mb-6" src="/images/touxiang.jpg" alt="Feng Yangyang">
                    
                    
                    <p class="is-size-4 is-block">
                        Feng Yangyang
                    </p>
                    
                    
                    <p class="is-size-6 is-block">
                        Data Analyst
                    </p>
                    
                    
                    <p class="is-size-6 is-flex is-flex-center has-text-grey">
                        <i class="fas fa-map-marker-alt has-mr-7"></i>
                        <span>PuDong,ShangHai</span>
                    </p>
                    
                </div>
            </div>
        </nav>
        <nav class="level is-mobile">
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        文章
                    </p>
                    <p class="title has-text-weight-normal">
                        23
                    </p>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        分类
                    </p>
                    <p class="title has-text-weight-normal">
                        2
                    </p>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        标签
                    </p>
                    <p class="title has-text-weight-normal">
                        22
                    </p>
                </div>
            </div>
        </nav>
        <div class="level">
            <a class="level-item button is-link is-rounded" href="/" target="_blank">
                关注我</a>
        </div>
        
        
    </div>
</div>
    
        
    
        

<div class="card widget">
    <div class="card-content">
        <div class="menu">
        <h3 class="menu-label">
            链接
        </h3>
        <ul class="menu-list">
        
            <li>
                <a class="level is-mobile" href="http://pcgan.site/" target="_blank">
                    <span class="level-left">
                        <span class="level-item">甘医生</span>
                    </span>
                    <span class="level-right">
                        <span class="level-item tag">pcgan.site</span>
                    </span>
                </a>
            </li>
        
            <li>
                <a class="level is-mobile" href="http://yinhongyu.com" target="_blank">
                    <span class="level-left">
                        <span class="level-item">尹宏宇</span>
                    </span>
                    <span class="level-right">
                        <span class="level-item tag">yinhongyu.com</span>
                    </span>
                </a>
            </li>
        
        </ul>
        </div>
    </div>
</div>


    
        
<div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                分类
            </h3>
            <ul class="menu-list">
            <li>
        <a class="level is-marginless" href="/categories/NLP/">
            <span class="level-start">
                <span class="level-item">NLP</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/大数据/">
            <span class="level-start">
                <span class="level-item">大数据</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li>
            </ul>
        </div>
    </div>
</div>
    
        
<div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            标签云
        </h3>
        <a href="/tags/ETL/" style="font-size: 10px;">ETL</a> <a href="/tags/Lubridate/" style="font-size: 10px;">Lubridate</a> <a href="/tags/MapReduce/" style="font-size: 10px;">MapReduce</a> <a href="/tags/R/" style="font-size: 10px;">R</a> <a href="/tags/clustering/" style="font-size: 10px;">clustering</a> <a href="/tags/kmeans/" style="font-size: 10px;">kmeans</a> <a href="/tags/优化算法/" style="font-size: 10px;">优化算法</a> <a href="/tags/岭回归/" style="font-size: 10px;">岭回归</a> <a href="/tags/感知机/" style="font-size: 10px;">感知机</a> <a href="/tags/排序/" style="font-size: 10px;">排序</a> <a href="/tags/数据仓库/" style="font-size: 10px;">数据仓库</a> <a href="/tags/数据结构/" style="font-size: 10px;">数据结构</a> <a href="/tags/机器学习/" style="font-size: 20px;">机器学习</a> <a href="/tags/梯度下降/" style="font-size: 10px;">梯度下降</a> <a href="/tags/深度学习/" style="font-size: 10px;">深度学习</a> <a href="/tags/爬虫/" style="font-size: 16.67px;">爬虫</a> <a href="/tags/矩阵分解/" style="font-size: 13.33px;">矩阵分解</a> <a href="/tags/神经网络/" style="font-size: 10px;">神经网络</a> <a href="/tags/线性回归/" style="font-size: 10px;">线性回归</a> <a href="/tags/自然语言处理/" style="font-size: 10px;">自然语言处理</a> <a href="/tags/逐步回归/" style="font-size: 10px;">逐步回归</a> <a href="/tags/降维/" style="font-size: 10px;">降维</a>
    </div>
</div>

    
    
        <div class="column-right-shadow is-hidden-widescreen ">
        
            
<div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            最新文章
        </h3>
        
        <article class="media">
            
            <a href="/2018/07/02/R语言简明教程/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/thumbnails/Rlogo.png" alt="R语言简明教程">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-07-02T03:43:55.696Z">2018-07-02</time></div>
                    <a href="/2018/07/02/R语言简明教程/" class="has-link-black-ter is-size-6">R语言简明教程</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2018/05/09/排序算法/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/thumbnails/sort.jpg" alt="排序算法">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-05-08T16:00:00.000Z">2018-05-09</time></div>
                    <a href="/2018/05/09/排序算法/" class="has-link-black-ter is-size-6">排序算法</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2018/05/05/梯度下降/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/thumbnails/gradient.jpg" alt="线性回归与梯度下降算法">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-05-04T16:00:00.000Z">2018-05-05</time></div>
                    <a href="/2018/05/05/梯度下降/" class="has-link-black-ter is-size-6">线性回归与梯度下降算法</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2018/05/04/牛顿法/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/thumbnails/optimize.jpg" alt="平方根与牛顿法">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-05-04T02:50:15.447Z">2018-05-04</time></div>
                    <a href="/2018/05/04/牛顿法/" class="has-link-black-ter is-size-6">平方根与牛顿法</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2018/05/03/逻辑回归/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/thumbnails/logistics.jpg" alt="逻辑回归">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-05-02T16:00:00.000Z">2018-05-03</time></div>
                    <a href="/2018/05/03/逻辑回归/" class="has-link-black-ter is-size-6">逻辑回归</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
    </div>
</div>

        
            <div class="card widget">
    <div class="card-content">
        <div class="menu">
        <h3 class="menu-label">
            归档
        </h3>
        <ul class="menu-list">
        
        <li>
            <a class="level is-marginless" href="/archives/2018/07/">
                <span class="level-start">
                    <span class="level-item">七月 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2018/05/">
                <span class="level-start">
                    <span class="level-item">五月 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">5</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2018/04/">
                <span class="level-start">
                    <span class="level-item">四月 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">15</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2018/03/">
                <span class="level-start">
                    <span class="level-item">三月 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">2</span>
                </span>
            </a>
        </li>
        
        </ul>
        </div>
    </div>
</div>
        
            <div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                标签
            </h3>
            <div class="field is-grouped is-grouped-multiline">
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/ETL/">
                        <span class="tag">ETL</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Lubridate/">
                        <span class="tag">Lubridate</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/MapReduce/">
                        <span class="tag">MapReduce</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/R/">
                        <span class="tag">R</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/clustering/">
                        <span class="tag">clustering</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/kmeans/">
                        <span class="tag">kmeans</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/优化算法/">
                        <span class="tag">优化算法</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/岭回归/">
                        <span class="tag">岭回归</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/感知机/">
                        <span class="tag">感知机</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/排序/">
                        <span class="tag">排序</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/数据仓库/">
                        <span class="tag">数据仓库</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/数据结构/">
                        <span class="tag">数据结构</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/机器学习/">
                        <span class="tag">机器学习</span>
                        <span class="tag is-grey">9</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/梯度下降/">
                        <span class="tag">梯度下降</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/深度学习/">
                        <span class="tag">深度学习</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/爬虫/">
                        <span class="tag">爬虫</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/矩阵分解/">
                        <span class="tag">矩阵分解</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/神经网络/">
                        <span class="tag">神经网络</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/线性回归/">
                        <span class="tag">线性回归</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/自然语言处理/">
                        <span class="tag">自然语言处理</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/逐步回归/">
                        <span class="tag">逐步回归</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/降维/">
                        <span class="tag">降维</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
            </div>
        </div>
    </div>
</div>
        
        </div>
    
</div>

                




<div class="column is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only has-order-3 column-right ">
    
        
<div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            最新文章
        </h3>
        
        <article class="media">
            
            <a href="/2018/07/02/R语言简明教程/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/thumbnails/Rlogo.png" alt="R语言简明教程">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-07-02T03:43:55.696Z">2018-07-02</time></div>
                    <a href="/2018/07/02/R语言简明教程/" class="has-link-black-ter is-size-6">R语言简明教程</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2018/05/09/排序算法/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/thumbnails/sort.jpg" alt="排序算法">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-05-08T16:00:00.000Z">2018-05-09</time></div>
                    <a href="/2018/05/09/排序算法/" class="has-link-black-ter is-size-6">排序算法</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2018/05/05/梯度下降/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/thumbnails/gradient.jpg" alt="线性回归与梯度下降算法">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-05-04T16:00:00.000Z">2018-05-05</time></div>
                    <a href="/2018/05/05/梯度下降/" class="has-link-black-ter is-size-6">线性回归与梯度下降算法</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2018/05/04/牛顿法/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/thumbnails/optimize.jpg" alt="平方根与牛顿法">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-05-04T02:50:15.447Z">2018-05-04</time></div>
                    <a href="/2018/05/04/牛顿法/" class="has-link-black-ter is-size-6">平方根与牛顿法</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2018/05/03/逻辑回归/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/thumbnails/logistics.jpg" alt="逻辑回归">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-05-02T16:00:00.000Z">2018-05-03</time></div>
                    <a href="/2018/05/03/逻辑回归/" class="has-link-black-ter is-size-6">逻辑回归</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
    </div>
</div>

    
        <div class="card widget">
    <div class="card-content">
        <div class="menu">
        <h3 class="menu-label">
            归档
        </h3>
        <ul class="menu-list">
        
        <li>
            <a class="level is-marginless" href="/archives/2018/07/">
                <span class="level-start">
                    <span class="level-item">七月 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2018/05/">
                <span class="level-start">
                    <span class="level-item">五月 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">5</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2018/04/">
                <span class="level-start">
                    <span class="level-item">四月 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">15</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2018/03/">
                <span class="level-start">
                    <span class="level-item">三月 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">2</span>
                </span>
            </a>
        </li>
        
        </ul>
        </div>
    </div>
</div>
    
        <div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                标签
            </h3>
            <div class="field is-grouped is-grouped-multiline">
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/ETL/">
                        <span class="tag">ETL</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Lubridate/">
                        <span class="tag">Lubridate</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/MapReduce/">
                        <span class="tag">MapReduce</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/R/">
                        <span class="tag">R</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/clustering/">
                        <span class="tag">clustering</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/kmeans/">
                        <span class="tag">kmeans</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/优化算法/">
                        <span class="tag">优化算法</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/岭回归/">
                        <span class="tag">岭回归</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/感知机/">
                        <span class="tag">感知机</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/排序/">
                        <span class="tag">排序</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/数据仓库/">
                        <span class="tag">数据仓库</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/数据结构/">
                        <span class="tag">数据结构</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/机器学习/">
                        <span class="tag">机器学习</span>
                        <span class="tag is-grey">9</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/梯度下降/">
                        <span class="tag">梯度下降</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/深度学习/">
                        <span class="tag">深度学习</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/爬虫/">
                        <span class="tag">爬虫</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/矩阵分解/">
                        <span class="tag">矩阵分解</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/神经网络/">
                        <span class="tag">神经网络</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/线性回归/">
                        <span class="tag">线性回归</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/自然语言处理/">
                        <span class="tag">自然语言处理</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/逐步回归/">
                        <span class="tag">逐步回归</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/降维/">
                        <span class="tag">降维</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
            </div>
        </div>
    </div>
</div>
    
    
</div>

            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/">
                
                    <img src="/images/logo.svg" alt="Young&#39;s Blog" height="28">
                
                </a>
                <p class="is-size-7">
                &copy; 2019 冯洋洋&nbsp;
                Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> & <a
                        href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a>
                
                </p>
            </div>
            <div class="level-end">
            
            </div>
        </div>
    </div>
</footer>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("zh-CN");</script>


    
    
    
    <script src="/js/animation.js"></script>
    

    
    
    
    <script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
    <script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
    <script src="/js/gallery.js" defer></script>
    

    
    

<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>


    
    
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        'HTML-CSS': {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)'],
				['$$','$$']
            ]
        }
    });
});
</script>

    
    

<a id="back-to-top" title="回到顶端" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/js/back-to-top.js" defer></script>


    
    
    
    

    
    
    
    
    
    <script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>
    <script src="/js/clipboard.js" defer></script>
    

    
    
    


<script src="/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="想要查找什么..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: '文章',
                PAGES: '页面',
                CATEGORIES: '分类',
                TAGS: '标签',
                UNTITLED: '(无标题)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js" defer></script>
<link rel="stylesheet" href="/css/search.css">
<link rel="stylesheet" href="/css/insight.css">
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>