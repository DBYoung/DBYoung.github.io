<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>R语言简明教程</title>
      <link href="/2018/07/02/R%E8%AF%AD%E8%A8%80%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/"/>
      <url>/2018/07/02/R%E8%AF%AD%E8%A8%80%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/</url>
      <content type="html"><![CDATA[<h1 id="R语言介绍"><a href="#R语言介绍" class="headerlink" title="R语言介绍"></a>R语言介绍</h1><p>教程测试</p><h1 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h1><p># </p><a id="more"></a>]]></content>
      
      
    </entry>
    
    <entry>
      <title>排序算法</title>
      <link href="/2018/05/09/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"/>
      <url>/2018/05/09/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/</url>
      <content type="html"><![CDATA[<h2 id="排序算法简介"><a href="#排序算法简介" class="headerlink" title="排序算法简介"></a>排序算法简介</h2><h2 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h2><h2 id="所有算法程序"><a href="#所有算法程序" class="headerlink" title="所有算法程序"></a>所有算法程序</h2><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#基础数据结果为python中的list</span></span><br><span class="line">testArr = [<span class="hljs-number">6</span>,<span class="hljs-number">2</span>,<span class="hljs-number">7</span>,<span class="hljs-number">3</span>,<span class="hljs-number">8</span>,<span class="hljs-number">9</span>]</span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">inverse</span><span class="hljs-params">(array)</span>:</span></span><br><span class="line">    lst = array.copy()</span><br><span class="line">    length = len(array)</span><br><span class="line">    mid = int((length - <span class="hljs-number">1</span>) / <span class="hljs-number">2</span>)</span><br><span class="line">    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(mid+<span class="hljs-number">1</span>):</span><br><span class="line">        temp = lst[i]</span><br><span class="line">        lst[i] = lst[length-i<span class="hljs-number">-1</span>]</span><br><span class="line">        lst[length-i<span class="hljs-number">-1</span>] = temp</span><br><span class="line">    <span class="hljs-keyword">return</span> lst</span><br></pre></td></tr></table></figure><a id="more"></a><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#冒泡排序</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bubbleSort</span><span class="hljs-params">(array,desc = False)</span>:</span></span><br><span class="line">    lst = array.copy()</span><br><span class="line">    length = len(array)</span><br><span class="line">    <span class="hljs-comment">#默认升序,降序只需改变符号</span></span><br><span class="line">    <span class="hljs-keyword">if</span> length == <span class="hljs-number">1</span>:</span><br><span class="line">        <span class="hljs-keyword">return</span> lst</span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(length<span class="hljs-number">-1</span>):</span><br><span class="line">            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(i+<span class="hljs-number">1</span>,length):</span><br><span class="line">                <span class="hljs-keyword">if</span> lst[i] &gt; lst[j]:</span><br><span class="line">                    temp = lst[i]</span><br><span class="line">                    lst[i] = lst[j]</span><br><span class="line">                    lst[j] = temp</span><br><span class="line">    <span class="hljs-keyword">return</span> lst</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#快速排序</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">shortQuickSort</span><span class="hljs-params">(array,left,right)</span>:</span></span><br><span class="line">    i = left</span><br><span class="line">    j = right</span><br><span class="line">    <span class="hljs-keyword">if</span> i &gt;= j:</span><br><span class="line">        <span class="hljs-keyword">return</span> array</span><br><span class="line">    key = array[i]</span><br><span class="line">    <span class="hljs-keyword">while</span> i &lt; j:</span><br><span class="line">        <span class="hljs-keyword">while</span> i &lt; j <span class="hljs-keyword">and</span> key &lt;= array[j]:</span><br><span class="line">            j -= <span class="hljs-number">1</span></span><br><span class="line">        array[i] = array[j]</span><br><span class="line">        <span class="hljs-keyword">while</span> i &lt; j <span class="hljs-keyword">and</span> key &gt;= array[i]:</span><br><span class="line">            i += <span class="hljs-number">1</span></span><br><span class="line">        array[j] = array[i]</span><br><span class="line">    array[i] = key</span><br><span class="line">    print(array)</span><br><span class="line">    shortQuickSort(array,left,i<span class="hljs-number">-1</span>)</span><br><span class="line">    shortQuickSort(array,j+<span class="hljs-number">1</span>,right)</span><br><span class="line">    <span class="hljs-keyword">return</span> array</span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">longQuickSort</span><span class="hljs-params">(array)</span>:</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">simInsertSort</span><span class="hljs-params">(array)</span>:</span></span><br><span class="line">    lst = array.copy()</span><br><span class="line">    <span class="hljs-keyword">if</span> len(lst) == <span class="hljs-number">0</span>:</span><br><span class="line">        <span class="hljs-keyword">return</span> lst</span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,len(lst)):</span><br><span class="line">            current = lst[i]</span><br><span class="line">            k = i</span><br><span class="line">            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(i<span class="hljs-number">-1</span>,<span class="hljs-number">-1</span>,<span class="hljs-number">-1</span>):</span><br><span class="line">                <span class="hljs-keyword">if</span> current &lt; lst[j]:</span><br><span class="line">                    lst[j+<span class="hljs-number">1</span>] = lst[j]</span><br><span class="line">                    k = j</span><br><span class="line">            lst[k] = current</span><br><span class="line">        <span class="hljs-keyword">return</span> lst</span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">shellSort</span><span class="hljs-params">()</span>:</span></span><br><span class="line">    <span class="hljs-keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">simSelectSort</span><span class="hljs-params">(array)</span>:</span></span><br><span class="line">    lst = array.copy()</span><br><span class="line">    <span class="hljs-keyword">if</span> len(lst) == <span class="hljs-number">1</span>:</span><br><span class="line">        <span class="hljs-keyword">return</span> lst</span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(lst)<span class="hljs-number">-1</span>):</span><br><span class="line">            minVal = lst[i]</span><br><span class="line">            k = i</span><br><span class="line">            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(i+<span class="hljs-number">1</span>,len(lst)):</span><br><span class="line">                <span class="hljs-keyword">if</span> lst[j] &lt; minVal:</span><br><span class="line">                    minVal = lst[j]</span><br><span class="line">                    k = j</span><br><span class="line">            temp = lst[i]</span><br><span class="line">            lst[i] = lst[k]</span><br><span class="line">            lst[k] = temp</span><br><span class="line">    <span class="hljs-keyword">return</span> lst</span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">heapSort</span><span class="hljs-params">()</span>:</span></span><br><span class="line">    <span class="hljs-keyword">pass</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mergerSort</span><span class="hljs-params">()</span>:</span></span><br><span class="line">    <span class="hljs-keyword">pass</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">radixSort</span><span class="hljs-params">()</span>:</span></span><br><span class="line">    <span class="hljs-keyword">pass</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bucketSort</span><span class="hljs-params">()</span>:</span></span><br><span class="line">    <span class="hljs-keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:</span><br><span class="line">    simSelectSort(testArr)</span><br><span class="line">    </span><br><span class="line">start = time.time()</span><br><span class="line">b = simInsertSort(a)</span><br><span class="line">time.time() - start</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fab</span><span class="hljs-params">(n)</span>:</span></span><br><span class="line">    <span class="hljs-keyword">if</span> n == <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> n == <span class="hljs-number">2</span>:</span><br><span class="line">        <span class="hljs-keyword">return</span> n</span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        <span class="hljs-keyword">return</span> fab(n<span class="hljs-number">-1</span>) + fab(n<span class="hljs-number">-2</span>)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#任何递归都可以通过循环实现，因此，快速排序也使用循环</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fab2</span><span class="hljs-params">(n)</span>:</span></span><br><span class="line">    <span class="hljs-keyword">if</span> n == <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> n ==<span class="hljs-number">2</span> :</span><br><span class="line">        <span class="hljs-keyword">return</span> n</span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        a = <span class="hljs-number">1</span></span><br><span class="line">        b = <span class="hljs-number">2</span></span><br><span class="line">        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">2</span>,n):</span><br><span class="line">            temp = b</span><br><span class="line">            b = a + b</span><br><span class="line">            a = temp</span><br><span class="line">        <span class="hljs-keyword">return</span> b</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> 排序 </tag>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>线性回归与梯度下降算法</title>
      <link href="/2018/05/05/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
      <url>/2018/05/05/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</url>
      <content type="html"><![CDATA[<blockquote><p>【摘要】</p><p>本文以线性回归为例，讲解了批量梯度下降、随机梯度下降、小批量梯度下降、冲量梯度下降等算法，由浅入深，并结合精心设计的例子，使读者最快掌握这种最常用的优化方法。每一种优化方法，笔者都基于R语言给出了相应的代码，供读者参考，</p></blockquote><a id="more"></a><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>假如我们有以下身高和体重的数据，我们希望用身高来预测体重。如果你学过统计，那么很自然地就能想到建立一个线性回归模型：</p><p>$$y=a+bx$$</p><p>其中$a$是截距，$b$是斜率，$y$是体重，$x$是身高。</p><p><img src="/picture/1525406306009-1525855946681.png" alt="1525406306009"></p><p><img src="/picture/1525406028458.png" alt="1525406028458"></p><p>我们将身高与体重的关系在Excel里面用折线图表示，并且添加了线性的趋势线。蓝色的线条是真实数据，红色的实线是模型给出的预测值。蓝色线条与红色线条之间的距离绝对值是预测误差。所以，我们要找到最优的$a$和$b$来拟合这条直线，使得我们模型的总误差最小。</p><p>$$Error = \frac{1}{2}(Actual\ weight - Predicted\ weight)^2=\frac{1}{2}(Y-Ypred)^2$$</p><p>我们使用均方误差来表示模型的误差，由于$Ypred = a + bx$，因此，模型的均方误差可以表示为</p><p>$$SSE = \sum \frac{1}{2}(Y-a-bx)^2$$</p><p>也就是说，$SSE$是关于$a$和$b$的函数，我们只需要不断调整$a$和$b$，使$SSE$降到最低就可以了。这个时候，我们就可以利用梯度下降算法，来求解$a$和$b$的值。</p><p>梯度下降的计算过程如下：</p><blockquote><p>step 1:随机初始化权重$a$和$b$，计算出误差$SSE$</p><p>step 2:计算梯度。    $a$和$b$的轻微变化都会导致$SSE$的变化，因此，我们只需要找到能使$SSE$减小的$a$和$b$的变化方向就可以了。这个方向，一般就是由梯度决定的。</p><p>step 3:调整权重值，使得$SSE$不断接近最小值。</p><p>step 4:使用新的权重去做预测，并且计算出新的$SSE$。</p><p>step 5:重复step2-step3，直到权重不再显著变化为止。</p></blockquote><p>我们在Excel中进行上述步骤。为了计算能够快一点，我们首先对数据进行Min-Max标准化。得到如下数据：</p><p><img src="/picture/1525407652713.png" alt="1525407652713"></p><p>step1:随机选取一组权重(此处我们设置a=0,b=1),我们计算出预测值和误差：</p><p><img src="/picture/1525408865090.png" alt="1525408865090"></p><p>step2:计算梯度</p><p>$$\frac{\partial SSE}{\partial a} = \sum-(Y-a-bx)=\sum-(Y-Ypred)$$</p><p>$$\frac{\partial SSE}{\partial b}=\sum-(Y-a-bx)x=\sum-(Y-Ypred)x$$</p><p>$\frac{\partial SSE}{\partial a}$和$\frac{\partial SSE}{\partial b}$就是梯度，他们决定了$a$和$b$的移动方向和距离。    </p><p>step3: 调整权重值，使得$SSE$不断接近最小值。</p><p>调整规则为:</p><p>$$a_{new} = a_{old} - \eta \nabla a = a_{old} - \eta \cdot \partial SSE/\partial a$$</p><p>$$b_{new} = b_{old} - \eta \nabla b = b_{old} - \eta \cdot \partial SSE / \partial b$$</p><p>其中，$\eta$是一个被我们称之为学习率(learning rate)的东西，一般设置为0.01或者你希望的任何比较小的数值。</p><p>本文选择0.01作为学习率。</p><p>$$a_{new} = 0 - 0.01 \times 1.925 = -0.01925$$</p><p>$$b_{new} = 1 - 0.01 \times 1.117 = 0.98883$$</p><p>step4:使用新的权重去做预测，并且计算出新的$SSE$。</p><p><img src="/picture/1525410198759.png" alt="1525410198759"></p><p>可以看出，SSE从0.155降低到0.111，说明系数有改善。</p><p>step 5:重复step2-step3，直到权重不再显著变化为止。</p><p>我们知道，一元线性回归的系数可以用公式计算，我们用R的lm()函数来计算权重，结果为</p><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">lm(y~x,dat)</span><br><span class="line"></span><br><span class="line">Call:</span><br><span class="line">lm(formula = y ~ x, data = dat)</span><br><span class="line"></span><br><span class="line">Coefficients:</span><br><span class="line">(Intercept)            x  </span><br><span class="line">-<span class="hljs-number">0.1167</span>       <span class="hljs-number">0.9777</span></span><br></pre></td></tr></table></figure><p>然后，我在R里面写了一个梯度下降的函数，当精度调到0.0000001的时候，与lm的结果已经很接近了。</p><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">gradientDescent &lt;- <span class="hljs-keyword">function</span>(dat,start = c(<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),learning_rate = <span class="hljs-number">0.01</span>,tol = <span class="hljs-number">0.001</span>)</span><br><span class="line">&#123;</span><br><span class="line">a = start[<span class="hljs-number">1</span>]</span><br><span class="line">b = start[<span class="hljs-number">2</span>]</span><br><span class="line">x = dat[,<span class="hljs-number">1</span>]</span><br><span class="line">y = dat[,<span class="hljs-number">2</span>]</span><br><span class="line">iters = <span class="hljs-number">0</span></span><br><span class="line"><span class="hljs-keyword">while</span>(<span class="hljs-literal">TRUE</span>)</span><br><span class="line">&#123;</span><br><span class="line">Ypred = a + b * x</span><br><span class="line">old_a = a</span><br><span class="line">old_b = b</span><br><span class="line">a = a + learning_rate * sum(y - Ypred)</span><br><span class="line">b = b + learning_rate * sum((y-Ypred) * x)</span><br><span class="line">iters = iters + <span class="hljs-number">1</span></span><br><span class="line"><span class="hljs-keyword">if</span>(abs(a-old_a) &lt;= tol &amp; abs(b-old_b) &lt;= <span class="hljs-number">0.01</span>)</span><br><span class="line"><span class="hljs-keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">list(weights = c(a,b),iters = iters)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">gradientDescent(dat,tol=<span class="hljs-number">0.0000001</span>)</span><br><span class="line">$weights</span><br><span class="line">[<span class="hljs-number">1</span>] -<span class="hljs-number">0.1167315</span>  <span class="hljs-number">0.9776839</span></span><br><span class="line"></span><br><span class="line">$iters <span class="hljs-comment">#迭代了975次</span></span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">975</span></span><br></pre></td></tr></table></figure><blockquote><p>我们常说的梯度，其实是指向量，其方向与切线方向相同。</p><p>利用梯度下降法进行权重更新的公式为:</p><p>$$weight_{new} = weight_{old} - \eta \cdot \nabla$$</p><p>其中，那个倒三角形就是梯度的意思。我们在高中数学学过，切线方向是函数变化速度最快的方向，</p></blockquote><h2 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h2><p>梯度下降算法，又可以称为Batch-Gradient-Gescent,即批量梯度下降算法。从上面的例子可以看出，批量梯度下降算法，每次更新系数都需要所有的样本参与计算，当样本规模达到一定数量以后，这个更新速度会非常慢。另外，还有可能导致内存溢出。</p><p>为了克服批量梯度下降的这个缺点，有人提出了随机梯度下降(Stochastic Gradient Descent)算法，即每次更新系数只需要一个样本参与计算，因此既可以减少迭代次数，节省计算时间，又可以防止内存溢出。</p><p>对于上述问题，随机梯度下降的算法过程如下：</p><blockquote><p>for every $Y_i$:</p><p>$Ypred = a + bx$</p><p>$a = a + \eta (Y-Ypred)$</p><p>$b = b+\eta(Y-Ypred)\cdot x$</p></blockquote><p>随机梯度下降算法适用于大数量的计算，对于小数据量不一定准确。为了检验随机梯度下降算法，我们构造了一个有10000个样本的数据，同样是计算一元线性回归的系数。</p><p>随机梯度下降的函数如下：</p><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">stochasticGradientDescent &lt;- <span class="hljs-keyword">function</span>(dat,start = c(<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),learning_rate = <span class="hljs-number">0.01</span>,tol = <span class="hljs-number">0.000001</span>,iteratons = <span class="hljs-number">100</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="hljs-comment">#start:初始参数</span></span><br><span class="line"><span class="hljs-comment">#learning_rate:学习率</span></span><br><span class="line"><span class="hljs-comment">#tol:精度</span></span><br><span class="line"><span class="hljs-comment">#iterations:迭代次数</span></span><br><span class="line"></span><br><span class="line">dat = as.matrix(dat)</span><br><span class="line">a = start[<span class="hljs-number">1</span>]</span><br><span class="line">b = start[<span class="hljs-number">2</span>]</span><br><span class="line">x = dat[,<span class="hljs-number">1</span>]</span><br><span class="line">y = dat[,<span class="hljs-number">2</span>]</span><br><span class="line">iters = <span class="hljs-number">0</span></span><br><span class="line"><span class="hljs-keyword">while</span>(iters &lt; iteratons)</span><br><span class="line">&#123;</span><br><span class="line"><span class="hljs-comment">#重排，即将样本的顺序打乱</span></span><br><span class="line">index = sample(length(x))</span><br><span class="line">old_a = a</span><br><span class="line">old_b = b</span><br><span class="line"><span class="hljs-keyword">for</span>(i <span class="hljs-keyword">in</span> index)</span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">Ypred = a + b * x[i]</span><br><span class="line">a = a + learning_rate * (y[i] - Ypred)</span><br><span class="line">b = b + learning_rate * (y[i]-Ypred) * x[i]</span><br><span class="line">&#125;</span><br><span class="line"><span class="hljs-keyword">if</span>(abs(a-old_a) &lt;= tol &amp; abs(b-old_b) &lt;= tol)</span><br><span class="line"> <span class="hljs-keyword">break</span>;</span><br><span class="line">learning_rate = learning_rate / (<span class="hljs-number">1</span> + <span class="hljs-number">0.01</span> * iters) <span class="hljs-comment">#自适应学习率</span></span><br><span class="line">iters = iters + <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">if</span>(iters &gt; iterations)</span><br><span class="line">            <span class="hljs-keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">list(weights = c(a,b),iters = iters)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后我们构造一个相对大的样本用来检验算法：</p><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">set.seed(<span class="hljs-number">100</span>)</span><br><span class="line">x &lt;- seq(<span class="hljs-number">1</span>,<span class="hljs-number">10</span>,length.out = <span class="hljs-number">10000</span>)</span><br><span class="line">y &lt;- <span class="hljs-number">2</span> * x + rnorm(<span class="hljs-number">10000</span>) * <span class="hljs-number">10</span> + <span class="hljs-number">2</span></span><br><span class="line">bigdata &lt;- data.frame(x ,y )</span><br><span class="line">plot(x,y)</span><br><span class="line">cor(x,y)</span><br></pre></td></tr></table></figure><p><img src="/picture/sgd.png" alt="sgd"></p><p>回归结果：</p><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lm(y~x,data = bigdata )</span><br><span class="line">Call:</span><br><span class="line">lm(formula = y ~ x, data = bigdata)</span><br><span class="line"></span><br><span class="line">Coefficients:</span><br><span class="line">(Intercept)            x  </span><br><span class="line">      <span class="hljs-number">2.004</span>        <span class="hljs-number">2.006</span></span><br></pre></td></tr></table></figure><p>随机梯度下降的结果：</p><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">stochasticGradientDescent(bigdata,learning_rate = <span class="hljs-number">0.001</span>,tol=<span class="hljs-number">0.000000001</span>)</span><br><span class="line">$weights</span><br><span class="line"><span class="hljs-number">2.01138749995603</span> <span class="hljs-number">2.00584502615877</span></span><br><span class="line">$iters</span><br><span class="line"><span class="hljs-number">69</span></span><br></pre></td></tr></table></figure><p>批量梯度下降的结果：</p><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">batchGradientDescent(bigdata,learning_rate = <span class="hljs-number">0.000001</span>,tol = <span class="hljs-number">0.000000001</span>)</span><br><span class="line">$weights</span><br><span class="line"><span class="hljs-number">2.00385275101478</span> <span class="hljs-number">2.00634924457312</span></span><br><span class="line">$iters</span><br><span class="line"><span class="hljs-number">8345</span></span><br></pre></td></tr></table></figure><p>可以看到，在同样的精度要求下，随机梯度下降进行59次迭代以后即收敛，而批量梯度下降则需要迭代8345次。</p><p>但是随机梯度下降也有一个缺点，即参数更新频率太快，有可能出现目标函数值在最优质附近的震荡现象，因为高频率的参数更新导致了高方差。 同时也可以看出，在相同精度要求下，随机梯度下降计算出来的系数与精确值离差较大，而批量随机下降则更接近精确值。</p><h2 id="Mini-batch-Gradient-Descent"><a href="#Mini-batch-Gradient-Descent" class="headerlink" title="Mini-batch Gradient Descent"></a>Mini-batch Gradient Descent</h2><p>小批量梯度下降(Mini-batch Gradient Descent)是介于上述两种方法之间的优化方法，即在更新参数时，只使用一部分样本（一般256以下）来更新参数，这样既可以保证训练过程更稳定，又可以利用批量训练方法中的矩阵计算的优势。</p><p>具体更新哪些样本，通常是随机确定的，下面，我们定义一下小批量梯度下降的函数，用来求解上述bigdata的系数。</p><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">miniBatchGradientDescent &lt;- <span class="hljs-keyword">function</span>(dat,start = c(<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),learning_rate = <span class="hljs-number">0.01</span>,tol = <span class="hljs-number">0.001</span>,batchSize = <span class="hljs-number">256</span>,iterations = <span class="hljs-number">10000</span>)</span><br><span class="line">&#123;</span><br><span class="line">    a = start[<span class="hljs-number">1</span>]</span><br><span class="line">    b = start[<span class="hljs-number">2</span>]</span><br><span class="line">    iters = <span class="hljs-number">0</span></span><br><span class="line">    len = length(y)</span><br><span class="line">    <span class="hljs-keyword">while</span>(<span class="hljs-literal">TRUE</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        mini_index = sample(len,batchSize,replace = <span class="hljs-literal">FALSE</span>)</span><br><span class="line">        x = dat[mini_index,<span class="hljs-number">1</span>]</span><br><span class="line">        y = dat[mini_index,<span class="hljs-number">2</span>]</span><br><span class="line">        Ypred = a + b * x</span><br><span class="line">        error = y - Ypred</span><br><span class="line">        old_a = a</span><br><span class="line">        old_b = b</span><br><span class="line">        a = a + learning_rate * sum(error)</span><br><span class="line">        b = b + learning_rate * sum((error) * x)</span><br><span class="line">        start = rbind(start,c(a,b))</span><br><span class="line">        iters = iters + <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">if</span>(abs(a-old_a) &lt;= tol &amp; abs(b-old_b) &lt;= tol)</span><br><span class="line">            <span class="hljs-keyword">break</span></span><br><span class="line">        <span class="hljs-keyword">if</span>(iters &gt;= iterations)</span><br><span class="line">            <span class="hljs-keyword">break</span></span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    list(weights = c(a,b),iters = iters,coes = start)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">miniBatchGradientDescent(bigdata,learning_rate = <span class="hljs-number">0.0001</span>,tol = <span class="hljs-number">0.00001</span>,batchSize = <span class="hljs-number">100</span>)</span><br><span class="line">$weights</span><br><span class="line"><span class="hljs-number">2.02646349019186</span> <span class="hljs-number">2.0439693915315</span></span><br><span class="line">$iters</span><br><span class="line"><span class="hljs-number">920064</span></span><br></pre></td></tr></table></figure><p>小梯度批量梯度下降收敛时需要迭代92万次，这显然有点多了。一般来说，当数据量非常大时，小批量梯度下降比较有效，否则计算结果很有可能出现偏移。</p><blockquote><p>先mark，偏移的原因待考究。</p></blockquote><h2 id="Momentum-optimization"><a href="#Momentum-optimization" class="headerlink" title="Momentum optimization"></a>Momentum optimization</h2><p>考虑这样一种情形，小球从山顶往下滚动，一开始很顺利，可是在接近最低点的时候，小球陷入了一段狭长的浅山谷。由于小球一开始并不是直接沿着山谷的方向滚下，因此小球会在这个浅浅的山谷中不断震荡——不断冲上墙壁，接着又从墙壁上滚下来。这种情况并不是我们想看到的，因为这增加了迭代时间。冲量(Momentnum)的引入，使得我们的目标更新的更快了，冲量的更新方式有以下两种，两种方式之间并无太大差异。</p><blockquote><p>第一种：</p><p>$Z^{k+1}=\beta Z_k + \nabla$</p><p>$w^{k+1} = w_k - \alpha Z^{k+1}$</p><p>其中，$Z$是一个与$w$方向相同的向量，</p></blockquote><blockquote><p>第二种：</p><p>$Z^{k+1}=\beta Z^k + \alpha \nabla$</p><p>$w^{k+1} = w^k - Z^{k+1}$</p></blockquote><p><img src="/picture/1525839842032.png" alt="1525839842032"></p><p>两者的差别仅仅在于$Z^{k+1}$的系数不同。</p><p>通常，这里的学习率要比随机梯度下降小一点，因为随机梯度下降的梯度大一点。$\beta$的取值决定了前一次的梯度有多少被纳入了本次的更新。一般来说，稳定前将$\beta$设置为0.5，稳定后可以设置为0.9或更高。</p><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#适用于求解一元或多元线性回归的回归系数，返回结果包括截距</span></span><br><span class="line">momentumGradientDescent &lt;- <span class="hljs-keyword">function</span>(dat,beta = <span class="hljs-number">0.9</span>,z = <span class="hljs-number">0</span>,start = rep(<span class="hljs-number">0</span>,dim(dat)[<span class="hljs-number">2</span>]),alpha = <span class="hljs-number">0.001</span>,tol=<span class="hljs-number">0.0000001</span>,iterations = <span class="hljs-number">100</span>)</span><br><span class="line">&#123;</span><br><span class="line">    dataSet = cbind(<span class="hljs-number">1</span>,dat)  <span class="hljs-comment">#将第一列加上1</span></span><br><span class="line">    cols = dim(dataSet)[<span class="hljs-number">2</span>] <span class="hljs-comment">#列数</span></span><br><span class="line">    x = as.matrix(dataSet[,<span class="hljs-number">1</span>:(cols - <span class="hljs-number">1</span>)])  <span class="hljs-comment">#自变量矩阵</span></span><br><span class="line">    y = as.matrix(dataSet[,cols],ncol = <span class="hljs-number">1</span>)  <span class="hljs-comment">#因变量，矩阵</span></span><br><span class="line">    w = as.matrix(start,ncol = <span class="hljs-number">1</span>)  <span class="hljs-comment">#权重矩阵</span></span><br><span class="line">    iters = <span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">while</span>(<span class="hljs-literal">TRUE</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        old_w = w</span><br><span class="line">        old_z = z</span><br><span class="line">        Ypred = x %*% w</span><br><span class="line">        error = y - Ypred</span><br><span class="line">        grad = - t(x) %*% error</span><br><span class="line">        z = beta * old_z + grad</span><br><span class="line">        w = old_w - alpha * z</span><br><span class="line">        <span class="hljs-keyword">if</span>(sum(abs(w-old_w)) &lt; tol)</span><br><span class="line">            <span class="hljs-keyword">break</span>;</span><br><span class="line">        iters = iters + <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">if</span>(is.integer(iterations))</span><br><span class="line">            <span class="hljs-keyword">if</span>(iters &gt;= iterations)</span><br><span class="line">                <span class="hljs-keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    list(weights = as.vector(w),iters = iters)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>利用冲量梯度下降求bigdata的系数：</p><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">momentumGradientDescent(bigdata,alpha=<span class="hljs-number">0.00001</span>)</span><br><span class="line">$weights</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">2.003851</span> <span class="hljs-number">2.006354</span></span><br><span class="line"></span><br><span class="line">$iters</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">248</span></span><br></pre></td></tr></table></figure><p>可以看到，迭代次数明显减少，并且系数与精确值更加接近了。</p><h2 id="Nesterov-Accelerated-Gradient"><a href="#Nesterov-Accelerated-Gradient" class="headerlink" title="Nesterov Accelerated Gradient"></a>Nesterov Accelerated Gradient</h2><p>然而，让一个小球盲目地沿着斜坡滚下山是不理想的。我们需要一个更聪明的球，它知道下一步要往哪里去，因此在斜坡有上升的时候，它能够自主调整方向。</p><p>Nesterov Accelerated Gradient 是基于冲量梯度下降算法进行改进的一种算法，也是梯度下降算法的变种。</p><p>在上一种算法中，我们使用了冲量$\beta Z^k$来调整我们的参数改变量，将上述第二种更新方法改写一下，得到如下式子：</p><blockquote><p>$Z^{k+1} = \beta Z^k + \alpha \nabla$</p><p>$w^{k+1} = w^k - Z^{k+1} = w^k - \beta Z_k - \alpha \nabla $</p></blockquote><p>$w^k - \beta Z_k $给出了下一个参数位置的近似，指明了参数该如何变化。现在，我们可以不用当前的参数，而是用未来的参数的近似位置来更新我们的参数，即</p><blockquote><p>$Z^{k+1} = \beta Z^k + \alpha \nabla _wJ(w - \beta Z^k)$</p><p>$w^{k+1} = w^k - Z^{k+1}$</p></blockquote><p>这里，我们仍然设置$\beta$的值在0.9附近。如下图所示，冲量梯度下降先计算当前的梯度（短的蓝色向量），然后根据累积的冲量向前跨越一大步（长的蓝色向量）。NAG首先根据之前的累积梯度向前迈一大步（棕色向量），然后对梯度进行修正（红色向量）。这种利用近似未来参数来更新参数的方法，可以防止梯度更新太快，并且增加了响应能力。</p><p><img src="/picture/nesterov_update_vector.png" alt="nesterov_update_vector"> </p><blockquote><p>假设我们的权重矩阵（系数矩阵）为$w$，自变量$x$，因变量$Y$，则损失函数为：</p><p>$$J(w) = 1/2 \sum _{i=1} ^n (Y_i - w’x_i)^2$$</p><p>则</p><p>$$\nabla J(w) = \frac{\partial J(w)}{\partial w} = - \sum_{i=1}^n(Y_i - w’x_i)x_i’$$</p><p>那么</p><p>$$\nabla J(w - \beta Z^{k}) = - \sum _{i=1}^n(Y_i - (w’-\beta Z^k)’x_i)x_i’$$</p></blockquote><p>根据上述思想，编写的NAG代码如下 ：</p><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#适用于求解一元或多元线性回归的回归系数，返回结果包括截距</span></span><br><span class="line">NAG &lt;- <span class="hljs-keyword">function</span>(dat,beta = <span class="hljs-number">0.9</span>,z = <span class="hljs-number">0</span>,start = rep(<span class="hljs-number">0</span>,dim(dat)[<span class="hljs-number">2</span>]),alpha = <span class="hljs-number">0.001</span>,tol=<span class="hljs-number">0.0000001</span>,iterations = <span class="hljs-number">100</span>)</span><br><span class="line">&#123;</span><br><span class="line">    dataSet = cbind(<span class="hljs-number">1</span>,dat)  <span class="hljs-comment">#将第一列加上1</span></span><br><span class="line">    cols = dim(dataSet)[<span class="hljs-number">2</span>] <span class="hljs-comment">#列数</span></span><br><span class="line">    x = as.matrix(dataSet[,<span class="hljs-number">1</span>:(cols - <span class="hljs-number">1</span>)])  <span class="hljs-comment">#自变量矩阵</span></span><br><span class="line">    y = dataSet[,cols]  <span class="hljs-comment">#因变量，矩阵</span></span><br><span class="line">    w = start  <span class="hljs-comment">#权重矩阵</span></span><br><span class="line">    iters = <span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">while</span>(<span class="hljs-literal">TRUE</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        old_w = w</span><br><span class="line">        old_z = z</span><br><span class="line">        <span class="hljs-comment">#中间过程用mid1 mid2代替</span></span><br><span class="line">        mid1 = apply((old_w - beta * old_z) * t(x),<span class="hljs-number">2</span>,sum)</span><br><span class="line">        mid2 = y - mid1</span><br><span class="line">        grad = - apply(mid2 * x,<span class="hljs-number">2</span>,sum)</span><br><span class="line">        z = beta * old_z + alpha * grad</span><br><span class="line">        w = old_w - z</span><br><span class="line">        <span class="hljs-keyword">if</span>(sum(abs(w-old_w)) &lt; tol)</span><br><span class="line">            <span class="hljs-keyword">break</span>;</span><br><span class="line">        iters = iters + <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">if</span>(is.integer(iterations))</span><br><span class="line">            <span class="hljs-keyword">if</span>(iters &gt;= iterations)</span><br><span class="line">                <span class="hljs-keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    list(weights = as.vector(w),iters = iters)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>用NAG来求bigdata的系数：</p><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">NAG(bigdata,alpha=<span class="hljs-number">0.000001</span>)</span><br><span class="line">$weights</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">2.003849</span> <span class="hljs-number">2.006350</span></span><br><span class="line"></span><br><span class="line">$iters</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">598</span></span><br></pre></td></tr></table></figure><h2 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h2><p>尽管我们可以根据损失函数的梯度来加快更新参数，我们也希望能够根据参数的重要性来决定其更新的幅度。</p><p>AdaGrad是一种基于梯度算法的优化算法,它只做了一件事:根据参数来自适应调整学习率。对于不常出现的参数进行较大的更新，对于经常出现的参数进行较少的更新，因此，这种方法非常适合处理稀疏数据。</p><p>之前，我们对每一个参数更新所使用的学习率都是一样的，而AdaGrad在每一步都使用不同的学习率对不同的参数进行更新。我们先写出AdaGrad的单个参数的更新方法，然后将其向量化。长话短说，假设$g_{t,i}$表示损失函数对于参数$\theta_i$的梯度：</p><p>在步骤$t$:</p><p>$$g_{t,i} = \nabla _\theta J(\theta_t,i)$$</p><p>那么，对于步骤$t$，使用随机梯度下将对$\theta_i$进行更新的公式为：</p><p>$$\theta_{t+1,i} = \theta_{t,i} - \eta_i \cdot g_{t,i}$$</p><p>在上述更新过程中，AdaGrad在每一步都对参数$\theta_i$对应的学习率$\eta_i$进行调整，调整的方法基于过去的所有梯度：</p><p>$$\theta_{t+1,i} = \theta_{t,i} - \frac{\eta_i}{\sqrt{G_{t,ii} + \epsilon}} \cdot g_{t,i}$$</p><p>$G_{t}\in R^{d\times d}$是一个对角矩阵，第$i$个对角元素是历史上损失函数对$\theta_i$的所有梯度的平方和，$\epsilon$是一个平滑参数，防止分母为0，通常取$10^{-8}$。有趣的是，如果不加开方，算法表现极差。</p><p>因为$G_t$包含了过去所有参数梯度的平方和，因此我们可以将其向量化：</p><p>$$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}}\odot g_t$$</p><p>AdaGrad的一个最大的好处是不用手动调整学习率的大小，通常设置默认值为0.01，然后顺其自然就好了。</p><p>AdaGrad的一个较大的缺点是，分母是不断增大的，当迭代次数不断增加时，分母会逐渐趋于无穷大，学习率进而趋于无穷小，此时，算法将变得不再有效。</p><p>我们延用上述符号来写出AdaGrad的函数。</p><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#SGD为基础</span></span><br><span class="line">AdaGrad &lt;- <span class="hljs-keyword">function</span>(dat,theta,learning_rate = <span class="hljs-number">0.01</span>,start = rep(<span class="hljs-number">0</span>,dim(dat)[<span class="hljs-number">2</span>]),tol = <span class="hljs-number">0.000001</span>,iterations = <span class="hljs-number">1000</span>)</span><br><span class="line">&#123;</span><br><span class="line">    dataSet = cbind(<span class="hljs-number">1</span>,dat) <span class="hljs-comment">#将自变量进行增广，第一列全为1</span></span><br><span class="line">    cols = dim(dataSet)[<span class="hljs-number">2</span>] <span class="hljs-comment">#列数</span></span><br><span class="line">    rows = dim(dataSet)[<span class="hljs-number">1</span>] <span class="hljs-comment">#行数</span></span><br><span class="line">    x = as.matrix(dataSet[,<span class="hljs-number">1</span>:(cols - <span class="hljs-number">1</span>)])  <span class="hljs-comment">#自变量矩阵</span></span><br><span class="line">    y = dataSet[,cols]  <span class="hljs-comment">#因变量，矩阵</span></span><br><span class="line">    g = <span class="hljs-number">0</span></span><br><span class="line">    theta = start  <span class="hljs-comment">#权重矩阵</span></span><br><span class="line">    iters = <span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">while</span>(<span class="hljs-literal">TRUE</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        old_theta = theta</span><br><span class="line">        index = sample(rows)</span><br><span class="line">        <span class="hljs-keyword">for</span>(i <span class="hljs-keyword">in</span> index)</span><br><span class="line">        &#123;</span><br><span class="line">            grad = - (y[i] - sum(theta * x[i,])) * x[i,]</span><br><span class="line">            g = g + grad * grad</span><br><span class="line">            theta = theta - learning_rate / sqrt(g + <span class="hljs-number">1e-8</span>) * grad</span><br><span class="line">        &#125;</span><br><span class="line">        iters = iters + <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">if</span>(is.integer(iterations))</span><br><span class="line">            <span class="hljs-keyword">if</span>(iters &gt; iterations)</span><br><span class="line">                <span class="hljs-keyword">break</span>;</span><br><span class="line">        <span class="hljs-keyword">if</span>(sum(abs(old_theta - theta)) &lt; tol)</span><br><span class="line">            <span class="hljs-keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    list(weights = as.vector(theta),iters = iters)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>AdaGrad的计算结果如下：</p><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">AdaGrad(bigdata)</span><br><span class="line">$weights</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">2.008219</span> <span class="hljs-number">2.005682</span></span><br><span class="line"></span><br><span class="line">$iters</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">6579</span></span><br></pre></td></tr></table></figure><h2 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h2><p>AdaDelta是在AdaGrad的基础上发展而来的，目的是解决AdaGrad算法中学习率的单调减少问题。AdaDelta不再采用累积梯度平方和的方法来调整学习率，而是根据一些固定的$w$的大小来限制过去累积梯度的窗口。</p><p>AdaDelta不再无效率地存储历史梯度的平方和，而将历史梯度平方和定义为衰减均值(decaying average)。第$t$步的移动平均值$E[g^2]_t$仅仅取决于过去的平均值和当前梯度（有点类似于momentum）：</p><p>$$E[g^2]<em>t = \gamma E[g^2]</em>{t-1} + (1-\gamma)g_t^2$$</p><p>同样的，我们把$\gamma$设置在0.9附近。为清楚起见，我们重新写下更新规则：</p><blockquote><p>$$\Delta \theta <em>t = - \eta \cdot g</em>{t,i}$$</p><p>$$\theta _{t+1} = \theta_t + \Delta \theta _t$$</p></blockquote><p>根据AdaGrad我们推出AdaDelta的参数更新公式：</p><blockquote><p>$$\Delta \theta _t = - \frac{\eta}{\sqrt{G_t + \epsilon}} \odot g_t$$</p></blockquote><p>我们只需把$G_t$替换$E[g^2]_t$就行了：</p><blockquote><p>$$\theta_t = - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}}$$</p></blockquote><p>因为分母刚好是梯度的均方根，我们将其简写为：</p><blockquote><p>$$\Delta \theta_t = -\frac{\eta}{RMS[g]_t}g_t$$</p></blockquote><p>作者们发现上述更新中的单位不一致（可以理解为量纲不一致），因此，他们定义了另外一个指数衰减均值，这次不用梯度的平方了，而是用参数的平方来进行更新：</p><p>$$E[\Delta \theta^2]<em>t = \gamma E[\Delta \theta^2]</em>{t-1} + (1-\gamma)\Delta\theta_t^2$$</p><p>参数更新的均方误差即：</p><p>$$RMS[\Delta\theta]_t = \sqrt{E[\Delta\theta^2]_t + \epsilon}$$</p><p>因为$RMS[\theta]<em>t$是未知的，我们可以用之前所有的更新过的参数的RMS来代替。将之前的学习率$\eta$换成$RMS[\Delta\theta]</em>{t-1}$，那么，我们得出AdaDelta的更新规则：</p><p>$$\Delta\theta_t = -\frac{RMS[\Delta\theta]_{t-1}}{RMS[g]_t}g_t$$</p><p>$$\theta_{t+1} = \theta_t + \Delta\theta_t$$</p><p>AdaDelta甚至不需要初始化学习率，因为在更新规则中已经不见它的身影了。</p><p>根据上述思路，我们写出AdaDelta的函数:</p><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">RMS &lt;- <span class="hljs-keyword">function</span>(x)</span><br><span class="line">&#123;</span><br><span class="line">    sqrt(mean(x^<span class="hljs-number">2</span>) - mean(x)^<span class="hljs-number">2</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">AdaDelta &lt;- <span class="hljs-keyword">function</span>()</span><br></pre></td></tr></table></figure><h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p><strong>上述所有改进方法均可以运用于批量梯度下降、小批量梯度下降和随机梯度下降！</strong></p><p>[1]<a href="http://netedu.xauat.edu.cn/jpkc/netedu/jpkc/gdsx/homepage/5jxsd/51/513/5308/530807.htm" target="_blank" rel="noopener">http://netedu.xauat.edu.cn/jpkc/netedu/jpkc/gdsx/homepage/5jxsd/51/513/5308/530807.htm</a></p><p>[2]<a href="https://www.kdnuggets.com/2017/04/simple-understand-gradient-descent-algorithm.html" target="_blank" rel="noopener">https://www.kdnuggets.com/2017/04/simple-understand-gradient-descent-algorithm.html</a></p><p>[3]<a href="https://distill.pub/2017/momentum/" target="_blank" rel="noopener">https://distill.pub/2017/momentum/</a></p><p>[4]<a href="http://ruder.io/optimizing-gradient-descent/index.html" target="_blank" rel="noopener">http://ruder.io/optimizing-gradient-descent/index.html</a></p>]]></content>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 优化算法 </tag>
            
            <tag> 梯度下降 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>平方根与牛顿法</title>
      <link href="/2018/05/04/%E7%89%9B%E9%A1%BF%E6%B3%95/"/>
      <url>/2018/05/04/%E7%89%9B%E9%A1%BF%E6%B3%95/</url>
      <content type="html"><![CDATA[<a id="more"></a>]]></content>
      
      
    </entry>
    
    <entry>
      <title>逻辑回归</title>
      <link href="/2018/05/03/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
      <url>/2018/05/03/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
      <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>逻辑回归常用来处理分类问题，最常用来处理二分类问题。</p><p>生活中经常遇到具有两种结果的情况（冬天的北京会下雪，或者不会下雪；暗恋的对象也喜欢我，或者不喜欢我；今年的期末考试会挂科，或者不会挂科……）。对于这些二分类结果，我们通常会有一些输入变量，或者是连续性，或者是离散型。那么，我们怎样来对这些数据建立模型并且进行分析呢？</p><a id="more"></a><p>我们可以尝试构建一种规则来根据输入变量猜测二分输出变量，这在统计机器学上被称为分类。然而，简单的给一个回答“是”或者“不是”显得太过粗鲁，尤其是当我们没有完美的规则的时候。总之呢，我们不希望给出的结果就是武断的“是”或“否”，我们希望能有一个概率来表示我们的结果。</p><p>一个很好的想法就是，在给定输入$X$的情况下，我们能够知道Y的条件概率$Pr(Y|X)$。一旦给出了这个概率，我们就能够知道我们预测结果的准确性。</p><p>让我们把其中一个类称为1，另一个类称为0。（具体哪一个是1，哪一个是0都无所谓）。$Y$变成了一个指示变量，现在，你要让自己相信，$Pr(Y=1)=EY$，类似的，$Pr(Y=1|X=x)=E[Y|X=x]$。</p><blockquote><p>假设$Y$有10个观测值，分别为 0 0 0 1 1 0 1 0 0 1.即6个0,4个1.那么，$Pr(Y=1)=\frac{count(1)}{count(n)}=\frac{4}{10}=0.4$，同时，$EY=\frac{sum(Y)}{count(n)}=\frac{4}{10}=0.4$</p></blockquote><p>换句话说，条件概率是就是指示变量（即$Y$)的条件期望。这对我们有帮助，因为从这个角度上，我们知道所有关于条件期望的估计。我们要做的最直接的事情是挑选出我们喜欢的平滑器，并估计指示变量的回归函数，这就是条件概率函数的估计。</p><p>有两个理由让我们放弃陷入上述想法。</p><ol><li>概率必须介于0和1之间，但是我们在上面估计出来的平滑函数的输出结果却不能保证如此，即使我们的指示变量$y_i$不是0就是1；</li><li>另一种情况是，我们可以更好地利用这个事实，即我们试图通过更显式地模拟概率来估计概率。</li></ol><p>假设$Pr(Y=1|X=x)=p(x;\theta)$,$p$是参数为$\theta$的函数。进一步，假设我们的所有观测都是相互独立的，那么条件似然函数可以写成：</p><p>$$\prod _{i=1}^nPr(Y=y_i|X=x_i)=\prod _{i=1}^np(x_i;\theta)^{y_i}(1-p(x_i;\theta))^{1-y_i}$$</p><p>回忆一下，对于一系列的伯努利试验$y_1,y_2,\cdots,y_n$，如果成功的概率都是常数$p$，那么似然概率为：</p><p>$$\prod _{i=1}^n p^{y_i}(1-p)^{1-y_i}$$</p><p>我们知道，当$p=\hat{p}=\frac{1}{n}\sum _{i=1}^ny_i$时，似然概率取得最大值。如果每一个试验都有对应的成功概率$p_i$，那么似然概率就变成了</p><p>$$\prod _{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i}$$</p><p>不添加任何约束的通过最大化似然函数来估计上述模型是没有意义的。当$\hat{p_i}=1$的时候，$y_i=1$，当$\hat{p_i}=0$的时候，$y_i=0$。我们学不到任何东西。如果我们假设所有的$p_i$不是任意的数字，而是相互连接在一起的，这些约束给我们提供了一个很重要的参数，我们可以通过这个约束来求得似然函数的最大值。对于我们正在讨论的这种模型，约束条件就是$p_i=p(x_i;\theta)$，当$x_i$相同的时候，$p_i$也必须相同。因为我们假设的$p$是未知的，因此似然函数是参数为$\theta$的函数，我们可以通过估计$\theta$来最大化似然函数。</p><h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p>总结一下：我们有一个二分输出变量$Y$，我们希望构造一个关于$x$的函数来计算$Y$的条件概率$Pr（{Y=1|X=x}）$，所有的未知参数都可以通过最大似然法来估计。到目前为止，你不会惊讶于发现统计学家们通过问自己“我们如何使用线性回归来解决这个问题”。</p><ul><li>最简单的一个想法就是令$p(x)$为线性函数。无论$x$在什么位置，$x$每增加一个单位，$p$的变化量是一样的。由于线性函数不能保证预测结果位于0和1之间，因此从概念上线性函数就不适合。另外，在很多情况下，根据我们的经验可知，当$p$很大的时候，对于$p$的相同的变化量，$x$的变化量将会大于$p$在1/2附近的变化量。线性函数做不到这样。</li><li>另一个最直观的想法是令$log\ p(x)$为$x$的线性函数。但是对数函数只在一个方向上是无界的，因此也不符合要求。</li><li>最后，我们选择了$p$经过logit变换以后的函数$ln\frac{p}{1-p}$。这个函数就很好啊，满足了我们的所有需求。</li></ul><p>最后一个选择就是我们所说的逻辑回归。一般的，逻辑回归的模型可以表示为如下形式：</p><p>$$ln\frac{p(x)}{1-p(x)}=\beta_0+x\beta$$</p><p>根据上式，解出$p$</p><p>$$p(x;b,w)=\frac{e^{\beta_0+x\beta}}{1+e^{\beta_0+x\beta}}=\frac{1}{1+e^{-(\beta_0+x\beta)}}$$</p><p>为了最小化错分率，当$p\ge 0.5$的时候，我们预测$Y=1$，否则$Y=0$。这意味着当$\beta_0+x\beta$非负的时候，预测结果为1，否则为预测结果为0.因此，逻辑回归为我么提供了一个线性分类器。决策边界是$\beta_0+x\beta=0$，当$x$是一维的时候，决策边界是一个点，当$x$是二维的时候，决策边界是一条直线，以此类推。空间中某个点到决策边界的距离为$\beta_0/||\beta||+x\cdot\beta/||\beta||$.逻辑回归不仅告诉我们两个类的决策边界，还以一种独特的方式根据样本点到决策边界的距离给出该点分属于某类的概率。当$||\beta||$越大的时候，概率取极端值（0或1）的速度就越快。上述说明使得逻辑回归不仅仅是一个分类器，它能做出更简健壮、更详细的预测，并能以一种不同的方式进行拟合;但那些强有力的预测可能是错误的。</p><h2 id="似然函数和逻辑回归"><a href="#似然函数和逻辑回归" class="headerlink" title="似然函数和逻辑回归"></a>似然函数和逻辑回归</h2><p>因为逻辑回归的预测结果是概率，而不是类别，因此我们可以用似然函数来拟合模型。对于每一个样本点，我们有一个特征向量$x_i$，这个向量的维度就是特征的个数。同时还有一个观测类别$y_i$。当$y_i=1$的时候，该类的概率为$p$，否则为$1-p$。因此，似然函数为：</p><p>$$L(\beta_0,\beta) = \prod _{i=1}^np(x_i)^{y_i}(1-p(x_i))^{1-y_i}$$</p><p>对数似然函数为：</p><p>$$\begin{aligned} \ell(\beta_0,\beta) &amp;= \sum_{i=1}^ny_iln(p(x_i))+(1-y_i)ln(1-p(x_i)) \&amp;=\sum_{i=1}^n(y_i(ln\frac{p(x_i)}{1-p(x_i)})+ln(1-p(x_i))) \&amp;=\sum_{i=1}^ny_i(\beta_0+x_i\beta)-ln(1+e^{\beta_0+x_i\beta}) \end{aligned}$$</p><p> 为了表示方便，统一将$\beta_0,\beta$表示成$\beta$,则$\ell$对$\beta$的一阶导数为： </p><p>$$\begin{aligned} \frac{\partial \ell}{\partial \beta} &amp;=\sum _{i=1}^n[y_i-\frac{e^{\beta^Tx_i}}{1+\beta^Tx_i}]x_i\&amp; = \sum _{i=1}^n(y_i-p_i)x_i\end{aligned}$$</p><h2 id="多分类逻辑回归"><a href="#多分类逻辑回归" class="headerlink" title="多分类逻辑回归"></a>多分类逻辑回归</h2><p>如果$Y$有多个类别，我们仍然可以使用逻辑回归。假如有$k$个类别，分别是$0,1,\cdots,k-1$，对于每一个类$k$，其都有对应的$\beta_0$和$\beta$，每个类对应的概率为:</p><p>$$P(Y=c|X=x)=\frac{e^{\beta_0^c+x\beta^c}}{\sum e^{\beta_0^c+x\beta^c}}$$</p><p>观察上式可以发现，二分类逻辑回归求是多分类逻辑回归的特例.</p><blockquote><p>在这里，读者可能比较好奇，根据上式，二分类逻辑回归的分母中的1是怎么来的。其实，无论有多少个类，我们总是将第一类的系数设置为0，那么类别为0的那部分在分母中对应的就是1.这样做对模型的通用性没有任何影响。</p><p>有读者可能会问，为什么偏要把第一个类的系数设置为0，而不是其他的类。事实上，你可以设置任何一个类的系数为0，并且最终计算出来的结果都是一样的。所以，按照惯例，我们都是把第一个类的系数设置为0.</p></blockquote><h2 id="牛顿法求解参数"><a href="#牛顿法求解参数" class="headerlink" title="牛顿法求解参数"></a>牛顿法求解参数</h2><p>为了求出待估参数$\beta$，我们利用Newton-Raphson算法。首先对对数似然函数求二阶偏导： </p><p>$$\frac{\partial ^2\ell (\beta)}{\partial \beta \partial \beta^T}=-\sum_{i=1}^nx_ix_i^Tp_i(1-p_i)$$ </p><p>注意，上面的$x_i$是个向量，也就是上面所说的特征向量，维度为特征个数加一。即假设原始数据为$n\times m$矩阵，其中n表示观测数，m表示特征数。则$x_i$的长度为m+1。根据上述说明，上面的二阶偏导实际上是一个$(m+1)\times (m+1)$的矩阵。</p><p>如果给定一个$\hat{\beta}^{old}$，则一步牛顿迭代为（梯度下降）：</p><p>$$\hat{\beta}^{new}=\hat{\beta}^{old}-(\frac{\partial ^2\ell (\beta)}{\partial \beta \partial \beta^T})^{-1} \cdot\frac{\partial \ell({\beta})}{\partial \beta}$$</p><p>将上述式子表示成矩阵的形式就是：</p><p>$$\frac{\partial \ell(\beta)}{\partial \beta}=X^T(y-p)$$</p><p>$$\frac{\partial ^2\ell (\beta)}{\partial \beta \partial \beta^T}=-X^TWX$$</p><p>其中，$X$为原始自变量矩阵，$y$为类别向量，$p$为预测概率向量，$W$是一个$n\times n$对角矩阵，第$i$个元素取值为$p(x_i,\hat{\beta}^{old})(1-p(x_i,\hat{\beta}^{old}))$.</p><p>联立上述两个式子，可以得出参数的迭代公式：</p><p>$$\begin{aligned}\hat{\beta}^{new} &amp;=\hat{\beta}^{old}+(X^TWX)^{-1}X^T(y-p) \ &amp;=(X^TWX)^{-1}X^TW(X\hat{\beta^{old}}+W^{-1}(y-p)) \ &amp;=(X^TWX)^{-1}X^TWz \end{aligned}$$</p><p>其中，$z=X\hat{\beta^{old}}+W^{-1}(y-p)$.</p><p>实际上，$X^TWX$是一个黑塞矩阵</p><p>$$H(\beta) = \frac{\partial ^2\ell (\beta)}{\partial \beta \partial \beta^T}$$</p><p>即目标函数对$\beta$的二阶偏导，那么，上述迭代公式也可以写作：</p><p>$$\begin{aligned} \hat{\beta}^{new} &amp;=\hat{\beta}^{old} - H(\beta)^{-1}\frac{\partial \ell(\beta)}{\beta} \ &amp;= \hat{\beta}^{old} - H(\beta)^{-1}X^T(y-p) \end{aligned} $$ </p><p>上述是我们熟悉的牛顿迭代公式。</p><p>矩阵相乘的计算不算复杂，但是当数据量上升以后，黑塞矩阵的求逆就非常复杂了，因此衍生出许多拟牛顿算法，本节不讨论优化算法。</p><p>很明显，本例的目标函数就是对数似然函数$\ell(\beta)$，也就是求其最大值。然而，很多同学已经习惯了牛顿法求最小值，因此，为了大家看着方便，下面介绍梯度下降法求解逻辑回归。</p><p>只需要在上述似然函数前面加一个负号，本例就变成了一个梯度下降的问题了。为了形式上好看，还可以在前面对数似然函数求一个均值，即除以样本量。</p><p>假设$J(\beta)$是我们的目标函数，则</p><p>$$<br>\begin{aligned}<br>J(\beta) &amp;= -\frac{1}{n}\ell(\beta) \<br>&amp;=-\frac{1}{n}\sum_{i=1}^ny_iln(p(x_i))+(1-y_i)ln(1-p(x_i))<br>\end{aligned}<br>$$</p><p>此时我们的梯度公式就变成了：</p><p>$$\frac{\partial J(\beta)}{\partial \beta}=-\frac{1}{n}\sum _{i=1}^n(y_i-p_i)x_i=\frac{1}{n}\sum _{i=1}^n(p_i-y_i)x_i$$</p><p>我们的二阶偏导数就变成了</p><p>$$\frac{\partial ^2J(\beta)}{\partial \beta\partial\beta^T} =\frac{1}{n} \sum_{i=1}^nx_ix_i^Tp_i(1-p_i)$$</p><p>那么，此时为了求得我们的回归系数，即求使得$J(\beta)$最小的系数。牛顿迭代公式就变成了：</p><p>$$\hat{\beta}^{new} = \hat{\beta}^{old}-H^{-1}\nabla=\hat{\beta}^{old}-\frac{1}{n}(X^T\cdot diag(p)\cdot diag(1-p) \cdot X)^{-1}\cdot \frac{1}{n}X^T(p-y)$$</p><p>按照上述思路，编程实现逻辑回归求解是比较简单的。</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#迭代函数</span></span><br><span class="line"><span class="hljs-keyword">from</span> numpy <span class="hljs-keyword">import</span> *</span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span><span class="hljs-params">(x)</span>:</span></span><br><span class="line">    <span class="hljs-keyword">return</span> <span class="hljs-number">1.0</span>/(<span class="hljs-number">1.0</span>+exp(-x))</span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">LogitReg</span><span class="hljs-params">(x,y,tol = <span class="hljs-number">0.001</span>,maxiter = <span class="hljs-number">1000</span>)</span>:</span></span><br><span class="line">    samples,features = x.shape  <span class="hljs-comment">#分别表示观测样本数量和特征数量</span></span><br><span class="line">    features += <span class="hljs-number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment">#全部转换为矩阵</span></span><br><span class="line">    xdata = array(ones((samples,features)))</span><br><span class="line">    xdata[:,<span class="hljs-number">0</span>] = <span class="hljs-number">1</span></span><br><span class="line">    xdata[:,<span class="hljs-number">1</span>:] = x</span><br><span class="line">    xdata = mat(xdata)  <span class="hljs-comment">#sample行，features列的输入</span></span><br><span class="line">    </span><br><span class="line">    y = mat(y.reshape(samples,<span class="hljs-number">1</span>))  <span class="hljs-comment">#label，一个长度为samples的向量</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment">#首先初始化beta，令所有的系数为1,生成一个长度为features的列向量</span></span><br><span class="line">    beta = mat(zeros((features,<span class="hljs-number">1</span>)))</span><br><span class="line">    </span><br><span class="line">    iternum = <span class="hljs-number">0</span> <span class="hljs-comment">#迭代计数器</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment">#计算初始损失</span></span><br><span class="line">    </span><br><span class="line">    loss0 = float(<span class="hljs-string">'inf'</span>)</span><br><span class="line">    J = []</span><br><span class="line">    <span class="hljs-keyword">while</span> iternum &lt; maxiter:</span><br><span class="line">        <span class="hljs-keyword">try</span>:</span><br><span class="line">            p = sigmoid(xdata*beta) <span class="hljs-comment">#计算似然概率</span></span><br><span class="line">            nabla = <span class="hljs-number">1.0</span>/samples*xdata.T*(p-y)   <span class="hljs-comment">#计算梯度</span></span><br><span class="line">            H = <span class="hljs-number">1.0</span>/samples*xdata.T*diag(p.getA1())* diag((<span class="hljs-number">1</span>-p).getA1())*xdata  <span class="hljs-comment">#计算黑塞矩阵</span></span><br><span class="line">            </span><br><span class="line">            loss = <span class="hljs-number">1.0</span>/samples*sum(-y.getA1()*log(p.getA1())-(<span class="hljs-number">1</span>-y).getA1()*log((<span class="hljs-number">1</span>-p).getA1())) <span class="hljs-comment">#计算损失</span></span><br><span class="line">            J.append(loss)</span><br><span class="line">            beta =beta -  H.I * nabla  <span class="hljs-comment">#更新参数</span></span><br><span class="line">            iternum += <span class="hljs-number">1</span> <span class="hljs-comment">#迭代器加一</span></span><br><span class="line">            <span class="hljs-keyword">if</span> loss0 - loss &lt; tol:</span><br><span class="line">                <span class="hljs-keyword">break</span></span><br><span class="line">            loss0 = loss</span><br><span class="line">        <span class="hljs-keyword">except</span>:</span><br><span class="line">            H = H + <span class="hljs-number">0.0001</span></span><br><span class="line">            <span class="hljs-comment">#通常当黑塞矩阵奇异的时候，将矩阵加上一个很小的常数。</span></span><br><span class="line">            <span class="hljs-keyword">break</span></span><br><span class="line">        </span><br><span class="line">    <span class="hljs-keyword">return</span> beta,J</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#预测函数</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predictLR</span><span class="hljs-params">(data,beta)</span>:</span></span><br><span class="line">    data = array(data)</span><br><span class="line">    <span class="hljs-keyword">if</span> len(data.shape) == <span class="hljs-number">1</span>:</span><br><span class="line">        length = len(data)</span><br><span class="line">        newdata = tile(<span class="hljs-number">0</span>,length+<span class="hljs-number">1</span>)</span><br><span class="line">        newdata[<span class="hljs-number">0</span>] = <span class="hljs-number">1</span></span><br><span class="line">        newdata[<span class="hljs-number">1</span>:] = data</span><br><span class="line">        newdata = mat(newdata)</span><br><span class="line">        <span class="hljs-keyword">pass</span></span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        shape = data.shape</span><br><span class="line">        newdata = zeros((shape[<span class="hljs-number">0</span>],shape[<span class="hljs-number">1</span>]+<span class="hljs-number">1</span>))</span><br><span class="line">        newdata[:,<span class="hljs-number">0</span>] = <span class="hljs-number">1</span></span><br><span class="line">        newdata[:,<span class="hljs-number">1</span>:] = data</span><br><span class="line">        newdata = mat(newdata)</span><br><span class="line">    <span class="hljs-keyword">return</span> sigmoid(newdata*beta)</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="hljs-string">'df.csv'</span>,header=<span class="hljs-keyword">None</span>)</span><br><span class="line">df = array(df)</span><br><span class="line">df.shape</span><br><span class="line">xdata = df[:,:<span class="hljs-number">3</span>]</span><br><span class="line">ydata = df[:,<span class="hljs-number">3</span>]</span><br><span class="line"></span><br><span class="line">beta,J = LogitReg(xdata,ydata) <span class="hljs-comment">#拟合</span></span><br><span class="line"></span><br><span class="line">testdata = xdata[<span class="hljs-number">1</span>:<span class="hljs-number">10</span>,]</span><br><span class="line"></span><br><span class="line">predictLR(testdata,beta)</span><br><span class="line"></span><br><span class="line">matrix([[ <span class="hljs-number">0.4959212</span> ],</span><br><span class="line">        [ <span class="hljs-number">0.44642627</span>],</span><br><span class="line">        [ <span class="hljs-number">0.47419207</span>],</span><br><span class="line">        [ <span class="hljs-number">0.42209742</span>],</span><br><span class="line">        [ <span class="hljs-number">0.41802565</span>],</span><br><span class="line">        [ <span class="hljs-number">0.51283217</span>],</span><br><span class="line">        [ <span class="hljs-number">0.44833226</span>],</span><br><span class="line">        [ <span class="hljs-number">0.41252982</span>],</span><br><span class="line">        [ <span class="hljs-number">0.47853786</span>]])</span><br></pre></td></tr></table></figure><blockquote><p><a href="http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf" target="_blank" rel="noopener">http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf</a></p><p>[1]周志华.机器学习[M].清华大学出版社,2016.</p><p>[2]李航著.统计学习方法[M].清华大学出版社,2012.</p></blockquote>]]></content>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>囚徒困境</title>
      <link href="/2018/05/02/%E5%9B%9A%E5%BE%92%E5%9B%B0%E5%A2%83/"/>
      <url>/2018/05/02/%E5%9B%9A%E5%BE%92%E5%9B%B0%E5%A2%83/</url>
      <content type="html"><![CDATA[<p>2018年4月27日18时10分许，米脂县第三中学学生放学途中遭犯罪嫌疑人袭击,造成19名学生受伤，其中7人死亡。</p><p>有同学会说，嫌犯只有一个人，却导致了7人死亡的惨剧，如果大家一起上前反抗，肯定能阻止这次犯罪，将伤害降到最低。</p><p>这让我想起了囚徒困境，因为人都是理智的，都会做出对自有利的选择，所以没有人上前反抗，大家只顾着跑，即使有人受了伤害。</p><p>为此我们做出如下假设：</p><ol><li>A和B同时反抗，则歹徒被制服，无人伤亡；</li><li>A上前反抗，B逃跑，则A死亡，B安全；</li><li>A和B同时逃跑，则A和B有可能死亡。</li></ol><a id="more"></a><p>绘制出如下二维表：</p><p><img src="/picture/PrisonerDilemma.png" alt="囚徒困境"></p><p>对于甲，有两种选择，即</p><p>1.上前阻止</p><ul><li>如果乙上前阻止，则甲安全，乙安全</li><li>如果乙逃跑，则甲死亡，乙安全</li></ul><p>2.逃跑</p><ul><li>如果乙上前阻止，则甲安全，乙死亡</li><li>如果乙逃跑，则甲可能死亡，乙可能死亡</li></ul><p>即对于甲来说，如果其上前阻止，那么乙的后果都会比甲要好，即乙都是安全的，那么甲不可能选择上前阻止。因此甲肯定选择逃跑。</p><p>而如果甲选择了逃跑，乙这时只有两种方案：</p><ol><li>上前阻止，则甲安全，乙死亡</li><li>逃跑，则甲可能死亡，乙可能死亡  </li></ol><p>以上两个方案中，对乙最有利的是方案2，即乙肯定也会选择逃跑。</p><p>分析到此结束，不难理解，所有人都只顾着才跑，而没有人上前反抗了。</p>]]></content>
      
      
    </entry>
    
    <entry>
      <title>新浪新闻爬虫</title>
      <link href="/2018/04/27/%E6%96%B0%E6%B5%AA%E7%88%AC%E8%99%AB/"/>
      <url>/2018/04/27/%E6%96%B0%E6%B5%AA%E7%88%AC%E8%99%AB/</url>
      <content type="html"><![CDATA[<h2 id="获取标题列表"><a href="#获取标题列表" class="headerlink" title="获取标题列表"></a>获取标题列表</h2><p>新浪新闻总是不断更新的，因此有可能每次查询的返回新闻数量都是不一样的，所以我们最好不要通过设置固定的条数取爬取。</p><p><img src="/picture/sina_scrapy.png" alt="sina_scrapy"></p><p>从上图可以看出，只要有“下一页”存在，我们就可以从下一页获取链接，从而开始下一页的爬取。</p><a id="more"></a><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> requests</span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="hljs-keyword">import</span> re</span><br><span class="line"><span class="hljs-keyword">import</span> time</span><br><span class="line"><span class="hljs-keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#第一页的URL</span></span><br><span class="line">startURL = <span class="hljs-string">'http://search.sina.com.cn/?c=news&amp;q=%D0%F0%C0%FB%D1%C7+%BF%D5%CF%AE&amp;range=all&amp;time=2018&amp;stime=&amp;etime=&amp;num=20'</span></span><br><span class="line"></span><br><span class="line">url = startURL</span><br><span class="line">allNewsURL = []</span><br><span class="line">errorPages = []</span><br><span class="line">count = <span class="hljs-number">1</span></span><br><span class="line"><span class="hljs-keyword">while</span> url:</span><br><span class="line">    <span class="hljs-keyword">try</span>:</span><br><span class="line">        res = requests.get(url)</span><br><span class="line">        print(<span class="hljs-string">'已经爬取了第'</span>,count,<span class="hljs-string">'页！'</span>)</span><br><span class="line">        print(<span class="hljs-string">'URL:'</span>,url)</span><br><span class="line">        soup = BeautifulSoup(res.text,<span class="hljs-string">'html.parser'</span>)</span><br><span class="line">        <span class="hljs-keyword">for</span> news <span class="hljs-keyword">in</span> soup.findAll(<span class="hljs-string">'h2'</span>):</span><br><span class="line">            allNewsURL.append(news.find(<span class="hljs-string">'a'</span>).get(<span class="hljs-string">'href'</span>))</span><br><span class="line">        pages = soup.find(<span class="hljs-string">'div'</span>,&#123;<span class="hljs-string">'id'</span>:<span class="hljs-string">"_function_code_page"</span>&#125;).findAll(<span class="hljs-string">'a'</span>)</span><br><span class="line">        <span class="hljs-keyword">if</span> pages[<span class="hljs-number">-1</span>].get(<span class="hljs-string">'title'</span>) == <span class="hljs-string">'下一页'</span>:</span><br><span class="line">            url = <span class="hljs-string">'http://search.sina.com.cn'</span> + pages[<span class="hljs-number">-1</span>].get(<span class="hljs-string">'href'</span>)</span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            <span class="hljs-keyword">break</span></span><br><span class="line">        time.sleep(np.random.rand() + <span class="hljs-number">2</span>)</span><br><span class="line">        count += <span class="hljs-number">1</span></span><br><span class="line">    <span class="hljs-keyword">except</span> AttributeError:</span><br><span class="line">        print(<span class="hljs-string">'ip被封！'</span>)</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">with</span> open(<span class="hljs-string">'xuliyakongxi.csv'</span>,<span class="hljs-string">'a'</span>) <span class="hljs-keyword">as</span> f:</span><br><span class="line">    <span class="hljs-keyword">for</span> url <span class="hljs-keyword">in</span> allNewsURL:</span><br><span class="line">        f.write(url+<span class="hljs-string">'\n'</span>)</span><br></pre></td></tr></table></figure><h2 id="获取文章详情"><a href="#获取文章详情" class="headerlink" title="获取文章详情"></a>获取文章详情</h2><p>获取文章详情需要注意以下两点：</p><ul><li>不同的新闻来源具有不同的channel。</li><li>不是所有的新闻都具有关键词、源链接等信息，因此需要进行异常处理。</li></ul><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line">URLs = []</span><br><span class="line"><span class="hljs-keyword">with</span> open(<span class="hljs-string">'xuliyakongxi.csv'</span>,<span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> f:</span><br><span class="line">    <span class="hljs-keyword">for</span> url <span class="hljs-keyword">in</span> f.readlines():</span><br><span class="line">        url = url.strip(<span class="hljs-string">'\n'</span>)</span><br><span class="line">        URLs.append(url)</span><br><span class="line"></span><br><span class="line">allNews = []</span><br><span class="line">count = <span class="hljs-number">0</span></span><br><span class="line">comments = []</span><br><span class="line">notMatch= []</span><br><span class="line"><span class="hljs-keyword">for</span> url <span class="hljs-keyword">in</span> URLs:</span><br><span class="line">    <span class="hljs-keyword">try</span>:</span><br><span class="line">        currentNews = []</span><br><span class="line">        request = requests.get(url,timeout=<span class="hljs-number">4</span>)</span><br><span class="line">        request.encoding = <span class="hljs-string">'utf-8'</span></span><br><span class="line">        soup = BeautifulSoup(request.text,<span class="hljs-string">'html.parser'</span>)</span><br><span class="line"></span><br><span class="line">        newsid = re.search(<span class="hljs-string">'doc-i(.+).shtml'</span>,url).group(<span class="hljs-number">1</span>) <span class="hljs-comment">#新闻ID</span></span><br><span class="line">        </span><br><span class="line">        allChannels = [<span class="hljs-string">'gz'</span>,<span class="hljs-string">'shc'</span>,<span class="hljs-string">'gn'</span>,<span class="hljs-string">'gj'</span>,<span class="hljs-string">'jc'</span>,<span class="hljs-string">'cj'</span>,<span class="hljs-string">'kj'</span>,<span class="hljs-string">'sh'</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment">#不同来源的新闻对应不同的channel，此处将所有的channel进行遍历，并加入了异常处理</span></span><br><span class="line">        <span class="hljs-keyword">for</span> channel <span class="hljs-keyword">in</span> allChannels:</span><br><span class="line">            <span class="hljs-keyword">try</span>:</span><br><span class="line">                comm = <span class="hljs-string">'http://comment5.news.sina.com.cn/page/info?version=1&amp;format=json&amp;channel='</span>+channel+<span class="hljs-string">'&amp;newsid=comos-'</span>+ newsid</span><br><span class="line">                res_comm = requests.get(comm)</span><br><span class="line">                commentsDesc = json.loads(res_comm.text)</span><br><span class="line">                engageNum = commentsDesc[<span class="hljs-string">'result'</span>][<span class="hljs-string">'count'</span>][<span class="hljs-string">'total'</span>] <span class="hljs-comment">#参与</span></span><br><span class="line">                commentsNum = commentsDesc[<span class="hljs-string">'result'</span>][<span class="hljs-string">'count'</span>][<span class="hljs-string">'show'</span>] <span class="hljs-comment">#评论</span></span><br><span class="line">                comments.append(commentsDesc[<span class="hljs-string">'result'</span>][<span class="hljs-string">'count'</span>])</span><br><span class="line">            <span class="hljs-keyword">except</span>:</span><br><span class="line">                <span class="hljs-keyword">if</span> channel == <span class="hljs-string">'sh'</span>:</span><br><span class="line">                    print(<span class="hljs-string">'没有匹配'</span>)</span><br><span class="line">                    engageNum = <span class="hljs-keyword">None</span></span><br><span class="line">                    commentsNum = <span class="hljs-keyword">None</span></span><br><span class="line">                    notMatch.append(comm)</span><br><span class="line">                <span class="hljs-keyword">else</span>:</span><br><span class="line">                    <span class="hljs-keyword">continue</span></span><br><span class="line">            <span class="hljs-keyword">else</span>:</span><br><span class="line">                <span class="hljs-keyword">break</span>;</span><br><span class="line">        </span><br><span class="line">        titleAndKeywords = soup.find(<span class="hljs-string">'title'</span>).text </span><br><span class="line">        tk = titleAndKeywords.split(<span class="hljs-string">'|'</span>)</span><br><span class="line">        title = tk[<span class="hljs-number">0</span>] <span class="hljs-comment">#标题</span></span><br><span class="line">        <span class="hljs-keyword">if</span> len(tk) &gt; <span class="hljs-number">2</span>:</span><br><span class="line">            keywords = tk[<span class="hljs-number">1</span>:(len(tk)<span class="hljs-number">-1</span>)]</span><br><span class="line">            keywords.append(tk[<span class="hljs-number">-1</span>].split(<span class="hljs-string">'_'</span>)[<span class="hljs-number">0</span>]) <span class="hljs-comment">#关键词</span></span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            keywords = tk[<span class="hljs-number">-1</span>].split(<span class="hljs-string">'_'</span>)[<span class="hljs-number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        paras = soup.find(<span class="hljs-string">'div'</span>,&#123;<span class="hljs-string">'class'</span>:<span class="hljs-string">'article'</span>&#125;).text <span class="hljs-comment">#正文，存在脏数据</span></span><br><span class="line">        </span><br><span class="line">        <span class="hljs-keyword">try</span>:</span><br><span class="line">            date = soup.find(<span class="hljs-string">'span'</span>,&#123;<span class="hljs-string">'class'</span>:<span class="hljs-string">'date'</span>&#125;).text <span class="hljs-comment">#日期</span></span><br><span class="line">        <span class="hljs-keyword">except</span>:</span><br><span class="line">            date = <span class="hljs-keyword">None</span></span><br><span class="line">            </span><br><span class="line">        <span class="hljs-keyword">try</span>:</span><br><span class="line">            sourceUrl = soup.find(<span class="hljs-string">'a'</span>,&#123;<span class="hljs-string">'class'</span>:<span class="hljs-string">'source'</span>&#125;).get(<span class="hljs-string">'href'</span>) <span class="hljs-comment">#源链接</span></span><br><span class="line">            source = soup.find(<span class="hljs-string">'a'</span>,&#123;<span class="hljs-string">'class'</span>:<span class="hljs-string">'source'</span>&#125;).text <span class="hljs-comment">#来源</span></span><br><span class="line">        <span class="hljs-keyword">except</span>:</span><br><span class="line">            sourceUrl = <span class="hljs-keyword">None</span></span><br><span class="line">            source = <span class="hljs-keyword">None</span></span><br><span class="line">            </span><br><span class="line">        currentNews.append(url) <span class="hljs-comment">#url</span></span><br><span class="line">        currentNews.append(title)  <span class="hljs-comment">#标题</span></span><br><span class="line">        currentNews.append(keywords) <span class="hljs-comment">#关键词</span></span><br><span class="line">        currentNews.append(paras) <span class="hljs-comment">#正文</span></span><br><span class="line">        currentNews.append(date) <span class="hljs-comment">#日期</span></span><br><span class="line">        currentNews.append(sourceUrl) <span class="hljs-comment">#源链接</span></span><br><span class="line">        currentNews.append(source) <span class="hljs-comment">#来源</span></span><br><span class="line">        currentNews.append(engageNum) <span class="hljs-comment">#参与人数</span></span><br><span class="line">        currentNews.append(commentsNum) <span class="hljs-comment">#评论人数</span></span><br><span class="line"></span><br><span class="line">        allNews.append(currentNews)</span><br><span class="line">        count += <span class="hljs-number">1</span></span><br><span class="line"></span><br><span class="line">        print(<span class="hljs-string">'已经爬取'</span>,str(count),<span class="hljs-string">'个页面.'</span>)</span><br><span class="line">        print(<span class="hljs-string">'comment URL:'</span>,comm)</span><br><span class="line">        time.sleep(<span class="hljs-number">1</span>+abs(np.random.rand()))</span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">if</span> count % <span class="hljs-number">20</span> == <span class="hljs-number">0</span>:</span><br><span class="line">            time.sleep(<span class="hljs-number">5</span>)</span><br><span class="line"><span class="hljs-comment">#         if count &gt; 36:</span></span><br><span class="line"><span class="hljs-comment">#             break;</span></span><br><span class="line">    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:</span><br><span class="line">        print(e)</span><br><span class="line">        <span class="hljs-keyword">continue</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>改进的删一法交叉验证与逐步回归比较</title>
      <link href="/2018/04/26/%E5%B2%AD%E5%9B%9E%E5%BD%92/"/>
      <url>/2018/04/26/%E5%B2%AD%E5%9B%9E%E5%BD%92/</url>
      <content type="html"><![CDATA[<h2 id="定义基本函数"><a href="#定义基本函数" class="headerlink" title="定义基本函数"></a>定义基本函数</h2><p>下面主要定义cholesky分解函数，以及求解线性方程组的两个函数和标准化函数。</p><p>代码分割线</p><hr><a id="more"></a><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="hljs-comment">## mchol函数将对称方阵分解为一个下三角矩阵乘以该矩阵转置的形式,</span></span><br><span class="line"><span class="hljs-comment">#函数返回值为下三角矩阵</span></span><br><span class="line"><span class="hljs-comment">#输入：欲分解的矩阵x</span></span><br><span class="line"><span class="hljs-comment">#输出：cholesky分解所得矩阵L</span></span><br><span class="line">mchol &lt;- <span class="hljs-keyword">function</span>(x)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="hljs-comment">#求矩阵x的行列数,m为行数,n为列数</span></span><br><span class="line">  mn &lt;- dim(x)</span><br><span class="line">  m &lt;- mn[<span class="hljs-number">1</span>]</span><br><span class="line">  n &lt;- mn[<span class="hljs-number">2</span>]</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#检验x是否为方阵</span></span><br><span class="line">  <span class="hljs-keyword">if</span>(m != n) </span><br><span class="line">  &#123;</span><br><span class="line">    <span class="hljs-keyword">return</span> (<span class="hljs-string">"Wrong dimensions of matrix!"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#检验x是否为对称矩阵</span></span><br><span class="line">  <span class="hljs-keyword">if</span>(sum(t(x) != x) &gt; <span class="hljs-number">0</span>) </span><br><span class="line">  &#123;</span><br><span class="line">    <span class="hljs-keyword">return</span> (<span class="hljs-string">"Input matrix is not symmetrical!"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#L为与x行列数相等的零矩阵，用于存放分解所得下三角矩阵</span></span><br><span class="line">  L &lt;- matrix(<span class="hljs-number">0</span>, m, m)</span><br><span class="line"></span><br><span class="line">  <span class="hljs-comment">#循环每进行一次,求解一列矩阵L的元素</span></span><br><span class="line">  <span class="hljs-comment">#矩阵x第i列和第i行之前的元素不再使用，相当于矩阵x减少一个维数，</span></span><br><span class="line">  <span class="hljs-comment">#故下述将循环所至第i列记为当前矩阵x和矩阵L的第一列</span></span><br><span class="line">  <span class="hljs-keyword">for</span>(i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:m)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="hljs-comment">#L的主对角线上第一个元素为x的主对角线上第一个元素开方</span></span><br><span class="line">    L[i,i] &lt;- sqrt(x[i,i])</span><br><span class="line">    <span class="hljs-keyword">if</span>(i &lt; m)</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="hljs-comment">#求当前矩阵L的第一列除第一个元素外的其他元素</span></span><br><span class="line">      L[(i+<span class="hljs-number">1</span>):m,i] &lt;- x[(i+<span class="hljs-number">1</span>):m,i]/L[i,i]</span><br><span class="line">      </span><br><span class="line">      <span class="hljs-comment">#矩阵L第一列（除第一个元素）乘以它的转置得到TLM用于更新矩阵x，效果同TLM%*%TLM</span></span><br><span class="line">      TLV &lt;- L[(i+<span class="hljs-number">1</span>):m,i] <span class="hljs-comment">#记录已求出第一列除第一个元素外剩下元素</span></span><br><span class="line">      TLM &lt;- matrix(TLV, m-i, m-i) <span class="hljs-comment">#TLV按列复制成矩阵</span></span><br><span class="line">      TLM &lt;- sweep(TLM, <span class="hljs-number">2</span>, TLV, <span class="hljs-string">"*"</span>)  </span><br><span class="line">      <span class="hljs-comment">#sweep(x， MARGIN， STATS， FUN=”-“， …) 对矩阵进行运算</span></span><br><span class="line">      <span class="hljs-comment">#MARGIN为1，表示行的方向上进行运算，</span></span><br><span class="line">      <span class="hljs-comment">#为2表示列的方向上运算(是指将参数从列的方向移下去算)</span></span><br><span class="line">      <span class="hljs-comment">#STATS是运算的参数，FUN为运算函数，默认是减法</span></span><br><span class="line">      </span><br><span class="line">      <span class="hljs-comment">#减少一个维数的矩阵x更新为原来对应位置上的元素减去TLM，为下一次循环做准备</span></span><br><span class="line">      x[(i+<span class="hljs-number">1</span>):m,(i+<span class="hljs-number">1</span>):m] &lt;- x[(i+<span class="hljs-number">1</span>):m,(i+<span class="hljs-number">1</span>):m] - TLM</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="hljs-comment">#矩阵的返回值为我们要求的下三角矩阵L</span></span><br><span class="line">  L  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">##mforwardsolve函数求解线性方程租Lx=b，其中L为下三角矩阵</span></span><br><span class="line"><span class="hljs-comment">#输入：下三角矩阵L，向量b</span></span><br><span class="line"><span class="hljs-comment">#输出：线性方程组的解x</span></span><br><span class="line">mforwardsolve &lt;- <span class="hljs-keyword">function</span>(L, b)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="hljs-comment">#求L的行列数,m为L的行数,n为L的列数</span></span><br><span class="line">  mn &lt;- dim(L)</span><br><span class="line">  m &lt;- mn[<span class="hljs-number">1</span>]</span><br><span class="line">  n &lt;- mn[<span class="hljs-number">2</span>]</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#判断L是否为方阵</span></span><br><span class="line">  <span class="hljs-keyword">if</span>(m != n) </span><br><span class="line">  &#123;</span><br><span class="line">    <span class="hljs-keyword">return</span> (<span class="hljs-string">"Wrong dimensions of matrix L!"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#判断L是否为下三角矩阵</span></span><br><span class="line">  <span class="hljs-keyword">for</span> (i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:(m-<span class="hljs-number">1</span>))</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="hljs-keyword">if</span>(sum(L[i,(i+<span class="hljs-number">1</span>):m] != <span class="hljs-number">0</span>) &gt; <span class="hljs-number">0</span>)<span class="hljs-comment">#逐行判断上三角是否全为0元素</span></span><br><span class="line">    &#123;</span><br><span class="line">      <span class="hljs-keyword">return</span> (<span class="hljs-string">"Matrix L must be a lower triangular matrix!"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#判断L的行数与b的长度是否相等</span></span><br><span class="line">  <span class="hljs-keyword">if</span>(m != length(b))</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="hljs-keyword">return</span> (<span class="hljs-string">"Wrong dimensions of matrix L or vector b!"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#0向量记录求解结果</span></span><br><span class="line">  x=rep(<span class="hljs-number">0</span>, m)</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#循环每进行一次,求解一个x中的元素，</span></span><br><span class="line">  <span class="hljs-comment">#看作矩阵L向量x向量b的维数减一</span></span><br><span class="line">  <span class="hljs-comment">#故下述将矩阵L的第i列记为当前矩阵L第一列，</span></span><br><span class="line">  <span class="hljs-comment">#将向量x向量b的第i个元素记为当前向量第一个元素</span></span><br><span class="line">  <span class="hljs-keyword">for</span>(i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:m)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="hljs-comment">#求当前循环中x的第一个元素</span></span><br><span class="line">    x[i] &lt;- b[i] / L[i,i]</span><br><span class="line">    <span class="hljs-comment">#降维后的b向量为原来位置上的元素减去当前矩阵L的第一列的乘积</span></span><br><span class="line">    <span class="hljs-keyword">if</span>(i &lt; m) </span><br><span class="line">    &#123;</span><br><span class="line">      b[(i+<span class="hljs-number">1</span>):m] &lt;- b[(i+<span class="hljs-number">1</span>):m] - x[i]*L[(i+<span class="hljs-number">1</span>):m,i]</span><br><span class="line">    &#125;      </span><br><span class="line">  &#125;</span><br><span class="line">  <span class="hljs-comment">#函数返回的x向量即为线性方程组的解</span></span><br><span class="line">  x  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">##mbacksolve函数求解线性方程租Lx=b，其中L为上三角矩阵</span></span><br><span class="line"><span class="hljs-comment">#输入：上三角矩阵L，向量b</span></span><br><span class="line"><span class="hljs-comment">#输出：线性方程组的解x</span></span><br><span class="line">mbacksolve &lt;- <span class="hljs-keyword">function</span>(L, b)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="hljs-comment">#求L的行列数,m为L的行数,n为L的列数</span></span><br><span class="line">  mn &lt;-dim(L)</span><br><span class="line">  m &lt;- mn[<span class="hljs-number">1</span>]</span><br><span class="line">  n &lt;- mn[<span class="hljs-number">2</span>]</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#判断L是否为方阵</span></span><br><span class="line">  <span class="hljs-keyword">if</span>(m != n)</span><br><span class="line">  &#123;  </span><br><span class="line">    <span class="hljs-keyword">return</span> (<span class="hljs-string">"Wrong dimensions of matrix L!"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#判断L是否为上三角矩阵</span></span><br><span class="line">  <span class="hljs-keyword">for</span> (i <span class="hljs-keyword">in</span> <span class="hljs-number">2</span>:m)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="hljs-keyword">if</span>(sum(L[i,<span class="hljs-number">1</span>:(i-<span class="hljs-number">1</span>)] != <span class="hljs-number">0</span>) &gt; <span class="hljs-number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="hljs-keyword">return</span> (<span class="hljs-string">"Matrix L must be a upper triangular matrix!"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#判断L的行数与b的列数是否相等</span></span><br><span class="line">  <span class="hljs-keyword">if</span>(m != length(b))</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="hljs-keyword">return</span> (<span class="hljs-string">"Wrong dimensions of matrix L or vector b!"</span>)</span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  x &lt;- rep(<span class="hljs-number">0</span>, m)</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#循环每进行一次,求解一个x中的元素，</span></span><br><span class="line">  <span class="hljs-comment">#看作矩阵L向量x向量b的维数减一</span></span><br><span class="line">  <span class="hljs-comment">#故下述将矩阵L的第i列记为当前矩阵L最后一列，</span></span><br><span class="line">  <span class="hljs-comment">#将向量x向量b的第i个元素记为当前向量最后一个元素</span></span><br><span class="line">  <span class="hljs-keyword">for</span>(i <span class="hljs-keyword">in</span> m:<span class="hljs-number">1</span>)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="hljs-comment">#求当前循环中x的最后一个元素</span></span><br><span class="line">    x[i] &lt;- b[i] / L[i,i]</span><br><span class="line">    <span class="hljs-comment">#降维后的向量b为原来位置上的元素减去刚才求出的</span></span><br><span class="line">    <span class="hljs-comment">#x元素与当前上三角矩阵L最后一列（除最后一个元素）的乘积</span></span><br><span class="line">    <span class="hljs-keyword">if</span>(i &gt; <span class="hljs-number">1</span>) </span><br><span class="line">    &#123;</span><br><span class="line">      b[(i-<span class="hljs-number">1</span>):<span class="hljs-number">1</span>] &lt;- b[(i-<span class="hljs-number">1</span>):<span class="hljs-number">1</span>] - x[i]*L[(i-<span class="hljs-number">1</span>):<span class="hljs-number">1</span>,i]</span><br><span class="line">    &#125;      </span><br><span class="line">  &#125;</span><br><span class="line">  <span class="hljs-comment">#函数返回值x向量即为线性方程组的解</span></span><br><span class="line">  x  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">##ridgereg函数用于实现岭回归参数beta的估计，</span></span><br><span class="line"><span class="hljs-comment">#参数x和y分别为回归方程的自变量和因变量,</span></span><br><span class="line"><span class="hljs-comment">#lambda为L2正则项的调节参数</span></span><br><span class="line"><span class="hljs-comment">#此函数求解线性方程租</span></span><br><span class="line"><span class="hljs-comment">#(t(x)%*%x+lambada)%*%beta=t(x)%*%y,</span></span><br><span class="line"><span class="hljs-comment">#将t(x)%*%x+lambada进行cholesky分解为R%*%t(R),</span></span><br><span class="line"><span class="hljs-comment">#forwardsolve求解L%*%d=t(x)%*%y,</span></span><br><span class="line"><span class="hljs-comment">#其中d=t(R)%*%beta,</span></span><br><span class="line"><span class="hljs-comment">#backsolve求解t(R)%*%beta=d,即得参数beta的估计值 </span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#输入：自变量x，因变量y，调节参数lambda</span></span><br><span class="line"><span class="hljs-comment">#输出：回归系数beta的估计值</span></span><br><span class="line">ridgereg &lt;- <span class="hljs-keyword">function</span>(lambda, x, y)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="hljs-comment">#y=data[,m]; x=data[,-m]</span></span><br><span class="line">  <span class="hljs-comment">#n为自变量矩阵行数,即n个样本,p为自变量矩阵列数,即p个参数</span></span><br><span class="line">  np &lt;- dim(x)</span><br><span class="line">  n &lt;- np[<span class="hljs-number">1</span>]</span><br><span class="line">  p &lt;- np[<span class="hljs-number">2</span>]</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#将自变量矩阵增加一列全1元素,以便于截距项的计算</span></span><br><span class="line">  x &lt;- as.matrix(cbind(rep(<span class="hljs-number">1</span>, n),x))</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#利用cholesky分解求取回归方程的参数beta的估计值  </span></span><br><span class="line">  V &lt;- t(x)%*%x + diag(c(<span class="hljs-number">0</span>, rep(lambda, p)))              </span><br><span class="line">  <span class="hljs-comment">#t(x)%*%x+lambda作为线性方程组的系数矩阵V</span></span><br><span class="line">  U &lt;- as.vector(t(x)%*%y)                           </span><br><span class="line">  R &lt;- mchol(V)                                           </span><br><span class="line">  <span class="hljs-comment">#调用mchol函数将系数矩阵V进行cholesky分解,V=R%*%t(R)</span></span><br><span class="line">  M &lt;- mforwardsolve(R, U)                                </span><br><span class="line">  <span class="hljs-comment">#使用前代法求解R%*%M=t(x)%*%y,其中M=t(R)%*%beta</span></span><br><span class="line">  mbacksolve(t(R), M)                                    </span><br><span class="line">  <span class="hljs-comment">#使用回代法求解t(R)%*%beta=M,即可得beta的估计值</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">##pred函数的参数b为参数向量,x为自变量,返回值为因变量的预测值</span></span><br><span class="line"><span class="hljs-comment">#输入：回归系数b向量,数据nx</span></span><br><span class="line"><span class="hljs-comment">#输出：因变量y的预测值</span></span><br><span class="line">pred &lt;- <span class="hljs-keyword">function</span>(b, nx)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="hljs-comment">#nx=prostate[1:2,1:8]</span></span><br><span class="line">  b &lt;- as.vector(b)</span><br><span class="line">  p &lt;- length(b) - <span class="hljs-number">1</span></span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#将数据矩阵nx重新排列，每一行为一个样品，</span></span><br><span class="line">  <span class="hljs-comment">#重排矩阵的原因是下面例子中调用的数据原结构为dataframe</span></span><br><span class="line">  nx &lt;- as.matrix(nx, ncol &lt;- p)</span><br><span class="line">  n &lt;- dim(nx)[<span class="hljs-number">1</span>]</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-comment">#计算预测值</span></span><br><span class="line">  apply(t(nx)*b[<span class="hljs-number">2</span>:(p+<span class="hljs-number">1</span>)], <span class="hljs-number">2</span>, sum) + b[<span class="hljs-number">1</span>]  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">##mridge函数用于实现删去一个样品的岭回归</span></span><br><span class="line">mridge=<span class="hljs-keyword">function</span>(i,lambda,x,y) </span><br><span class="line">&#123;</span><br><span class="line">  ridgereg(lambda,x[-i,],y[-i])</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">##cvridgeregerr函数用交叉验证实现岭回归，</span></span><br><span class="line"><span class="hljs-comment">#参数依次为调节参数lambda,自变量x(数据矩阵),因变量y,返回值为测试均方误差   </span></span><br><span class="line"><span class="hljs-comment">#输入：超参数lambda,自变量x(数据矩阵),因变量y</span></span><br><span class="line"><span class="hljs-comment">#输出：删一交叉验证岭回归测试均方误差</span></span><br><span class="line">cvridgeregerr&lt;-<span class="hljs-keyword">function</span>(lambda,x,y)</span><br><span class="line">&#123;  </span><br><span class="line">  <span class="hljs-comment">#lambda=1</span></span><br><span class="line">  np&lt;-dim(x)</span><br><span class="line">  n&lt;-np[<span class="hljs-number">1</span>]</span><br><span class="line">  p&lt;-np[<span class="hljs-number">2</span>]</span><br><span class="line">  <span class="hljs-comment">#矩阵中的元素作为第一个参数输入mridge，表示去掉的数据编号，</span></span><br><span class="line">  <span class="hljs-comment">#结果第i行为删去第i个样本的岭回归系数估计值</span></span><br><span class="line">  coe&lt;-t(apply(as.matrix(<span class="hljs-number">1</span>:n,ncol=<span class="hljs-number">1</span>),<span class="hljs-number">1</span>,mridge,lambda,x,y))</span><br><span class="line">  <span class="hljs-comment">#coe第i行和数据矩阵第i个样本做点对点相乘，对行求和，计算测试均方误差</span></span><br><span class="line">  mean((apply(coe*cbind(<span class="hljs-number">1</span>,x),<span class="hljs-number">1</span>,sum)-y)^<span class="hljs-number">2</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">##ridgeregerr函数用于计算训练均方误差  </span></span><br><span class="line"><span class="hljs-comment">#输入：岭回归超参数lambda，数据矩阵x，因变量y</span></span><br><span class="line"><span class="hljs-comment">#输出：训练均方误差</span></span><br><span class="line">ridgeregerr=<span class="hljs-keyword">function</span>(lambda,x,y)</span><br><span class="line">&#123;</span><br><span class="line">  mean((pred(ridgereg(lambda,x,y),x)-y)^<span class="hljs-number">2</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#矩阵标准化，即先减去列均值，再除以列标准差</span></span><br><span class="line">mystandard = <span class="hljs-keyword">function</span>(x)</span><br><span class="line">&#123;</span><br><span class="line">  mx = apply(x, <span class="hljs-number">2</span>, mean)</span><br><span class="line">  sdx = apply(x, <span class="hljs-number">2</span>, sd)</span><br><span class="line">  t = sweep(x, <span class="hljs-number">2</span>, mx, <span class="hljs-string">'-'</span>)</span><br><span class="line">  sweep(t, <span class="hljs-number">2</span>, sdx, <span class="hljs-string">'/'</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">##在不同的lambda下,比较训练均方误差和测试均方误差，</span></span><br><span class="line"><span class="hljs-comment">#以选取合适的调节参数lambda</span></span><br><span class="line"><span class="hljs-comment">#----------------选取lambda---------------</span></span><br><span class="line"><span class="hljs-keyword">library</span>(ElemStatLearn)</span><br><span class="line">x &lt;- as.matrix(prostate[ ,<span class="hljs-number">1</span>:<span class="hljs-number">8</span>])</span><br><span class="line">x = mystandard(x)  <span class="hljs-comment">#标准化x</span></span><br><span class="line">y &lt;- as.vector(prostate[ ,<span class="hljs-number">9</span>])</span><br><span class="line">LAM &lt;- seq(<span class="hljs-number">0.001</span>, <span class="hljs-number">10</span>, len=<span class="hljs-number">50</span>)</span><br><span class="line"><span class="hljs-comment">#计算岭回归50个模型的训练均方误差，将结果从list展开成向量</span></span><br><span class="line">err &lt;- unlist(lapply(LAM, ridgeregerr, x, y))</span><br><span class="line"><span class="hljs-comment">#计算岭回归50个模型的测试均方误差，将结果从list展开成向量</span></span><br><span class="line">pe &lt;- unlist(lapply(LAM, cvridgeregerr, x, y))</span><br><span class="line">x &lt;- rep(<span class="hljs-number">1</span>:<span class="hljs-number">50</span>, <span class="hljs-number">2</span>)</span><br><span class="line">plot(pe)</span><br></pre></td></tr></table></figure><p><img src="/picture/output_10_0.png" alt="png"></p><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#取交叉验证中使测试均方误差最小的lambda</span></span><br><span class="line">which.min(pe)  <span class="hljs-comment">#30</span></span><br><span class="line">lam=LAM[which.min(pe)]  <span class="hljs-comment">#5.918776</span></span><br></pre></td></tr></table></figure><p>30</p><p>5.91877551020408</p><h2 id="改进删一法交叉验证"><a href="#改进删一法交叉验证" class="headerlink" title="改进删一法交叉验证"></a>改进删一法交叉验证</h2><p>设$$<br>X = \left [<br>\begin{matrix}<br>x_1 \<br>x_2 \<br>\cdots \<br>x_n<br>\end{matrix}<br>\right ]<br>$$</p><p>则，<br>$$<br>X’ = [x_1’,x_2’,…,x_n’]<br>$$</p><p>那么，<br>$$<br>X’X = x_1’x_1+x_2’x_2+\cdots+x_n’x_n<br>$$</p><p>删一法每次删除一个样本，又上述$x_i$对应于一个样本$i$，因此，删除一个样本后的$X’X$变成了$X’X-x_i’x_i$。</p><p>同理，<br>$$<br>y’X = y’<br>\left [<br>\begin{matrix}<br>x_1 \<br>x_2 \<br>\cdots \<br>x_n<br>\end{matrix}<br>\right ]<br>=y_1x_1+y_2x_2+\cdots+y_nx_n<br>$$</p><p>那么，删除一个样本$i$，则$y’X$变成了$y’X-y_ix_i$</p><p>根据上述思路，可以将删一法交叉验证算法进行改进，避免了每次都要计算矩阵相乘，大大降低了计算压力。</p><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">## 改进的删一法</span></span><br><span class="line"><span class="hljs-comment">##mridge_aug函数用于实现删去一个样品的岭回归</span></span><br><span class="line">mridge_aug=<span class="hljs-keyword">function</span>(i,lambda,x,y,xx,xy) </span><br><span class="line">&#123;</span><br><span class="line">  xx =xx-x[i,]%*%t(x[i,])</span><br><span class="line">  xy =xy-y[i]*x[i,]</span><br><span class="line">  xvar=apply(x,<span class="hljs-number">2</span>,var)  <span class="hljs-comment">#各变量方差</span></span><br><span class="line">  <span class="hljs-comment">#利用cholesky分解求取回归方程的参数beta的估计值  </span></span><br><span class="line">  V &lt;- xx + diag(lambda*xvar)  </span><br><span class="line">  <span class="hljs-comment">#t(x)%*%x+lambda*xvar作为线性方程组的系数矩阵V</span></span><br><span class="line">  R &lt;- mchol(V)   </span><br><span class="line">  <span class="hljs-comment">#调用mchol函数将系数矩阵V进行cholesky分解,V=R%*%t(R)</span></span><br><span class="line">  M &lt;- mforwardsolve(R, xy) </span><br><span class="line">  <span class="hljs-comment">#使用前代法求解R%*%M=t(x)%*%y,其中M=t(R)%*%beta</span></span><br><span class="line">  beta=mbacksolve(t(R), M)   </span><br><span class="line">  <span class="hljs-comment">#使用回代法求解t(R)%*%beta=M,即可得beta的估计值</span></span><br><span class="line">  <span class="hljs-keyword">return</span> (beta)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#验证上述函数功能</span></span><br><span class="line">x &lt;- as.matrix(prostate[ ,<span class="hljs-number">1</span>:<span class="hljs-number">8</span>])</span><br><span class="line">x &lt;- as.matrix(cbind(<span class="hljs-number">1</span>,x))</span><br><span class="line">y &lt;- as.vector(prostate[ ,<span class="hljs-number">9</span>])</span><br><span class="line">xx=t(x)%*%x</span><br><span class="line">xy=t(y)%*%x</span><br><span class="line">mridge_aug(<span class="hljs-number">8</span>,<span class="hljs-number">0</span>,x,y,xx,xy)</span><br></pre></td></tr></table></figure><ol class="list-inline">    <li>0.491888211499562</li>    <li>0.570089057625066</li>    <li>0.601708515193068</li>    <li>-0.0231551000547407</li>    <li>0.112972308293185</li>    <li>0.773102592770376</li>    <li>-0.11433738535188</li>    <li>0.031977556499466</li>    <li>0.0045456886523957</li></ol><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#改进的删一法交叉验证,计算测试均方误差</span></span><br><span class="line">cvridgeregerr_aug &lt;- <span class="hljs-keyword">function</span>(lambda,x,y)</span><br><span class="line">&#123;</span><br><span class="line">  np&lt;-dim(x)</span><br><span class="line">  n&lt;-np[<span class="hljs-number">1</span>]</span><br><span class="line">  <span class="hljs-comment">#列表中的元素作为第一个参数输入mridge，示去掉的数据编号，</span></span><br><span class="line">  <span class="hljs-comment">#结果第i行为删去第i个样本的岭回归系数估计值</span></span><br><span class="line">  coe&lt;-t(apply(as.matrix(<span class="hljs-number">1</span>:n,ncol=<span class="hljs-number">1</span>),<span class="hljs-number">1</span>,mridge_aug,lambda,x,y,xx,xy))</span><br><span class="line">  <span class="hljs-comment">#coe第i行和数据矩阵第i个样本做点对点相乘，对行求和，</span></span><br><span class="line">  <span class="hljs-comment">#计算测试均方误差</span></span><br><span class="line">  mean((apply(coe*x,<span class="hljs-number">1</span>,sum)-y)^<span class="hljs-number">2</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#验证上述函数功能</span></span><br><span class="line">x &lt;- as.matrix(prostate[ ,<span class="hljs-number">1</span>:<span class="hljs-number">8</span>])</span><br><span class="line">x &lt;- mystandard(x)</span><br><span class="line">y &lt;- as.vector(prostate[ ,<span class="hljs-number">9</span>])</span><br><span class="line">y &lt;- (y-mean(y)) / sd(y)</span><br><span class="line">xx=t(x)%*%x</span><br><span class="line">xy=t(y)%*%x</span><br><span class="line">cvridgeregerr_aug(<span class="hljs-number">0.3</span>,x,y)</span><br></pre></td></tr></table></figure><p>0.396699322945445</p><h2 id="利用标准化的X和Y训练模型"><a href="#利用标准化的X和Y训练模型" class="headerlink" title="利用标准化的X和Y训练模型"></a>利用标准化的X和Y训练模型</h2><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">LAM = seq(<span class="hljs-number">0.001</span>,<span class="hljs-number">10</span>,length.out = <span class="hljs-number">50</span>)</span><br><span class="line">x &lt;- as.matrix(prostate[ ,<span class="hljs-number">1</span>:<span class="hljs-number">8</span>])</span><br><span class="line">x &lt;- mystandard(x) <span class="hljs-comment">#y也进行标准化，因此X不用增广为(1,X)</span></span><br><span class="line">y &lt;- as.vector(prostate[ ,<span class="hljs-number">9</span>])</span><br><span class="line">y &lt;- (y-mean(y)) / sd(y) <span class="hljs-comment">#对y进行标准化</span></span><br><span class="line">xx=t(x)%*%x</span><br><span class="line">xy=t(y)%*%x</span><br><span class="line"><span class="hljs-comment">#计算岭回归50个模型的测试均方误差，将结果从list展开成向量</span></span><br><span class="line">pe1 &lt;- unlist(lapply(LAM, cvridgeregerr_aug, x, y))</span><br></pre></td></tr></table></figure><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">plot(pe1) <span class="hljs-comment">#</span></span><br><span class="line">lam1 =  LAM[which.min(pe1)]</span><br></pre></td></tr></table></figure><p><img src="/picture/output_26_0.png" alt="png"></p><h2 id="利用未标准化的X和Y训练模型"><a href="#利用未标准化的X和Y训练模型" class="headerlink" title="利用未标准化的X和Y训练模型"></a>利用未标准化的X和Y训练模型</h2><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x &lt;- as.matrix(prostate[ ,<span class="hljs-number">1</span>:<span class="hljs-number">8</span>])</span><br><span class="line">x &lt;- cbind(<span class="hljs-number">1</span>,x) <span class="hljs-comment">#X为未标准化</span></span><br><span class="line">y &lt;- as.vector(prostate[ ,<span class="hljs-number">9</span>]) <span class="hljs-comment">#y未标准化</span></span><br><span class="line">xx=t(x)%*%x</span><br><span class="line">xy=t(y)%*%x</span><br><span class="line"><span class="hljs-comment">#计算岭回归50个模型的测试均方误差，将结果从list展开成向量</span></span><br><span class="line">pe2 &lt;- unlist(lapply(LAM, cvridgeregerr_aug, x, y))</span><br></pre></td></tr></table></figure><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot(pe2) <span class="hljs-comment">#</span></span><br><span class="line">lam2 =  LAM[which.min(pe2)]</span><br></pre></td></tr></table></figure><p><img src="/picture/output_29_0.png" alt="png"></p><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lam1;lam2</span><br></pre></td></tr></table></figure><p>5.91877551020408</p><p>5.91877551020408</p><p><strong>可以看到，无论是用标准化的X和y还是未标准化的X和y来进行训练，最佳的lambda结果是一样的。</strong></p><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#用本文自定义的函数计算回归系数</span></span><br><span class="line">x &lt;- as.matrix(prostate[ ,<span class="hljs-number">1</span>:<span class="hljs-number">8</span>])</span><br><span class="line">y &lt;- as.vector(prostate[ ,<span class="hljs-number">9</span>]) <span class="hljs-comment">#y未标准化</span></span><br><span class="line">ridgereg(lambda = lam1,x,y)</span><br></pre></td></tr></table></figure><ol class="list-inline">    <li>0.857190872862373</li>    <li>0.54969271971446</li>    <li>0.450878931201676</li>    <li>-0.0172129105085714</li>    <li>0.103056884295319</li>    <li>0.466532216724395</li>    <li>-0.0285916273005211</li>    <li>0.0157990230188132</li>    <li>0.00488299779401389</li></ol><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#用mda内置函数实现岭回归(输出结果中缺少截距项)</span></span><br><span class="line"><span class="hljs-keyword">library</span>(mda)</span><br><span class="line">ridge1 &lt;- gen.ridge(prostate[ ,<span class="hljs-number">1</span>:<span class="hljs-number">8</span>], prostate[ ,<span class="hljs-number">9</span>], </span><br><span class="line">                    drop &lt;- <span class="hljs-literal">FALSE</span>, lambda =lam)  </span><br><span class="line">ridge1$coe</span><br></pre></td></tr></table></figure><pre><code>Loading required package: classLoaded mda 0.4-10</code></pre><table><tbody>    <tr><td> 0.549692720</td></tr>    <tr><td> 0.450878931</td></tr>    <tr><td>-0.017212911</td></tr>    <tr><td> 0.103056884</td></tr>    <tr><td> 0.466532217</td></tr>    <tr><td>-0.028591627</td></tr>    <tr><td> 0.015799023</td></tr>    <tr><td> 0.004882998</td></tr></tbody></table><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">min(pe1);min(pe2)  <span class="hljs-comment">#用标准化的X和Y来进行训练，可将预测误差降低到0.39</span></span><br></pre></td></tr></table></figure><p>0.393552554589219</p><p>0.536324873140668</p><p>对于标准化的岭回归模型，有：<br>$$\frac{y-\bar{y}}{sd_y}=\sum \frac{x_i-\bar{x_i}}{ {sd_x}_i} \beta_i<br>$$</p><p>即：<br>$$y = \bar{y} + sd_y \sum \frac{x_i-\bar{x_i}}{ {sd_x}_i} \beta_i = \sum x_i \frac{\beta_i sd_y}{ {sd_x}_i} + \bar{y} - \sum \frac{\bar{x_i}\beta_i sd_y}{ {sd_x}_i}$$</p><p>令 $$\beta’_1 = \frac{\beta_i sd_y}{ {sd_x}_i}, \beta’_0 = \bar{y} - \sum \frac{\bar{x_i}\beta_i sd_y}{ {sd_x}_i}$$</p><p>当遇到一个新样本时，可以利用上面变换得到的回归系数进行预测。</p><h2 id="逐步回归结果"><a href="#逐步回归结果" class="headerlink" title="逐步回归结果"></a>逐步回归结果</h2><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data &lt;- prostate[,<span class="hljs-number">1</span>:<span class="hljs-number">9</span>]</span><br><span class="line">lm.data &lt;- lm(lpsa~.,data = data)</span><br><span class="line">lm.step &lt;- step(lm.data)</span><br></pre></td></tr></table></figure><pre><code>Start:  AIC=-60.78lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason +     pgg45          Df Sum of Sq    RSS     AIC- gleason  1    0.0491 43.108 -62.668- pgg45    1    0.5102 43.569 -61.636- lcp      1    0.6814 43.740 -61.256&lt;none&gt;                 43.058 -60.779- lbph     1    1.3646 44.423 -59.753- age      1    1.7981 44.857 -58.810- lweight  1    4.6907 47.749 -52.749- svi      1    4.8803 47.939 -52.364- lcavol   1   20.1994 63.258 -25.467Step:  AIC=-62.67lpsa ~ lcavol + lweight + age + lbph + svi + lcp + pgg45          Df Sum of Sq    RSS     AIC- lcp      1    0.6684 43.776 -63.176&lt;none&gt;                 43.108 -62.668- pgg45    1    1.1987 44.306 -62.008- lbph     1    1.3844 44.492 -61.602- age      1    1.7579 44.865 -60.791- lweight  1    4.6429 47.751 -54.746- svi      1    4.8333 47.941 -54.360- lcavol   1   21.3191 64.427 -25.691Step:  AIC=-63.18lpsa ~ lcavol + lweight + age + lbph + svi + pgg45          Df Sum of Sq    RSS     AIC- pgg45    1    0.6607 44.437 -63.723&lt;none&gt;                 43.776 -63.176- lbph     1    1.3329 45.109 -62.266- age      1    1.4878 45.264 -61.934- svi      1    4.1766 47.953 -56.336- lweight  1    4.6553 48.431 -55.373- lcavol   1   22.7555 66.531 -24.572Step:  AIC=-63.72lpsa ~ lcavol + lweight + age + lbph + svi          Df Sum of Sq    RSS     AIC&lt;none&gt;                 44.437 -63.723- age      1    1.1588 45.595 -63.226- lbph     1    1.5087 45.945 -62.484- lweight  1    4.3140 48.751 -56.735- svi      1    5.8509 50.288 -53.724- lcavol   1   25.9427 70.379 -21.119</code></pre><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lm.step$coefficients</span><br></pre></td></tr></table></figure><dl class="dl-horizontal">    <dt>(Intercept)</dt>        <dd>0.494729262182627</dd>    <dt>lcavol</dt>        <dd>0.543997856944351</dd>    <dt>lweight</dt>        <dd>0.58821270309519</dd>    <dt>age</dt>        <dd>-0.016444846497545</dd>    <dt>lbph</dt>        <dd>0.101223333723462</dd>    <dt>svi</dt>        <dd>0.714903976347167</dd></dl><p>根据AIC信息准则，最终的模型为“lpsa ~ lcavol + lweight + age + lbph + svi”，系数如上述结果所示。</p><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lm.err = mean((apply(t(t(cbind(<span class="hljs-number">1</span>,data[,c(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>)]))</span><br><span class="line">              *lm.step$coefficients),<span class="hljs-number">1</span>,sum)-data[,<span class="hljs-number">9</span>])^<span class="hljs-number">2</span>)</span><br><span class="line">lm.err <span class="hljs-comment">#逐步回归的预测误差</span></span><br></pre></td></tr></table></figure><p>0.458110121687733</p><p>可以看到，逐步回归的预测误差为0.458，略高于岭回归。</p>]]></content>
      
      
        <tags>
            
            <tag> 矩阵分解 </tag>
            
            <tag> 岭回归 </tag>
            
            <tag> 逐步回归 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>神经网络和深度学习</title>
      <link href="/2018/04/13/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2018/04/13/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      <content type="html"><![CDATA[<h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><h2 id="前向神经网络"><a href="#前向神经网络" class="headerlink" title="前向神经网络"></a>前向神经网络</h2><h3 id="S型神经元"><a href="#S型神经元" class="headerlink" title="S型神经元"></a>S型神经元</h3><a id="more"></a><h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><h3 id="反向神经网络"><a href="#反向神经网络" class="headerlink" title="反向神经网络"></a>反向神经网络</h3><h2 id="编程实现"><a href="#编程实现" class="headerlink" title="编程实现"></a>编程实现</h2>]]></content>
      
      
        <tags>
            
            <tag> 感知机 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>线性模型</title>
      <link href="/2018/04/04/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
      <url>/2018/04/04/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</url>
      <content type="html"><![CDATA[<a id="more"></a>]]></content>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>降维方法</title>
      <link href="/2018/04/04/%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95/"/>
      <url>/2018/04/04/%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95/</url>
      <content type="html"><![CDATA[<h2 id="线性判别分析-LDA"><a href="#线性判别分析-LDA" class="headerlink" title="线性判别分析(LDA)"></a>线性判别分析(LDA)</h2><h2 id="主成分分析-PCA"><a href="#主成分分析-PCA" class="headerlink" title="主成分分析(PCA)"></a>主成分分析(PCA)</h2><h2 id="奇异值分解-SVD"><a href="#奇异值分解-SVD" class="headerlink" title="奇异值分解(SVD)"></a>奇异值分解(SVD)</h2><h2 id="非负矩阵分解-NMF"><a href="#非负矩阵分解-NMF" class="headerlink" title="非负矩阵分解(NMF)"></a>非负矩阵分解(NMF)</h2><h2 id="tsne分解"><a href="#tsne分解" class="headerlink" title="tsne分解"></a>tsne分解</h2><a id="more"></a>]]></content>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 降维 </tag>
            
            <tag> 矩阵分解 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>集成学习</title>
      <link href="/2018/04/04/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
      <url>/2018/04/04/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</url>
      <content type="html"><![CDATA[<h2 id="个体与集成"><a href="#个体与集成" class="headerlink" title="个体与集成"></a>个体与集成</h2><blockquote><p>Hoeffding不等式：给定m个取值在[0,1]区间的独立随机变量$x_1,x_2,\cdots,x_n$，对任意$\epsilon &gt; 0$有如下等式成立：</p><p>$$P(|\frac{1}{m}\sum _{1=1}^mx_i-\frac{1}{m}E(x_i)|\ge \epsilon) \le 2e^{-2m\epsilon^2}$$</p></blockquote><a id="more"></a><h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p>提升（boosting）方法是一种常用的机器学习方法，应用广泛且有效。在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。</p><h3 id="提升方法的基本思路"><a href="#提升方法的基本思路" class="headerlink" title="提升方法的基本思路"></a>提升方法的基本思路</h3><p>提升方法基于这样一种思想：对于一个复杂的任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好。实际上，就是“三个臭皮匠赛过诸葛亮”的道理。</p><p>历史上，Kearns和Valiant首先提出了“强可学习（strong learnable）”和“弱可学习（weakly learnable）”的概念。指出：在概率近似正确学习的框架中，一个概念（一个类），如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的；一个概念，如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。非常有趣的是Schapire后来证明强可学习与弱可学习是等价的，也就是说，在PAC学习的框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。</p><p>这样一来，问题便成为，在学习中，如果已经发现了“弱学习算法”，那么能否将它提升（boost）为“强学习算法”。大家知道，发现弱学习算法比发现强学习算法容易得多。那么如何具体实施提升，便成为开发提升方法时所要解决的问题。关于提升方法的研究很多，有很多算法被提出。最具有代表性的是AdaBoost算法。</p><p>对于分类问题而言，给定一个训练样本集，求比较粗糙的分类规则（弱分类器）要比求精确的分类规则（强分类器）容易的多。提升方法就是从弱学习算法出发，反复学习，得到一系列弱分类器，然后组合这些弱分类器，构成一个强分类器。大多数的提升方法都是改变训练数据的概率分布（训练数据的权值分布），针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。</p><p>这样，对提升方法来说，有两个问题需要回答：一是在每一轮如何改变数据的权值或概率分布；二是如何将弱分类器组合为一个强分类器。关于第1个问题，AdaBoost的做法是，提高那些被前一轮弱分类器分错的样本权值，而降低那些被正确分类样本的权值。这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注。于是，分类问题被一系列弱分类器“分而治之”。至于第2个问题，即弱分类器的组合，AdaBoost采取加权多数表决的方法。具体地，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。</p><p>AdaBoost的巧妙之处在于它将这些想法自然且有效地实现在一种算法里。</p><h3 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h3><p>现在叙述AdaBoost算法。假设给定一个二分类的训练数据集</p><p>$$T={(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)}$$</p><p>其中，每个样本点由实例与标记组成。实例$x_i\in R^n$,标记$y_i\in {-1,1}$。AdaBoost利用以下算法，从训练数据中学习一系列弱分类器或基本分类器，并将这些弱分类器线性组合为一个强分类器。</p><p>输入：训练数据集$T={(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)}$,其中$x_i\in R^n$,标记$y_i\in {-1,1}$；弱分类算法。</p><p>输出：最终分类器$G(x)$。</p><ol><li>初始化训练数据的权值分布，</li></ol><p>$$D_1=(w_{11},\cdots,w_{1i},\cdots,w_{1N}),w_{1i}=\frac{1}{N},i=1,2,\cdots,N$$</p><ol start="2"><li>对$m=1,2,\cdots,M$  </li></ol><ul><li>使用具有权值分布$D_m$的训练数据集学习，得到基本分类器</li></ul><blockquote><p>$$G_m(x):R^n \rightarrow {-1,1}$$</p></blockquote><ul><li>计算$G_m(x)$在训练数据上的分类误差率</li></ul><blockquote><p>$$e_m=\sum <em>{i=1}^NP(G_m(x)\ne y_i)=\sum _{i=1}^Nw</em>{mi}I(G_m(x)\ne y_i) $$</p></blockquote><ul><li>计算$G_m(x)$的系数</li></ul><blockquote><p>$$\alpha _m = \frac{1}{2} ln \frac{1-e_m}{e_m}$$ </p></blockquote><ul><li>更新训练数据集的权值分布</li></ul><blockquote><p>$$D_{m+1} = (w_{m+1,1},\cdots,w_{m+1,i},\cdots,w_{m+1,N})$$ </p></blockquote><blockquote><p>$$w_{m+1,i} = \frac{w_{mi}}{Z_m}e^{-\alpha _m y_i G_m(x_i)},i=1,2,\cdots,N$$ </p></blockquote><p>这里，$Z_m$是规范化因子</p><p>$$Z_m = \sum _{i=1}^N e^{-\alpha _m y_i G_m(x_i)}$$ </p><p>它使$D_{m+1}$成为一个概率分布。</p><ol start="3"><li>构建基本分类器的线性组合</li></ol><p>$$f(x) = \sum _{m=1}^M \alpha_m G_m(x)$$ </p><p>得到最终的分类器</p><p>$$G(X) = sign(f(x)) = sign(\sum _{m=1}^M \alpha_m G_m(x))$$ </p><p>对AdaBoost算法作如下说明：</p><p>步骤1 假设数据集具有均匀的权值分布，即每个训练样本在基本分类器中作用相同，这一假设保证第1步能够在原始数据集上学习基本分类器$G_1(x)$。</p><p>步骤2 AdaBoost反复学习基本分类器，在每一轮$m=1,2,\cdots,M$中顺次地执行下列操作：</p><p>（a）使用当前分布$D_m$加权的训练数据集，学习基本分类器$G_m(x)$。</p><p>（b）计算基本分类器$G_m(x)$在加权训练数据集上的分类误差率：</p><p>$$e_m = \sum <em>{i=1}^N P(G_m(x_i)\ne y_i) = \sum _{G_m(x_i)\ne y_i} w</em>{mi}$$  </p><p>这里，$w_{mi}$表示第$m$轮中第$i$个实例的权值，$\sum <em>{i=1}^N w</em>{mi}=1$。这表明，$G_m(x)$在加权的训练数据上的分类误差率是被$G_m(x)$误分类样本的权值之和，由此可以看出数据权值分布$D_m$与基本分类器$G_m(x)$的分类误差率的关系。</p><p>（c）计算基本分类器$G_m(x)$的系数$\alpha _m$，$\alpha _m$表示$G_m(x)$在最终分类器中的重要性。由(2)可知，当$e_m\le \frac{1}{2}$时，$\alpha_m \ge 0$，并且$\alpha_m$随着$e_m$的减小而增大，所以分类误差率越小的基本分类器在最终分类器中的所用越大。</p><p>（d）更新训练数据的权值分布为下一轮作准备，式(4)可以写成：</p><p>$$ w_{m+1,i}=\left{\begin{aligned}\frac{w_{mi}}{Z_m}e^{-\alpha_m} &amp;  &amp; G_m(x_i)=y_i \\frac{w_{mi}}{Z_m}e^{\alpha_m} &amp;  &amp; G_m(x_i) \ne y_i \end{aligned}\right.$$</p><p>由此可知，被基本分类器$G_m(x)$误分类样本的权值得以扩大，而被正确分类样本的权值却得以缩小。两相比较，由式(2)知误分类样本的权值被放大$e^{2\alpha_m}=\frac{1-e_m}{e_m}$倍。因此，误分类样本在下一轮学习中起更大的作用。不改变所给的训练数据，而不断改变训练数据的权值分布，使得训练数据在基本分类器的学习中起到不同的作用，这是AdaBoost的一个特点。</p><p>步骤3 线性组合$f(x)$实现$M$个基本分类器的加权表决。系数$\alpha_m$表示了基本分类器$G_m(x)$的重要性，这里，所有$\alpha_m$之和并不为1.$f(x)$的符号决定实例$x$的分类，$f(x)$的绝对值表示分类的确信度。利用基本分类器的线性组合构建最终分类器是AdaBoost的另一个特点。</p><h3 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h3><h3 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h3><p>XGBoost是”极端梯度上升”(Extreme Gradient Boosting)的简称,从技术上说，XGBoost是Extreme Gradient Boosting的缩写。它的流行源于在著名的Kaggle数据科学竞赛上被称为”奥托分类”的挑战。它可以处理多种目标函数，包括回归，分类和排序，是一个较为全面的分类器。<br>由于其他许多分类器，不管是强分类器或是集成分类器，在预测性能上的强大但是相对缓慢的实现，如上一章的Adaboost集成算法，不管是在在运行时间上还是在内存占有上开销都很大。XGBoost成为很多比赛的理想选择。XGBoost包还添加了做交叉验证和发现关键变量的额外功能。在优化模型时，这个算法使用了好几个参数。所以为了提高模型的表现，参数的调整十分必要。本节将讨论这些因素。</p><h4 id="XGBoost优势"><a href="#XGBoost优势" class="headerlink" title="XGBoost优势"></a>XGBoost优势</h4><p>XGBoost算法总结起来大致其有三个优点：高效、准确度、模型的交互性。</p><ul><li>正则化：标准GBDT提升树算法的实现没有像XGBoost这样的正则化步骤。正则化用于控制模型的复杂度，对减少过拟合也是有帮助的。XGBoost也正是以“正则化提升”技术而闻名。</li><li>并行处理：XGBoost可以实现并行处理，相比GBM有了速度的飞跃。不过，需要注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。XGBoost的并行是在特征粒度上的。决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点）。因此XGBoost在R重定义了一个自己数据矩阵类DMatrix。XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复利用索引地使用这个结构，获得每个节点的梯度，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</li><li>高度灵活性：XGBoost允许用户定义自定义优化目标和评价标准，它对模型增加了一个全新的维度，所以我们的处理不会受到任何限制。</li><li>缺失值处理：XGBoost内置处理缺失值的规则。 用户需要提供一个和其它样本不同的值，然后把它作为一个参数传进去，以此来作为缺失值的取值。XGBoost在不同节点遇到缺失值时采用不同的处理方法，并且会学习未来遇到缺失值时的处理方法。</li><li>剪枝：当分裂时遇到一个负损失时，传统GBDT会停止分裂。因此传统GBDT实际上是一个贪心算法。XGBoost会一直分裂到指定的最大深度(max_depth)，然后回过头来剪枝。如果某个节点之后不再有正值，它会去除这个分裂。这种做法的优点，当一个负损失（如-2）后面有个正损失（如+10）的时候，就显现出来了。GBM会在-2处停下来，因为它遇到了一个负值。但是XGBoost会继续分裂，然后发现这两个分裂综合起来会得到+8，因此会保留这两个分裂。</li><li>内置交叉验证：XGBoost允许在每一轮boosting迭代中使用交叉验证。因此，可以方便地获得最优boosting迭代次数。而传统的GBDT使用网格搜索，只能检测有限个值。</li></ul><h4 id="XGBoost算法推导"><a href="#XGBoost算法推导" class="headerlink" title="XGBoost算法推导"></a>XGBoost算法推导</h4><p>XGBoost 在函数空间中用牛顿法进行优化。首先，boosting是一种加法模型。XGBoost同样属于GBDT梯度提升法，模型的基分类器都包含有树，对于给定的数据集D={($x_i​$,$y_i​$)}，XGBoost进行additive learning，学习K棵树，采用以下函数对样本进行预测。</p><p>$$\widehat{y}=\phi(x_i)=\sum_{k=1}^{K}f_{k}(x_i)\qquad f_k\in F$$</p><p>这里F是函数空间，$f(x)$是回归树CART。</p><p>$$F={f(x)=w_{q(x)}}(q:R^m\to T,w\in R)$$</p><p>$q(x)$标识将样本x分到了某个叶子节点上，$w$是叶子节点的分数，（leaf score），所以$w_q(x)$ 表示回归树对样本的预测值。<br>回归树的预测输出是实数分数，可以用于回归，分类，排序等任务中，对于回归问题，可以直接作为目标值，对于分类问题，需要映射成概率，比如采用逻辑函数，然后可以控制阈值，进行两种分类错误的把控。<br>XGBoost对传统的提升树算法Adaboost等的改进，在于在参数空间的目标函数中加入了正则化项，来惩罚模型的复杂程度，进而控制过拟合。和Adaboost一样都是通过最小化损失函数求解最优模型，并加入了阈值，如下公式所示：</p><p>$$L(\phi)=\sum_{i}l(\widehat{y}<em>l,y_i)+\sum</em>{k}\Omega(f_k)$$</p><p>误差函数可以是square loss，logloss等，也可以自己定义损失函数，只要能够求出目标函数的梯度和Hessian矩阵，用户就可以自定义训练模型时的目标函数 这也正是XGBoost的优势之一，可以通过研究目的的不同自己定义损失函数，<br>在公式（3-3）中，相比于原始的GBDT，XGBoost的目标函数多了正则项，是学习出来的模型更加不容易过拟合。衡量树的复杂程度主要与树的深度，内部节点的个数，叶子节点的个数，叶子节点的权重有关，因此XGBoost对这些参量进行了约束。得出了正则项为：</p><p>$$\Omega(f)=\gamma T+\frac{1}{2}\rho|w|^2$$</p><p>正则项对每棵树的复杂程度都应进行惩罚，对每个节点进行了复杂度的惩罚，从另一种角度来说也就进行了自动的剪枝。<br>另外，还可以选择使用线性模型替代树模型，从而得到带$L1+L2$惩罚的线性回归正则项可以是$L1$正则，$L2$正则 。<br>第$t$次迭代后，模型的预测等于前$t-1$次的模型预测加上第t棵树的预测。将目标函数在前$t-1$次的模型$y_i^{t-1}$处进行泰勒展开，并将常数项去掉即得到：</p><p>$$L^{(t)}=\sum_{i=1}^{n}[g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]+\Omega(f_t)$$</p><p>公式中，$g_i=\delta_{\widehat{y}^{(t-1)}}l(y_i,\widehat{y}^{(t-1)})\qquad h_i=\delta^2_{\widehat{y}^{(t-1)}}l(y_i,\widehat{y}^{(t-1)})$<br>把$f_t$, $\Omega(f_t)$写成树结构的形式，得到：</p><p>$$L^{(t)}=\sum_{i=1}^{n}[g_iw_{q(x_i)}+\frac{1}{2}h_iw^2_{q(x_i)}]+\gamma T+\rho\frac{1}{2}\sum_{j=i}^{T}w_j^2$$</p><p>则目标函数可以写成按叶节点累加的形式：</p><p>$$L^{(t)}=\sum_{j=1}^{T}[G_iw_j+\frac{1}{2}(H_j+\rho)w_j^2]+\gamma T$$</p><p>如果确定了树的结构（即$q(x)$确定），为了使目标函数最小，可以令其导数为0，解得每个叶节点的最优预测分数为：</p><p>$$W_j^*=-\frac{G_j}{H_j+\rho}$$</p><p>$$\widehat{L}^*=-\frac{1}{2}\sum_{j=1}^{T}\frac{G_j^2}{H_j+\rho}+\gamma T$$</p><p>公式（3-9）的负部衡量了每个叶子节点对总体损失的的贡献，我们希望损失越小越好，则公式的（3-9）的负部值越大越好。<br>因此，对一个叶子节点进行分裂，分裂前后的增益定义为：</p><p>$$Gain=\frac{G_L^2}{H_L+\rho}+\frac{G_R^2}{H_R+\rho}-\frac{(G_L+G_R)^2}{H_L+H_R+\rho}-\gamma$$</p><p>Gain的值越大，分裂后L减小越多。所以当对一个叶节点分割时，计算所有候选特征值所对应的gain，选取gain最大的进行分割。但由于精确遍历所有可能的分割点是效率很低的，所以，实际上XGBoost采用的是对于每个特征，不是简单地按照样本个数进行分位，而是以二阶导数值作为权重，进行分位点的选择，以此减少计算复杂度。在学习每棵树前，提出候选切分点，这也是XGBoost可以实现并行的原因之一，可以提前切割分位点。<br> 最后，XGBoost算法还借鉴了bagging的bootstrap自助法行抽样，还借鉴了随机森林的列抽样，即特征抽样。这样减少过拟合同时还降低了计算复杂度。</p><h2 id="Bagging-amp-随机森林"><a href="#Bagging-amp-随机森林" class="headerlink" title="Bagging &amp; 随机森林"></a>Bagging &amp; 随机森林</h2>]]></content>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>贝叶斯分类器</title>
      <link href="/2018/04/04/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/"/>
      <url>/2018/04/04/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/</url>
      <content type="html"><![CDATA[<a id="more"></a>]]></content>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>特征选择</title>
      <link href="/2018/04/04/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
      <url>/2018/04/04/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/</url>
      <content type="html"><![CDATA[<a id="more"></a>]]></content>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>支持向量机</title>
      <link href="/2018/04/04/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
      <url>/2018/04/04/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</url>
      <content type="html"><![CDATA[<a id="more"></a>]]></content>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>决策树算法</title>
      <link href="/2018/04/04/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
      <url>/2018/04/04/%E5%86%B3%E7%AD%96%E6%A0%91/</url>
      <content type="html"><![CDATA[<h2 id="几种主要的决策树"><a href="#几种主要的决策树" class="headerlink" title="几种主要的决策树"></a>几种主要的决策树</h2><p>决策树算法的关键是选择最优划分属性，据此人们提出了三种决策树模型。</p><h3 id="ID3决策树"><a href="#ID3决策树" class="headerlink" title="ID3决策树"></a>ID3决策树</h3><p>信息熵(Information entropy)是度量样本集合纯度最常用的一种指标，假定当前样本集合$D$中第$k$类样本所占的比例为$p_k,(k=1,2,\cdots,n)$，则$D$的信息熵定义为：</p><p>$$Ent(D)=-\sum _{k=1}^np_k\ log_2p_k$$</p><p>$Ent(D)$的值越小，则$D$的纯度越高</p><a id="more"></a><blockquote><p>其实我们在高中化学就接触过“熵”的概念，指的是物质的混乱程度。是纯度的对立面。我们说熵越大，混乱程度越高，也就是纯度越低。同理，熵越小，混乱程度越低，即纯度越高。 </p></blockquote><p>假设离散属性$a$有$V$个可能的取值${a^1,a^2,\cdots,a^V}$,若使用$a$来对样本集$D$进行划分，则会产生$V$个分支结点，其中第$v$个分支结点包含了$D$中所有在属性$a$上取值为$a^v$的样本，记为$D^v$。我们可以根据上式计算法$D^v$的信息熵，再考虑到不同的分支结点所包含的样本数不同，给分支结点赋予权重$|D^v|/|D|$，即样本越多的分支结点的影响越大，于是计算出用属性$a$对样本集$D$进行划分所获得的“信息增益”：</p><p>$$Gain(D,a) = Ent(D) - \sum _{v=1}^V \frac{|D^v|}{|D|}Ent(D^v)$$</p><p>一般来说，信息增益越大，意味着使用属性$a$来进行划分所获得的“纯度提升”越大。因此，我们总是选择使得信息增益最大的那个属性来进行划分。</p><p>我们用下面的例子来说明划分的过程。该数据是一份医学数据，根据病人的一些特征，给出佩戴硬质隐形眼镜、软质隐形眼镜和不佩戴隐形眼镜的建议。数据共有5个变量，其中4个自变量，1个因变量。</p><ul><li>age of patient：简称age，患者年龄，(1) young, (2) pre-presbyopic, (3) presbyopic</li><li>spectacle prescription：简称sp，视力情况，(1)近视myope，(2)远视hypermetrope</li><li>astigmatic：是否散光，(1) no, (2) yes</li><li>tear production rate:  简称tpr，眼泪生成率，(1) reduced, (2) normal</li><li>suggestion：1 : hard contact lenses,     2 : soft contact lenses,     3 : should not</li></ul><table><thead><tr><th>id</th><th>age</th><th>sp</th><th>astigmatic</th><th>tpr</th><th>suggestion</th></tr></thead><tbody><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>3</td></tr><tr><td>2</td><td>1</td><td>1</td><td>2</td><td>1</td><td>3</td></tr><tr><td>3</td><td>1</td><td>2</td><td>1</td><td>1</td><td>3</td></tr><tr><td>4</td><td>1</td><td>2</td><td>2</td><td>1</td><td>3</td></tr><tr><td>5</td><td>2</td><td>1</td><td>1</td><td>1</td><td>3</td></tr><tr><td>6</td><td>2</td><td>1</td><td>2</td><td>1</td><td>3</td></tr><tr><td>7</td><td>2</td><td>2</td><td>1</td><td>1</td><td>3</td></tr><tr><td>8</td><td>2</td><td>2</td><td>2</td><td>1</td><td>3</td></tr><tr><td>9</td><td>3</td><td>1</td><td>1</td><td>1</td><td>3</td></tr><tr><td>10</td><td>3</td><td>1</td><td>2</td><td>1</td><td>3</td></tr><tr><td>11</td><td>3</td><td>2</td><td>1</td><td>1</td><td>3</td></tr><tr><td>12</td><td>3</td><td>2</td><td>2</td><td>1</td><td>3</td></tr><tr><td>13</td><td>1</td><td>1</td><td>2</td><td>2</td><td>1</td></tr><tr><td>14</td><td>1</td><td>2</td><td>2</td><td>2</td><td>1</td></tr><tr><td>15</td><td>2</td><td>1</td><td>2</td><td>2</td><td>1</td></tr><tr><td>16</td><td>3</td><td>1</td><td>2</td><td>2</td><td>1</td></tr><tr><td>17</td><td>1</td><td>1</td><td>1</td><td>2</td><td>2</td></tr><tr><td>18</td><td>1</td><td>2</td><td>1</td><td>2</td><td>2</td></tr><tr><td>19</td><td>2</td><td>1</td><td>1</td><td>2</td><td>2</td></tr><tr><td>20</td><td>2</td><td>2</td><td>1</td><td>2</td><td>2</td></tr><tr><td>21</td><td>3</td><td>2</td><td>1</td><td>2</td><td>2</td></tr><tr><td>22</td><td>2</td><td>2</td><td>2</td><td>2</td><td>3</td></tr><tr><td>23</td><td>3</td><td>1</td><td>1</td><td>2</td><td>3</td></tr><tr><td>24</td><td>3</td><td>2</td><td>2</td><td>2</td><td>3</td></tr></tbody></table><p>我们首先计算出总的信息熵：</p><p>$$Ent(D) = \sum_{k=1}^3 -p_klog_2p_k = -(\frac{4}{24}log_2\frac{4}{24} + \frac{5}{24}log_2\frac{5}{24} + \frac{15}{24}log_2\frac{15}{24})=1.326$$</p><p>我们首先选择age作为分支结点，age有三个取值，分布为1,2,3。当age=1时，suggestion取值为2个1,2个2,4个3。则当age=1时，$D^1$的信息熵为：</p><p>$$Ent(D^1) = \sum_{k=1}^3 -p_klog_2p_k=-(\frac{2}{8}log_2\frac{2}{8} + \frac{2}{8}log_2\frac{2}{8} + \frac{4}{8}log_2\frac{4}{8})=1.5$$</p><p>同理，</p><p>$$Ent(D^2)== \sum_{k=1}^3 -p_klog_2p_k=-(\frac{1}{8}log_2\frac{1}{8} + \frac{2}{8}log_2\frac{2}{8} + \frac{5}{8}log_2\frac{5}{8})=1.299$$</p><p>$$Ent(D^2)== \sum_{k=1}^3 -p_klog_2p_k=-(\frac{1}{8}log_2\frac{1}{8} + \frac{1}{8}log_2\frac{1}{8} + \frac{6}{8}log_2\frac{6}{8})=1.061$$</p><p>那么，age的信息增益为：</p><p>$$\begin{aligned}Gain(D,age) = &amp; Ent(D) - \sum _{v=1}^3\frac{|D^v|}{|D|}Ent(D^v) \ =&amp;   1.326 - (\frac{1}{3}\times 1.5 + \frac{1}{3}\times 1.299+ \frac{1}{3}\times 1.061)=0.0393\end{aligned}$$</p><p>类似的，我们可以计算出其他属性的信息增益：</p><p>$$Gain(D,sp)=0.0395$$</p><p>$$Gain(D,astigmatic ) = 0.377$$</p><p>$$Gain(D,tpr) = 0.549$$</p><p>可以看到，$tpr$的信息增益最大，因此把它选为划分属性，下图表示基于$tpr$进行划分的结果。</p><p><img src="/picture/%E5%9B%BE2-1525346184642.png" alt="图2"></p><p>然后，决策树按照同样的规则，对上面两个已经生成的结点继续划分。这个时候，决策树有以下三个停止原则：</p><ul><li>当前结点包含的样本全部属于同一个类别，无需划分。上述左子树就是这种情况；</li><li>当前属性集为空，或是所有样本在所有属性上取值相同，无法划分。</li><li>当前结点包含的样本集合为空，不能划分。</li></ul><p>接下来，由于左子树无法继续划分，因此我们继续划分右子树。此时，右子树相当于根节点，我们计算其他三个属性的信息增益。</p><p>上述计算出的右子树的熵为1.555.</p><p>则有：</p><p>$$Gain(D^2,age)=0.221$$</p><p>$$Gain(D^2,sp)=0.095$$</p><p>$$Gain(D^2,astigmatic) = 0.771$$</p><p>此轮选择的划分属性为$astigmatic$，下图表示继续划分的结果。</p><p><img src="/picture/1525347678542.png" alt="1525347678542"></p><p>截止目前，决策树还没结束，以上两个新的叶子节点包含的样本仍然归属于不同的类型，因此还需要继续进行划分。</p><p>我们仍然沿用上面的符号，根据计算可知，</p><p>$$Ent(D^1) = 0.650$$</p><p>$Ent(D^2) = 0.918$</p><p>目前还剩下age和sp两个属性，计算出相应的信息增益：</p><p>$$Gain(D^1,age) = 0.32$$</p><p>$$Gain(D^1,sp) = 0.191$$</p><p>$$Gain(D^2,age) = 0.251$$</p><p>$$Gain(D^2,sp) = 0.459$$</p><p>因此，对于$D^1$，选择age进行划分，对于$D^2$，选择sp进行划分。划分结果如下。</p><p><img src="/picture/1525348835180.png" alt="1525348835180"></p><p>截止目前，我们的决策树还没有完全将样本划分开来。比如21和23分属于2和3。但是目前每个分支只剩下一个属性可以继续划分，因此我们不需要再计算信息增益，直接划分即可。</p><p><img src="/picture/1525349480833.png" alt="1525349480833"></p><p>上述每个叶子结点包含的样本都归属于同一类别，且所有的属性被用于划分。</p><blockquote><p>上面的决策树是根据所有样本进行划分的，因此无法用来进行预测。如果要用于预测，应该把数据分成训练集和测试集。</p></blockquote><p>ID3决策树就是根据信息增益来选择最优划分属性，然后构建决策树的。该算法简单易懂，十分容易上手。</p><h3 id="C4-5决策树"><a href="#C4-5决策树" class="headerlink" title="C4.5决策树"></a>C4.5决策树</h3><p>如果在上述划分的过程中，把id也作为一个候选属性参与划分，那么可计算出$Gain(D,id)=1.326$，是最大的。但是根据id会划分出24个分支，再出现一个新样本的话，则无法预测该样本属于哪一类，也就是泛化能力较差。</p><p>实际上，信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，C4.5算法使用增益率(gain ratio)来选择最有划分属性。延用上述符号，增益率定义为：</p><p>$$Gain_ratio(D,a) = \frac{Gain(D,a)}{IV(a)}$$</p><p>其中</p><p>$$IV(a) = - \sum _{v=1}^V\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}$$</p><p>称为$a$的固有值。属性$a$的可能取值数目却大，则$IV(a)$的值通常会越大。</p><h3 id="CART决策树"><a href="#CART决策树" class="headerlink" title="CART决策树"></a>CART决策树</h3><p>CART树使用基尼指数(Gini index)来选择划分属性，延用上述符号，数据集$D$的纯度可用基尼值来度量：</p><p>$$Gini(D) = \sum <em>{k=1}^n\sum _{k’ \ne k}p_kp</em>{k’} = 1- \sum _{k=1}^n p_k^2$$</p><p>直观来说，$Gini(D)$反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率。因此，Gini(D)越小，则数据的纯度越高。</p><blockquote><p>假设所有的类别都是一样的，则Gini(D)=0,纯度最高</p></blockquote><p>那么，属性$a$的基尼指数定义为：</p><p>$$Gini_index(D,a)=\sum _{v=1}^V\frac{|D^v|}{|D|}Gini(D^v)$$</p><p>于是，我们在候选属性集合$A$中，选择那个使得划分后基尼指数最小的属性作为划分属性。</p><blockquote><p>可以看出，CART树的构建比前两种树都简单一点。</p></blockquote><h3 id="剪枝处理"><a href="#剪枝处理" class="headerlink" title="剪枝处理"></a>剪枝处理</h3><h4 id="前剪枝"><a href="#前剪枝" class="headerlink" title="前剪枝"></a>前剪枝</h4><h4 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h4>]]></content>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>k-means聚类算法学习笔记</title>
      <link href="/2018/04/04/kmeans/"/>
      <url>/2018/04/04/kmeans/</url>
      <content type="html"><![CDATA[<h2 id="聚类算法介绍"><a href="#聚类算法介绍" class="headerlink" title="聚类算法介绍"></a>聚类算法介绍</h2><h3 id="k-means算法介绍"><a href="#k-means算法介绍" class="headerlink" title="k-means算法介绍"></a>k-means算法介绍</h3><p>k-means聚类是最初来自于信号处理的一种矢量量化方法，现被广泛应用于数据挖掘。k-means聚类的目的是将n个观测值划分为k个类，使每个类中的观测值距离该类的中心（类均值）比距离其他类中心都近。</p><p>k-means聚类的一个最大的问题是计算困难，然而，常用的启发式算法能够很快收敛到局部最优解。这通常与高斯分布的期望最大化算法相似，这两种算法都采用迭代求精的方法。此外，它们都使用聚类中心来对数据进行建模</p><a id="more"></a><h3 id="k-means算法的提出与发展"><a href="#k-means算法的提出与发展" class="headerlink" title="k-means算法的提出与发展"></a>k-means算法的提出与发展</h3><p>詹姆斯·麦奎恩（James MacQueen）1967年第一次使用这个术语“k-means”，虽然这个想法可以追溯到1957年的雨果·斯坦豪斯（Hugo Steinhaus）。<br>标准算法首先由Stuart Lloyd在1957年提出，作为脉冲编码调制技术，尽管直到1982年才发布在贝尔实验室以外。<br>1965年，E. W. Forgy发表了基本相同的方法，这就是为什么它有时被称为Lloyd-Forgy。</p><h3 id="k-means算法的优势适应问题"><a href="#k-means算法的优势适应问题" class="headerlink" title="k-means算法的优势适应问题"></a>k-means算法的优势适应问题</h3><ul><li><p>k-means算法优点</p><p>是解决聚类问题的一种经典算法，简单、快速。<br>对处理大数据集，该算法是相对可伸缩和高效率的。因为它的复杂度是$O(nkt)$, 其中, n 是所有对象的数目, k 是簇的数目, t 是迭代的次数。通常$k\ll n$且$t\ll n$ 。<br>当结果簇是密集的，而簇与簇之间区别明显时, 它的效果较好。</p></li><li><p>k-means算法缺点</p><p>在簇的平均值被定义的情况下才能使用，这对于处理符号属性的数据不适用。<br>必须事先给出k（要生成的簇的数目），而且对初值敏感，对于不同的初始值，可能会导致不同结果。<br>它对于“躁声”和孤立点数据是敏感的，少量的该类数据能够对平均值产生极大的影响。</p></li></ul><h3 id="k-means算法的思想介绍"><a href="#k-means算法的思想介绍" class="headerlink" title="k-means算法的思想介绍"></a>k-means算法的思想介绍</h3><p>(1) 选定某种距离作为数据样本件的相似性度量</p><p>由于k-means聚类算法不适合处理离散型数据，因此在计算个样本距离时，可以根据实际需要选择欧氏距离、曼哈顿距离或者明可夫斯基距离中的一种来作为算法的相似性度量。</p><p>假设给定的数据集$X={x_m|m=1,2,…,total}$,$X$中的样本用d个属性$A_1,A_2,…,A_d$来表示，并且d个描述属性都是连续型数据。数据样本$x_i=(x_{i1},x_{i2},x_{id}),x_j=(x_{j1},x_{j2}…,x_{jd})$，其中，$x_{i1},x_{i2},x_{id}$和$x_{j1},x_{j2}…,x_{jd}$分别是样本$x_i$和$x_j$对应的d个描述属性$A_1,A_2,…,A_d$的具体取值。样本$x_i$和$x_j$之间的相似度通常用他们之间的距离d($x_i,x_j$)来表示，距离越小，样本$x_i$和$x_j$越相似，差异度越小。距离越大，样本$x_i$和$x_j$越不相似，差异越大。<br>欧氏距离公式如下：<br>$$d(x_i,x_j)=\sqrt{\sum_{k=1}^d(x_{ik}-x_{jk})^2}$$<br>曼哈顿距离如下：<br>$$d(x_i,x_j)=\sum_{k=1}^d|x_{ik}-x_{jk}|$$<br>明可夫斯基距离如下：<br>$$d(x_i,x_j)=\sqrt[p]{\sum_{k=1}^d|x_{ik}-x_{jk}|^p}$$<br>当$p=1$时，明氏距离即为曼哈顿距离，当$p=2$时，明氏距离即为欧氏距离，当$p=\infty$时，明氏距离即为切比雪夫距离。</p><p>(2) 选择评价聚类性能的准则函数</p><p>k-means聚类算法使用误差平方和准则函数来评价聚类性能。给定数据集X，其中只包含描述属性，不包含类别属性。假设X包含k个聚类子集$X_1,X2,…,X_k$,各个聚类子集中的样本量分别为$n_1,n_2,…,n_k$，各个聚类子集均值代表点分别为$m_1,m_2,…,m_k$，则误差平方和准则函数公式为：</p><p>$$E=\sum_{i=1}^k \sum_{p\in X_i}(p-m_i)^2$$</p><p>(3) 相似度的计算根据一个簇中对象的平均值来进行</p><ol><li><p>将所有对象随机分配到k个非空的簇中。</p></li><li><p>计算每个簇的平均值，并用该平均值代表相应的簇。</p></li><li><p>根据每个对象与各个簇中心的距离，分配给最近的簇。</p></li><li><p>然后转2，重新计算每个簇的均值。这个过程不断重复知道满足某个准则函数为止。</p></li></ol><h3 id="k-means实现流程：分步骤写"><a href="#k-means实现流程：分步骤写" class="headerlink" title="k-means实现流程：分步骤写"></a>k-means实现流程：分步骤写</h3><p>k-means算法2个核心问题，一是度量记录之间的相关性的计算公式，此处采用欧氏距离。一是更新簇内质心的方法，此处用平均值法，即means。<br>此时的输入数据为簇的数目k和包含n个对象的数据库——通常在软件中用数据框或者矩阵表示。输出k个簇，使平方误差准则最小。<br>下面为实现k-means聚类的Python代码</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># (1)选择初始簇中心。</span></span><br><span class="line"><span class="hljs-comment"># (2)对剩余的每个对象，根据其与各个簇中心的距离，将它赋给最近的簇。</span></span><br><span class="line"><span class="hljs-comment"># (3)计算新的簇中心。</span></span><br><span class="line"><span class="hljs-comment"># (4)重复(2)和(3)，直至准则函数不再明显变小为止。</span></span><br><span class="line"><span class="hljs-keyword">from</span> numpy <span class="hljs-keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#定义加载数据的函数。如果数据以文本形式存储在磁盘内，可以用此函数读取</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">loadDataSet</span><span class="hljs-params">(fileName)</span></span></span><br><span class="line">dataMat = []</span><br><span class="line">fr = open(fileName)</span><br><span class="line"><span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> fr.readlines()</span><br><span class="line">curLine = line.strip().split(<span class="hljs-string">'\t'</span>)</span><br><span class="line">fltLine = list(map(float,curLine))</span><br><span class="line">dataMat.append(list(fltLine))</span><br><span class="line"><span class="hljs-keyword">return</span> dataMat</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#该函数计算两个向量的距离，即欧氏距离</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">distEclud</span><span class="hljs-params">(vecA,vecB)</span></span></span><br><span class="line"><span class="hljs-function"><span class="hljs-title">return</span> <span class="hljs-title">sqrt</span><span class="hljs-params">(sum<span class="hljs-params">(power<span class="hljs-params">(vecA - vecB,<span class="hljs-number">2</span>)</span>)</span>)</span></span></span><br><span class="line"><span class="hljs-function"></span></span><br><span class="line"><span class="hljs-function">#曼哈顿距离</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-title">def</span> <span class="hljs-title">Manhattan</span><span class="hljs-params">(vecA,vecB)</span></span></span><br><span class="line"><span class="hljs-function"><span class="hljs-title">return</span> <span class="hljs-title">sum</span><span class="hljs-params">(abs<span class="hljs-params">(vecA-vecB)</span>)</span></span></span><br><span class="line"><span class="hljs-function"></span></span><br><span class="line"><span class="hljs-function">#明考夫斯基距离</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-title">def</span> <span class="hljs-title">MinKowski</span><span class="hljs-params">(vecA,vecB,p)</span></span></span><br><span class="line"><span class="hljs-function"><span class="hljs-title">return</span> <span class="hljs-title">power</span><span class="hljs-params">(sum<span class="hljs-params">(power<span class="hljs-params">(abs<span class="hljs-params">(vecA-vecB)</span>,p)</span>)</span>,<span class="hljs-number">1</span>/p)</span></span></span><br><span class="line"><span class="hljs-function"></span></span><br><span class="line"><span class="hljs-function">#此处为构造质心，而不是从数据集中随机选择<span class="hljs-title">k</span>个样本点作为质心，</span></span><br><span class="line"><span class="hljs-function">#也是比较合理的方法</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-title">def</span> <span class="hljs-title">randCent</span><span class="hljs-params">(dataSet,k)</span></span></span><br><span class="line">    n = shape(dataSet)[1]  #n为dataSet的列数</span><br><span class="line">    centroids = mat(zeros((k,n)))  <span class="hljs-comment">#构造k行n列的矩阵，就是k个质心的坐标</span></span><br><span class="line">    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(n)  <span class="hljs-comment">#</span></span><br><span class="line">        minJ = min(dataSet[,j])  <span class="hljs-comment">#该列数据的最小值</span></span><br><span class="line">        maxJ = max(dataSet[,j])  <span class="hljs-comment">#该列数据的最大值</span></span><br><span class="line">        rangeJ = float(maxJ-minJ)  <span class="hljs-comment">#全距</span></span><br><span class="line">        <span class="hljs-comment">#随机生成k个数值，介于minJ和maxJ之间，填充J列</span></span><br><span class="line">        centroids[,j] = minJ + rangeJ * random.rand(k,<span class="hljs-number">1</span>)  </span><br><span class="line">      </span><br><span class="line">    <span class="hljs-keyword">return</span> centroids</span><br><span class="line">   </span><br><span class="line"><span class="hljs-comment">#定义kMeans函数 </span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">kMeans</span><span class="hljs-params">(dataSet,k,distMeas=distEclud,createCent=randCent)</span></span></span><br><span class="line">    m = shape(dataSet)[0]  #m为原始数据的行数</span><br><span class="line">    <span class="hljs-comment">#clusterAssment包含两列，一列记录簇索引值，一列存储误差</span></span><br><span class="line">    clusterAssment = mat(zeros((m,<span class="hljs-number">2</span>)))  </span><br><span class="line">    centroids = createCent(dataSet,k)  <span class="hljs-comment">#构造初始的质心</span></span><br><span class="line">    clusterChanged = <span class="hljs-keyword">True</span>  <span class="hljs-comment">#控制变量</span></span><br><span class="line">    <span class="hljs-keyword">while</span> clusterChanged  <span class="hljs-comment">#当控制变量为真时，执行下述循环</span></span><br><span class="line">        clusterChanged = <span class="hljs-keyword">False</span>  </span><br><span class="line">        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(m)   </span><br><span class="line">            minDist = inf  <span class="hljs-comment">#首先令最小值为无穷大 </span></span><br><span class="line">            minIndex = <span class="hljs-number">-1</span>  <span class="hljs-comment">#令最小索引为-1</span></span><br><span class="line">            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(k)  <span class="hljs-comment">#下面是一个嵌套循环</span></span><br><span class="line">             <span class="hljs-comment">#计算点数据点I和簇中心点J的距离</span></span><br><span class="line">                distJI=distMeas(centroids[j,],dataSet[i,])</span><br><span class="line">                <span class="hljs-keyword">if</span> distJI &lt; minDist  </span><br><span class="line">                    minDist = distJI</span><br><span class="line">                    minIndex = j</span><br><span class="line">            <span class="hljs-keyword">if</span> clusterAssment[i,<span class="hljs-number">0</span>] != minIndex</span><br><span class="line">                clusterChanged = <span class="hljs-keyword">True</span></span><br><span class="line">            clusterAssment[i,] = minIndex,minDist**<span class="hljs-number">2</span></span><br><span class="line">        print(centroids)</span><br><span class="line">        <span class="hljs-comment">#更新质心的位置</span></span><br><span class="line">        <span class="hljs-keyword">for</span> cent <span class="hljs-keyword">in</span> range(k)</span><br><span class="line">        <span class="hljs-comment">#mat.A意味着将矩阵转换为数组，即matrix--&gt;array</span></span><br><span class="line">            ptsInClust = dataSet[nonzero(clusterAssment[,<span class="hljs-number">0</span>].A==cent)[<span class="hljs-number">0</span>]]</span><br><span class="line">            centroids[cent,] = mean(ptsInClust,axis=<span class="hljs-number">0</span>)</span><br><span class="line">    <span class="hljs-keyword">return</span> centroids,clusterAssment</span><br></pre></td></tr></table></figure><h3 id="k-means与EM算法"><a href="#k-means与EM算法" class="headerlink" title="k-means与EM算法"></a>k-means与EM算法</h3><p>EM是机器学习十大算法之一，是一种好理解，但是推导过程比较复杂的方法。<br>下面将原英文版的EM算法介绍翻译一遍，在翻译的过程中也加深一点自己的理解。</p><h4 id="Jensen不等式"><a href="#Jensen不等式" class="headerlink" title="Jensen不等式"></a>Jensen不等式</h4><p>假设$f$是一个定义域为实数的函数。回忆一下，如果对于所有的$x \in R,f’’(x) \geq 0$，则$f$是一个凸函数。 如果$f$的输入值是一个向量，则当$x$的海塞矩阵(hessian矩阵$H$)是半正定矩阵时，$f$是凸函数。如果对于所有的$x$，$f’’(x)&gt;0$恒成立，那么我们说$f$是严格凸函数（如果$x$是一个向量，则当$H$是严格半正定矩阵时，写做$H&gt;0$，则$f$是严格凸函数）。<br>设$f$是一个凸函数，并且$X$是一个随机变量，则：<br>$$E[f(X)] \geq f(EX).$$<br>当且仅当$x=$常数时，$E[f(X)]=f(EX)$$.$<br>进一步说，如果$f$是一个严格凸函数，则当P{X=E(X)}=1时，满足$E[f(X)] = f(EX).$</p><p>由于我们在写某一随机变量的期望时，习惯上是不写方括号的，因此在上述式子中，$f(EX)=f(E[X])$<br>为了解释上述理论，可以用图1帮助理解。</p><p><img src="/picture/kmeans_1.PNG" alt="图1"></p><p>如图1所示，实黑线表示凸函数$f$，$X$是一个随机变量，取值为$a$和$b$的概率均为0.5。因此，$a$和$b$的均值$E(X)$在二者之间。</p><p>与此同时，我们在y轴上可以看见$f(x)$,$f(b)$和$f(EX)$的值，并且，$E[f(X)]$$f(a)$和$f(b)$之间。从这个例子可以看出，因为$f$是一个凸函数，所以肯定满足$E[f(X)]\ge f(EX)$。<br>一般来说，很多人会忘记上式的不等号方向，那么，记住这个图，就很易能够想起来上面的公式了。</p><p>注意，如果$f$是一个(严格的)凹函数($f’’(x)\le 0$或者$H\le0$)，Jensen不等式仍然成立，只是方向反过来而已，即$E[f(X)]\le f(EX).$}</p><h4 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h4><p>假设我们有包含m个独立样本的训练集${x^{(1)},x^{(2)},…,x^{(m)}}$,基于该样本和模型$p(x,z)$估计待估参数$\theta$,似然函数如下：</p><p>$$\ell (\theta)=\sum _{i=1}^mlogp(x;\theta)=\sum _{i=1}^mlog\sum _zp(x,z;\theta). $$</p><p>但是，明确找出$\theta$的最大似然估计值恐怕非常困难。因为在这里，$z^{(i)}$是隐含随机变量，通常情况下如果已知$z^{(i)}$，那么求上述最大似然估计值会比较容易。</p><p>在这种情况下，EM算法给出了一种有效的求最大似然估计值的方法。直接求$\ell (\theta)$的最大似然估计值也许会很困难，我们的策略是在$\ell (\theta)$下面构造一个下界(E-步骤)，然后优化这个下界，使其不断逼近求$\ell (\theta)$的最大似然估计值(M-步骤)。</p><p> 对于每一个$i$，令$Q_i$是$z^{(i)}$上关于某一分布的概率，($\sum _zQ_i(z)=1,Q_i(z)\ge 0$).}得到下面的式子.如果z是连续型随机变量，那么$Q_i$就是密度函数，$Q_i$在z上的总和就是$Q_i$在z上的积分：</p><p> $$\sum _i logp(x^{(i)};\theta)=\sum _ilog\sum _{z^{(i)}} p(x^{(i)},z^{(i)};\theta)<br> $$<br> $$<br> =\sum _i log \sum _{z^{(i)}}Q_i(z^{(i)})\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}<br> $$</p><p> $$<br> \ge \sum _i \sum _{z^{(i)}}Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$$</p><p> 最后一步应用了Jensen不等式。特别地，在这里$f(x)=logx$是一个凹函数，因为在实数范围内</p><p> $$f’’(x)=-1/x^2&lt;0$$</p><p> 式子</p><p> $$\sum _{z^{(i)}}Q_i(z^{(i)})[\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}] $$</p><p> 是式子的$[p(x^{(i)},z^{(i)};\theta)/Q_i(z^{(i)})]$关于$z^{(i)}$在$Q_i$给出的分布上的期望。根据Jensen不等式，有：</p><p> $$f(E_{z^{(i)}\sim Q_i}[\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}])\ge E_{z^{(i)}\sim Q_i}[f(\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})})]$$</p><p> 下标$z^{(i)}\sim Q_i$表示期望是对来自于分布$Q_i$的随机变量$z^{(i)}$所求的期望。这个条件使式子(2)到式子(3)得以成立。</p><p> 现在，对于来自于$Q_i$的任意一个集合，式子(3)给出了一个$\ell (\theta)$的一个下界。$Q_i$有很多种可能的选择，我们应该选哪个呢?如果我们当前对参数$\theta$(基于已有的条件)已经有了一些猜测，那么使这个下界与$\theta$的值接近看起来就比较自然了。换句话说，我们可以使上述的不等式在$\theta$的某个特定值的条件下变成等式。(稍后我们就能看到上述是何如使$\ell (\theta)$在EM的迭代下单调递增的。)</p><p> 为了使下界尽可能地靠近$\theta$的某个特定值，我们需要在步骤中加入上面推导的Jensen不等式以得到等式。为了实现上述步骤，我们知道只需要使期望变成常量——固定的随机变量就行了。例如，我们需要满足：</p><p>$$\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}=c$$</p><p> 而这个常数$c$不依赖于$z^{(i)}$。这个很简单就可以实现，只需要使</p><p>$$Q_i(z^{(i)})\propto p(x^{(i)},z^{(i)};\theta) $$</p><p> 实际上，因为我们已知$\sum _zQ_i(z^{(i)})=1$,所以有：</p><p>$$Q_i(z^{(i)})=\frac{p(x^{(i)},z^{(i)};\theta)}{\sum _zp(x^{(i)},z^{(i)};\theta)} $$<br>$$=\frac{p(x^{(i)},z^{(i)};\theta)}{p(x^{(i)};\theta)} $$<br>$$=p(z^{(i)}|x^{(i)};\theta)$$</p><p> 因此，我们简单假设$Q_i$是在给定$x^{(i)}$和参数$\theta$的情况下$z^{(i)}$的后验概率.</p><p> 现在，对于上述的$Q_i$，公式(3)给出了我们想取极大值的似然函数$\ell$的下界。这是E步骤。在算法里的M步骤，我们又对公式(3)取极大值以获得新的$\theta$.重复执行上述步骤，即：</p><p> start循环直到收敛</p><p>(E-step)对于每一个i,令：</p><p>$$Q_i(z^{(i)})=p(z^{(i)}|x^{(i)};\theta) $$</p><p>M-step 令：</p><p>$$\theta=arg\ max_\theta \sum _i \sum _{z^{(i)}}Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})} $$</p><p> 我们怎么知道这个算法最终收敛呢？假设$\theta^{(t)}$和$\theta^{(t+1)}$是EM算法连续迭代两次以后得到的参数。下面证明$\theta^{(t)} \le \theta^{(t+1)}$，即证明EM算法会使log似然概率单调递增。证明上述的关键是对于$Q_i$的选择。在以$\theta^{(t)}$为起始标志的EM算法迭代过程中，我们会选择$Q_i^{(t)}(z^{(i)})=p(z^{(i)}|x^{(i)};\theta^{(t)})$.我们在前面已经看到了，上述选择会满足Jensen不等式，像公式(3)的应用一样，能够得到收敛的等式，即：</p><p>$$\ell (\theta^{(t)})=\sum _i \sum _{z^{(i)}}Q_i^{(t)}(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta^{(t)})}{Q_i^{(t)}(z^{(i)})}$$</p><p> 最大化等式右边得到参数$\theta^{(t+1)}$，因此：</p><p> $$<br> \ell (\theta^{(t+1)})\ge<br>  \sum _i \sum _{z^{(i)}}Q_i^{(t)}(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta^{(t+1)})}{Q_i^{(t)}(z^{(i)})}<br>$$</p><p>$$<br> \ge \sum _i \sum _{z^{(i)}}Q_i^{(t)}(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta^{(t)})}{Q_i^{(t)}(z^{(i)})}<br> =\ell (\theta^{(t)})<br> $$</p><p> 第一个不等式来自于：<br>$$<br> \ell(\theta) \ge \sum _i \sum _{z^{(i)}}Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}<br> $$<br> 上式对于$Q_i$和$\theta$的任意值，尤其是$Q_i=Q_i^{(t)}$,$\theta=\theta^{(t+1)}$时成立。为了得到不等式(5)，我们依据的是$\theta^{(t+1)}$应该等于：<br>$$<br> arg\ max_\theta \ \sum _i \sum _{z^{(i)}}Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}<br> $$<br> 因此根据上式计算的$\theta^{(t+1)}$肯定是大于等于$\theta^{(t)}$的。最后，自然而然就推到了公式(6).</p><p> 因此，EM使得似然函数单调收敛。我们们关于EM算法的描述中，我们说最终似然函数会收敛。尽管我们的结果已经展示了上述结论，但是一个令人信服的收敛性检验将会检查在给定的参数容忍度的情况下，</p><p> $\ell(\theta)$会随着EM算法的迭代而不断增加，如果EM使得$\ell(\theta)$的上升速度慢到一定程度，就意味着收敛。</p><p> 如果我们定义<br>$$<br> J(Q,\theta)=\sum _i \sum _{z^{(i)}}Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}<br> ,$$<br> 在我们前面的推导中已经知道了$\ell (\theta)\ge J(Q,\theta)$。EM算法还可以看作使畸变函数J不断上升的过程。E-step固定Q，最大化J；M-step固定$\theta$最大化J。而Q和$\theta$都是在迭代过程中不断更新的。</p><h4 id="k-means与EM"><a href="#k-means与EM" class="headerlink" title="k-means与EM"></a>k-means与EM</h4><p>在1.8.4节对k-means算法的迭代思想进行了说明，可以看出与EM算法具有很大的相似性，在原理上几乎相同。</p><h3 id="k-means举例及R实现"><a href="#k-means举例及R实现" class="headerlink" title="k-means举例及R实现"></a>k-means举例及R实现</h3><p>本文以R自带数据集iris为例，实现聚类的k-means算法。首先，看一下iris数据集的基本结构，如表1所示。</p><p> | Sepal.Length | Sepal.Width | Petal.Length | Petal.Width | Species<br> -|-|-|-|-|-<br>1 | 5.10 | 3.50 | 1.40 | 0.20 | setosa<br>  2 | 4.90 | 3.00 | 1.40 | 0.20 | setosa<br>  3 | 4.70 | 3.20 | 1.30 | 0.20 | setosa<br>  4 | 4.60 | 3.10 | 1.50 | 0.20 | setosa<br>  5 | 5.00 | 3.60 | 1.40 | 0.20 | setosa<br>  6 | 5.40 | 3.90 | 1.70 | 0.40 | setosa </p><p>该数据集共有5个变量，表1显示的是前6行的数据，完整数据一共有150行。五个变量的中文名称分别是：萼片长度、萼片宽度、花瓣长度、花瓣宽度和品种。以此样本作为聚类的原始数据，属于有标签聚类，可以将聚类结果与真实情况进行对比，以得出聚类效果。<br>下面是实现k-means聚类的R语言代码：</p><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">&gt; iris_test&lt;-iris[,<span class="hljs-number">14</span>]</span><br><span class="line">&gt; iris_test_cl&lt;-kmeans(x = iris_test,centers = <span class="hljs-number">3</span>,iter.max = <span class="hljs-number">20</span>)</span><br><span class="line">&gt; iris_test_cl</span><br><span class="line">K-means clustering with <span class="hljs-number">3</span> clusters of sizes <span class="hljs-number">50</span>, <span class="hljs-number">62</span>, <span class="hljs-number">38</span></span><br><span class="line"></span><br><span class="line">Cluster means</span><br><span class="line">  Sepal.Length Sepal.Width Petal.Length Petal.Width</span><br><span class="line"><span class="hljs-number">1</span>     <span class="hljs-number">5.006000</span>    <span class="hljs-number">3.428000</span>     <span class="hljs-number">1.462000</span>    <span class="hljs-number">0.246000</span></span><br><span class="line"><span class="hljs-number">2</span>     <span class="hljs-number">5.901613</span>    <span class="hljs-number">2.748387</span>     <span class="hljs-number">4.393548</span>    <span class="hljs-number">1.433871</span></span><br><span class="line"><span class="hljs-number">3</span>     <span class="hljs-number">6.850000</span>    <span class="hljs-number">3.073684</span>     <span class="hljs-number">5.742105</span>    <span class="hljs-number">2.071053</span></span><br><span class="line"></span><br><span class="line">Clustering vector</span><br><span class="line">  [<span class="hljs-number">1</span>] <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span></span><br><span class="line"> [<span class="hljs-number">39</span>] <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span></span><br><span class="line"> [<span class="hljs-number">77</span>] <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">3</span> <span class="hljs-number">3</span> <span class="hljs-number">3</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">3</span> <span class="hljs-number">3</span> <span class="hljs-number">3</span> <span class="hljs-number">3</span> <span class="hljs-number">3</span> <span class="hljs-number">2</span></span><br><span class="line">[<span class="hljs-number">115</span>] <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">3</span> <span class="hljs-number">3</span> <span class="hljs-number">3</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">3</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">3</span> <span class="hljs-number">3</span> <span class="hljs-number">3</span> <span class="hljs-number">3</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">3</span> <span class="hljs-number">3</span> <span class="hljs-number">3</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">3</span> <span class="hljs-number">3</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">3</span> <span class="hljs-number">3</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">3</span> <span class="hljs-number">2</span></span><br><span class="line"></span><br><span class="line">Within cluster sum of squares by cluster</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">15.15100</span> <span class="hljs-number">39.82097</span> <span class="hljs-number">23.87947</span></span><br><span class="line"> (between_SS / total_SS =  <span class="hljs-number">88.4</span> %)</span><br><span class="line"></span><br><span class="line">Available components</span><br><span class="line"></span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-string">"cluster"</span>      <span class="hljs-string">"centers"</span>      <span class="hljs-string">"totss"</span>        <span class="hljs-string">"withinss"</span>     <span class="hljs-string">"tot.withinss"</span></span><br><span class="line">[<span class="hljs-number">6</span>] <span class="hljs-string">"betweenss"</span>    <span class="hljs-string">"size"</span>         <span class="hljs-string">"iter"</span>         <span class="hljs-string">"ifault"</span>      </span><br><span class="line">&gt; table(iris_test_cl$cluster,iris$Species)</span><br><span class="line">   </span><br><span class="line">    setosa versicolor virginica</span><br><span class="line">  <span class="hljs-number">1</span>     <span class="hljs-number">50</span>          <span class="hljs-number">0</span>         <span class="hljs-number">0</span></span><br><span class="line">  <span class="hljs-number">2</span>      <span class="hljs-number">0</span>         <span class="hljs-number">48</span>        <span class="hljs-number">14</span></span><br><span class="line">  <span class="hljs-number">3</span>      <span class="hljs-number">0</span>          <span class="hljs-number">2</span>        <span class="hljs-number">36</span></span><br></pre></td></tr></table></figure><p>根据输出结果可以看出，setosa类分类正确率为100%，versicolor类有48个分类正确，有两个错误分到了virginica中，viginica有36个分类分类正确，有14个错误分到了versicolor中。总体来说，聚类的正确率为89.3%</p><h3 id="k-means算法的改进"><a href="#k-means算法的改进" class="headerlink" title="k-means算法的改进"></a>k-means算法的改进</h3><h4 id="k-mode算法"><a href="#k-mode算法" class="headerlink" title="k-mode算法"></a>k-mode算法</h4><p>k-mode算法实现了对离散型数据的快速聚类，保留了k-means算法的效率的同时，将k-means的应用范围扩大到了离散型数据。</p><p>k-mode算法是按照k-means算法的核心内容进行修改，针对分类属性的度量和更新质心的问题而改进。具体如下：</p><p> 度量记录之间的相关性D的计算公式是比较两记录之间，属性相同为0，不同为1，并把所有的值相加。因此D越大，就说明两个记录之间的不相关性越强，也可以理解为距离越大，与欧氏距离代表的意义是一样的。<br> 更新modes，使用每个簇的每个属性出现频率最大的那个属性值作为代表簇的属性值。</p><h4 id="k-prototype算法"><a href="#k-prototype算法" class="headerlink" title="k-prototype算法"></a>k-prototype算法</h4><p>k-prototype算法可以对离散型和数值型两种混合的数据进行聚类，在k-prototype中定义了一个对数值型和离散型属性都计算的相异性度量标准。</p><p>k-prototype是结合k-means和k-mode算法，针对混合属性的，解决两个核心问题如下：</p><p> 度量具有混合属性的方法是，数值型属性采用k-means方法得到P1，分类属性采用k-mode方法得到P2，那么D=P1+a*P2，a是权重，如果觉得分类属性重要，则增加权重的值。当a=0时，只有数值型属性。<br> 更新一个簇的中心的方法是结合k-means和k-mode的方法。</p><h4 id="k-中心点算法"><a href="#k-中心点算法" class="headerlink" title="k-中心点算法"></a>k-中心点算法</h4><p>k-中心点算法是针对k-means算法对于孤立点敏感所提出的改进方法。为了解决上述问题，不采用簇中的平均值作为参照点，可以选用簇中位置最中间的对象，即中心点作为参照点。这样划分方法仍然基于最小化所有对象与其参照点的相异度之和的原则来执行的。<br>对于算法的实现来说，就是在更新质心的时候，不是计算所有点的平均值，而是中位数来代表其中心。然后进行迭代，直到质心不再变化为止。</p><h4 id="Enhanced-k-means"><a href="#Enhanced-k-means" class="headerlink" title="Enhanced k-means"></a>Enhanced k-means</h4><p>下面讨论一下k-means聚类的收敛性。</p><p>任意生成k个初始类中心$\mu_1,\mu_2,…,\mu_k \in R$.</p><p>重复如下步骤直到收敛：</p><p> 对于每一个i，令</p><p>$$<br>c^{(i)}=arg \   {min_j ||x^{(i)}-\mu_j||^2}<br>$$</p><p> 对于每一个j，令<br>$$\mu_j=\frac{\sum _{i=1}^m1{c^{(i)}=j}x^{(i)}}<br>{\sum _{i=1}^m1{c^{(i)}=j}} $$</p><p>那么，k-means算法是否能够保证一定收敛呢？答案是肯定的。<br>定义畸变函数</p><p>$$J(c,\mu)=\sum _{i=1}^n||x^{(i)}-\mu _c^{(i)}||^2$$</p><p>k-means算法的目的就是使J降至最小。在内循环中，固定$\mu$,可以通过调整$c$来使J减小；同样的，固定$c$，调整每个类的质心$\mu$也可以使J减小。当J减到最小时，$\mu$和$c$也同时收敛。</p><p>但是，需要注意的是，J函数是一个非凸函数，所以J不能保证收敛到全局最优，而可以保证收敛到局部最优。通常，k-means达到局部最优的结果差不多是全局最优。但是如果担心陷入了很严重的局部最优，可以多运行几次k-means算法（(给出不同的初始类中心)，选出使J达到最小值的那个模型的$\mu$和$c$。</p><p>为了克服k-means上述缺点，可以在k-mean的结果上对其进行改进(enhanced k-means)。该算法步骤如下：</p><p> 根据k-means算法得出k个类和相应的类中心。</p><p> 计算每一个样本点与所有类中心的欧氏距离。</p><p> 假设$x_i$在第$r$个类中，$n_r$表示第$r$个类包含的样本点数目，$d_{ir}^2$表示$x_i$与第$r$个类的中心点的欧氏距离。如果存在类$s$，使得<br>$$\frac{n_r}{n_r-1}d_{ir}^2&gt;\frac{n_s}{n_s+1}d_{is}^2,$$<br>则将$x_i$移到类$s$中。<br> 如果有多个类满足上述不等式，则将$x_i$移动到使得<br>$\frac{n_s}{n_s+1}d_{ir}^2$最小的那个类中。<br> 重复步骤2$\sim$4，直到没有变化为止。</p><h4 id="二分K-均值算法"><a href="#二分K-均值算法" class="headerlink" title="二分K-均值算法"></a>二分K-均值算法</h4><p>为了克服k-means算法收敛于局部最小值的问题，有人提出了使用另一个称为二分-K均值(bisecting K-means)的算法。该算法首先将所有的点作为一个簇，然后将该簇一分为二。之后选择其中一个簇继续划分，选择哪一个簇进行划分取决于对其划分是否可以最大程度降低SSE的值。上述基于SSE的划分过程不断重复，直到得到用户指定的簇数目为止。</p><p>二分K-均值算法的步骤如下：</p><p> 将所有的点看成一个簇<br> 当簇数目小于k时，对于每一个簇</p><p> 计算总误差<br> 在给定的簇上面进行K-means聚类(k=2)<br> 计算将该簇一分为二后的总误差</p><p> 选择使得误差最小的那个簇进行划分操作。</p><p>另一种方法是选择SSE最大的那个簇进行划分。该算法的R代码如下。</p><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#计算误差平方和SSE，group接受一个数值型矩阵，centroid为其均值向量</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SSE&lt;-<span class="hljs-keyword">function</span>(group,centroid)</span><br><span class="line">&#123;</span><br><span class="line">  rows= nrow(group)</span><br><span class="line">  group = as.matrix(group)</span><br><span class="line">  centroid = matrix(rep(centroid,rows),nrow = rows,byrow = <span class="hljs-literal">TRUE</span>)</span><br><span class="line">  group = group - centroid</span><br><span class="line">  group = group * group</span><br><span class="line">  sums = apply(X = group,MARGIN = <span class="hljs-number">1</span>,FUN = sum)</span><br><span class="line">  SSE = sum(sums)</span><br><span class="line">  <span class="hljs-keyword">return</span>(SSE)</span><br><span class="line">&#125;</span><br><span class="line"><span class="hljs-comment">#定义二分K-means聚类函数，dataSet为需要聚类的数据集，k为指定聚类个数</span></span><br><span class="line">biKmeans&lt;-<span class="hljs-keyword">function</span>(dataSet,k)  </span><br><span class="line">&#123;</span><br><span class="line">  <span class="hljs-keyword">if</span>(!is.matrix(dataSet))</span><br><span class="line">    dataSet = as.matrix(dataSet)</span><br><span class="line">  <span class="hljs-keyword">if</span>(k==<span class="hljs-number">1</span>)</span><br><span class="line">    <span class="hljs-keyword">return</span>(dataSet)</span><br><span class="line">  <span class="hljs-keyword">else</span></span><br><span class="line">  &#123;</span><br><span class="line">    <span class="hljs-comment">#定义一个空矩阵来存放不需要再次处理的已经聚好的类</span></span><br><span class="line">    clusteredDataSet = matrix(nrow = <span class="hljs-number">0</span>,ncol = ncol(dataSet))</span><br><span class="line">    <span class="hljs-comment">#定义一个空向量用来存放上述类的标签</span></span><br><span class="line">    clustered = vector(length = <span class="hljs-number">0</span>)</span><br><span class="line">    <span class="hljs-comment">#计数器clusters</span></span><br><span class="line">    clusters = <span class="hljs-number">1</span></span><br><span class="line">    <span class="hljs-comment">#i用来改变每次聚类的标签</span></span><br><span class="line">    i = <span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">while</span>(clusters &lt; k)</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="hljs-comment">#使用k-means用来二分聚类</span></span><br><span class="line">      cl = kmeans(dataSet,<span class="hljs-number">2</span>)</span><br><span class="line">      <span class="hljs-comment">#类标签</span></span><br><span class="line">      cluster = cl$cluster+i</span><br><span class="line">      <span class="hljs-comment">#计算聚好的两个类的与类中心的离差平方和</span></span><br><span class="line">      SSE1 = SSE(group = dataSet[cluster==(<span class="hljs-number">1</span>+i),],centroid = cl$centers[<span class="hljs-number">1</span>,])</span><br><span class="line">      SSE2 = SSE(group = dataSet[cluster==(<span class="hljs-number">2</span>+i),],centroid = cl$centers[<span class="hljs-number">2</span>,])</span><br><span class="line">      <span class="hljs-comment">#将误差平方和大的拿出来，下次继续二分</span></span><br><span class="line">      <span class="hljs-keyword">if</span>(SSE1 &gt; SSE2)</span><br><span class="line">        index = (cluster == (<span class="hljs-number">1</span>+i))</span><br><span class="line">      <span class="hljs-keyword">else</span></span><br><span class="line">        index = (cluster ==(<span class="hljs-number">2</span>+i))</span><br><span class="line">      <span class="hljs-comment">#将不需要再次聚类的类放在clusterDataSet例</span></span><br><span class="line">      clusteredDataSet = rbind(clusteredDataSet,dataSet[!index,])</span><br><span class="line">      <span class="hljs-comment">#将上述类的标签存放在clustered里</span></span><br><span class="line">      clustered = c(clustered,cluster[!index])</span><br><span class="line">      <span class="hljs-comment">#更新dataSet，下次继续二分</span></span><br><span class="line">      dataSet = dataSet[index,]</span><br><span class="line">      <span class="hljs-comment">#计数器+1</span></span><br><span class="line">      clusters = clusters +<span class="hljs-number">1</span></span><br><span class="line">      <span class="hljs-comment">#i+2，因为每次二分的结果是1和2，在结果的基础上加上2，则类标签可以明确区分</span></span><br><span class="line">      i = i + <span class="hljs-number">2</span></span><br><span class="line">    &#125;<span class="hljs-comment">#end while</span></span><br><span class="line">    <span class="hljs-comment">#得出最终聚类结果</span></span><br><span class="line">    clusteredDataSet = rbind(clusteredDataSet,dataSet)</span><br><span class="line">    <span class="hljs-comment">#类标签</span></span><br><span class="line">    clustered = c(clustered,cluster[index])</span><br><span class="line">    <span class="hljs-comment">#转换为数据框</span></span><br><span class="line">    clusteredDataSet = as.data.frame(clusteredDataSet)</span><br><span class="line">    <span class="hljs-comment">#添加类标签列</span></span><br><span class="line">    clusteredDataSet$cluster = clustered</span><br><span class="line">    <span class="hljs-keyword">return</span>(clusteredDataSet)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">&gt; iris_test_bik&lt;-biKmeans(iris_test,<span class="hljs-number">3</span>)</span><br><span class="line">&gt; dim(iris_test_bik)</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">150</span>   <span class="hljs-number">5</span></span><br><span class="line">&gt; table(iris_test_bik$cluster,iris$Species)</span><br><span class="line">   </span><br><span class="line">    setosa versicolor virginica</span><br><span class="line">  <span class="hljs-number">1</span>     <span class="hljs-number">50</span>          <span class="hljs-number">3</span>         <span class="hljs-number">0</span></span><br><span class="line">  <span class="hljs-number">3</span>      <span class="hljs-number">0</span>          <span class="hljs-number">9</span>        <span class="hljs-number">50</span></span><br><span class="line">  <span class="hljs-number">4</span>      <span class="hljs-number">0</span>         <span class="hljs-number">38</span>         <span class="hljs-number">0</span></span><br></pre></td></tr></table></figure><p>由上述最终的聚类结果显示，聚类正确率约为92%，比单纯使用k-means算法正确率高。</p><h3 id="Enhanced-k-means算法更加稳健的原因"><a href="#Enhanced-k-means算法更加稳健的原因" class="headerlink" title="Enhanced k-means算法更加稳健的原因"></a>Enhanced k-means算法更加稳健的原因</h3><p>k-means算法具有明显的两个缺点，即容易陷入局部最优和出现“超级类”。局部最优比较好理解，“超级类”是指聚类结果中有一个包含许多样本点的大类，其他的类包含的样本点都较少。enhanced k-means同时考虑了样本点与聚类中心的距离和类中样本点的个数，保留了k-means聚类算法的优点，同时又克服了出现”超级类”的问题。</p><h3 id="Enhanced-k-means举例及R实现"><a href="#Enhanced-k-means举例及R实现" class="headerlink" title="Enhanced k-means举例及R实现"></a>Enhanced k-means举例及R实现</h3><p>k-中心点算法可以使用R中cluster包中的pam函数(pam的全称为Partitioning Around Medoids，即围绕中心点分割)，同样以iris数据集为例，说明pam函数的使用。</p><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="hljs-keyword">library</span>(cluster)</span><br><span class="line">&gt; pam.cl&lt;-pam(iris_test,k = <span class="hljs-number">3</span>)</span><br><span class="line">&gt; table(pam.cl$clustering,iris$Species)</span><br><span class="line">   </span><br><span class="line">    setosa versicolor virginica</span><br><span class="line">  <span class="hljs-number">1</span>     <span class="hljs-number">50</span>          <span class="hljs-number">0</span>         <span class="hljs-number">0</span></span><br><span class="line">  <span class="hljs-number">2</span>      <span class="hljs-number">0</span>         <span class="hljs-number">48</span>        <span class="hljs-number">14</span></span><br><span class="line">  <span class="hljs-number">3</span>      <span class="hljs-number">0</span>          <span class="hljs-number">2</span>        <span class="hljs-number">36</span></span><br><span class="line">&gt; par(mfrow=c(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>))</span><br><span class="line">&gt; plot(pam.cl)</span><br></pre></td></tr></table></figure><p><img src="/picture/kmeans_2.PNG" alt="图2"></p><p>图2将原始数据进行了降维，提取了两个主成分，并且这两个主成分能够解释原始数据方差的95.81%，说明降维效果很好。将这两个成分绘制成散点图，就得到了图2.结合table函数的输出结果和图2可知，setosa类与其他两个类的界限比较明显，而其他两个类之间具有交叉。</p><p><img src="/picture/kmeans_3.PNG" alt="图3"></p><h2 id="聚类指标评价"><a href="#聚类指标评价" class="headerlink" title="聚类指标评价"></a>聚类指标评价</h2><h3 id="多种聚类结果的比较"><a href="#多种聚类结果的比较" class="headerlink" title="多种聚类结果的比较"></a>多种聚类结果的比较</h3><p>聚类性能的度量大致有两类，一类是将聚类结果与某个“参考模型”进行比较，称为“外部指标”；另一类是直接考察聚类结果而不利用任何参考模型，称为“内部指标”。</p><h4 id="外部指标"><a href="#外部指标" class="headerlink" title="外部指标"></a>外部指标</h4><p>对数据集$D={x_1,x_2,…,x_m}$,假定通过聚类给出的簇划分为</p><p>$$C={C_1,C_2,…,C_k},$$</p><p>参考模型给出的簇划分为</p><p>$$C^+={C_1^+,C_2^+,…,C_s^+}$$</p><p>相应地，令 $\lambda$ 与 $\lambda^+$ 分别表示与 $C$ 和 $C^+$ 对应的簇标记向量，我们将样本两两配对考虑，定义</p><p>$$a=|SS|,SS={(x_i,x_j)|\lambda _i=\lambda _j,\lambda _i^+=\lambda _j^+,i&lt;j }$$<br>$$b=|SD|,SD={(x_i,x_j)|\lambda _i =\lambda _j,\lambda _i^+ \ne \lambda _j^+,i&lt;j }$$<br>$$c=|DS|,DS={(x_i,x_j)|\lambda _i \ne \lambda _j,\lambda _i^+=\lambda _j^+,i&lt;j }$$<br>$$d=|DD|,DD={(x_i,x_j)|\lambda _i \ne \lambda _j,\lambda _i^+ \ne \lambda _j^+,i&lt;j }$$</p><p>其中集合$SS$包含了在$C$中隶属于相同簇且在$C^+$中也隶属于相同簇的样本对，集合$SD$包含了在$C$中隶属于相同簇但在$C^+$中隶属于不同簇的样本对，……由于每个样本对 $(x_i,x_j)(i&lt;j)$ 仅能出现在一个集合中，因此有 $a+b+c+d=m(m-1)/2$成立。</p><p>基于以上给出的信息，可以导出下面常用的聚类性能度量外部指标。</p><p> Jaccard系数(Jaccard Coefficient,简称JC)<br>$$JC=\frac{a}{a+b+c}.$$<br> FM指数(Fowlkws and Mallows Index,简称FMI)</p><p>$$FMI=\sqrt{\frac{a}{a+b}\times \frac{a}{a+c}}.$$</p><p> Rand指数(Rand Index,简称RI)</p><p>$$RI=\frac{a+d}{m(m-1)/2}=\frac{a+d}{C_m^2}.$$</p><p>显然，上述性能度量的结果均在[0,1]之间，值越大越好。}</p><h4 id="内部指标"><a href="#内部指标" class="headerlink" title="内部指标"></a>内部指标</h4><p>考虑聚类结果的划分$C={C_1,C_2,…,C_k}$,定义：<br>$$avg(C)=\frac{2}{|C|(|C|-1)} \sum <em>{1\le i&lt;j \le |C|}dist(x_i,x_j),$$<br>$$diam(C)=max</em>{1\le i&lt;j \le |C|}dist(x_i,x_j),$$<br>$$d_{min}(C_i,C_j)=min_{x_i\in C_i,x_i\in C_j}dist(x_i,x_j),$$<br>$$d_{cen}(C_i,C_j)=dist(\mu _i,\mu _j),$$</p><p>其中，$dist(·,·)$ 用于计算两个样本之间的距离，$\mu$代表$C$的中心点$\mu =\frac{1}{|C|}\sum <em>{1\le i \le |C|}x</em>{i}.$显然，$avg(C)$对应于$C$内样本间的平均距离，$diam(C)$对应于簇$C$内样本间的最远距离，$d_{min}(C_i,C_j)$对应于簇$C_i$与簇$C_j$最近样本间的距离，$d_{cen}(C_i,C_j)$对应于簇$C_i$与簇$C_j$中心点的距离。</p><p>基于上式可以导出下面这些常用的聚类性能度量内部指标。</p><p> DB指数(Davies-Bouldin Index,简称DBI)<br>$$DBI=\frac{1}{k}\sum <em>{i=1}^k max</em>{j\ne i}(\frac{avg(C_i)+avg(C_j)}{d_{cen}(\mu <em>i,\mu _j)}).$$<br> Dunn指数(Dunn Index，简称DI)<br>$$DI=min</em>{1\le i \le k}{min_{j\ne i}(\frac{d_{min}(C_i,C_j)}{max_{1\le l \le k}\ diam(C_l)})}$$</p><p>显然，DBI的值越小越好，而DI则相反，值越大越好。 </p><h4 id="共表型相关系数"><a href="#共表型相关系数" class="headerlink" title="共表型相关系数"></a>共表型相关系数</h4><p>共表型相关系数(Cophenetic Correlation Coefficient，简称CPCC)</p><p>内容参考：http//people.revoledu.com/kardi/tutorial/Clustering/index.html.</p><p>另外，共表相关系数也属于内部指标，这里单独拿出来是因为其计算比较复杂，且仅仅用在层次聚类方法上。共表型相关系数是用于衡量层次聚类聚类性能的指标，为了清楚说明共表相关系数的计算，下面先从层次聚类说起。<br>层次聚类的标准输出结果是一个树状图，树状图的横坐标显示的是样本点，纵坐标是树的高度。树状图可以看作是层次聚类的可视化结果。</p><p>使用树状图，我们可以很容易地找出决定聚类个数的分割点。比如说，在图4中，如果将分割高度设置为2，则将6个样本聚成2类(一类包含了A、B样本，一类包含了C、E、D、F)；如果将分割高度设置为1.2，则将6个样本聚成3类(一类包含A、B，一类包含C，一类包含E、D、F)。</p><p><img src="/picture/kmeans_4.PNG" alt="图4"></p><p>层次聚类算法一共有两种实现方法。第一种方法是$agglomerative \ approach$，先从底部将所有的样本点单独看作一类，然后将两个最近的样本点聚在一起，形成一个新的类。接着计算所有类之间的聚类，将两个最近的类聚在一起。上述步骤重复下去，直到把所有的点聚为一类。该方法也称为自底向上的层次聚类方法。第二种方法是$divisive\ approach$，即先从顶部开始，将所有的样本点看作一个大类。然后将该大类分成两个小类，直到所有的样本点自成一类结束。上述层次聚类的一个可行的方法是先将上述所有样本生成一个最小生成树(例如使用Kruskal算法)，然后通过最大距离将该树分割。这种方法也被称为自顶向下的层次聚类方法。</p><p>这里使用第一种方法演示层次聚类方法，步骤如下：</p><p> 计算各个变量之间的聚类，生成距离矩阵</p><p> 将每个样本点视为一个类</p><p> 迭代，直到类的个数为1</p><p> 将最近的两个类合并</p><p> 更新距离矩阵</p><p>为了以图示方法演示层次聚类算法，本文引入了一个小例子。假设我们有6个对象，每个对象有两个特征，具体数据如表2所示。我们可以绘制X1与X2的散点图观察对象之间的关系，如图5所示。可以看到，A与B的距离较近，D、F、E的距离较近，C有点离群。可以通过计算样本间的距离矩阵来反应样本之间的关系。一般来说，计算距离会采用上面所说的欧氏距离。注意，距离矩阵的输出结果肯定是一个对称矩阵。这里的输出结果如表3所示。</p><p><img src="/picture/kmeans_5.PNG" alt="图5"></p><hr><table><thead><tr><th>-</th><th>X1</th><th>X2</th></tr></thead><tbody><tr><td>A</td><td>1.00</td><td>1.00</td></tr><tr><td>B</td><td>1.50</td><td>1.50</td></tr><tr><td>C</td><td>5.00</td><td>5.00</td></tr><tr><td>D</td><td>3.00</td><td>4.00</td></tr><tr><td>E</td><td>4.00</td><td>4.00</td></tr><tr><td>F</td><td>3.00</td><td>3.50</td></tr></tbody></table><hr><table><thead><tr><th>-</th><th>A</th><th>B</th><th>C</th><th>D</th><th>E</th><th>F</th></tr></thead><tbody><tr><td>A</td><td>0.00</td><td>0.71</td><td>5.66</td><td>3.61</td><td>4.24</td><td>3.20</td></tr><tr><td>B</td><td>0.71</td><td>0.00</td><td>4.95</td><td>2.92</td><td>3.54</td><td>2.50</td></tr><tr><td>C</td><td>5.66</td><td>4.95</td><td>0.00</td><td>2.24</td><td>1.41</td><td>2.50</td></tr><tr><td>D</td><td>3.61</td><td>2.92</td><td>2.24</td><td>0.00</td><td>1.00</td><td>0.50</td></tr><tr><td>E</td><td>4.24</td><td>3.54</td><td>1.41</td><td>1.00</td><td>0.00</td><td>1.12</td></tr><tr><td>F</td><td>3.20</td><td>2.50</td><td>2.50</td><td>0.50</td><td>1.12</td><td>0.00</td></tr></tbody></table><p>因为距离矩阵是一个对称矩阵，所以一般只采用上三角或者下三角数据，本文采用的是下三角矩阵。对角线上的元素全为0，因为计算的是样本点与自己的距离。总的来说，如果有m个样本点，那么下三角矩阵包含有$\frac{1}{2}m(m-1)$的数据。在我们的例子里，一共有$1/2\times6\times(6-1)=15$个元素。可以看出，在距离矩阵中最小的元素是0.5，即D与F之间的距离。如果样本点包含的还有分类数据，则可以采用其他方法计算样本点之间的距离。</p><p>如何将对象聚在一个类中是层次聚类的关键。给出一个距离矩阵，对象之间的距离可以通过计算类之间的距离的一些标准来确定。最基础和最常用的几个方法如下。</p><p>Single Linkageminimun distance criteron</p><p>$$d_{A\rightarrow B}=min_{\forall i \in A,\forall j \in B}(d_{ij}) $$</p><p>Complete Linkagemaximun distance criteron</p><p>$$d_{A\rightarrow B}=max_{\forall i \in A,\forall j \in B}(d_{ij}) $$</p><p>Average Groupaverage distance criteron</p><p>$$d_{A\rightarrow B}=average_{\forall i \in A,\forall j \in B}(d_{ij}) $$</p><p>Centroid distance criteron</p><p>$$d_{A\rightarrow B}=||c_A-c_B||=\frac{1}{n_in_j}\sum_ {\forall i \in A,\forall j \in B}(d_{ij}) $$</p><p>已存在的包含有$n_k$个样本的类k与新聚成的类$(r,s)$之间的距离公式如下：</p><p>$$<br>d_{k \rightarrow (r,s)} =\alpha_r d_{k\rightarrow r}+\alpha_s d_{k\rightarrow s}+\beta d_{r\rightarrow s}+<br>\gamma |d_{k\rightarrow r}-d_{k\rightarrow s}|<br>$$</p><p>具体的参数如表4所示。</p><table><thead><tr><th>clustering method</th><th>$\alpha_r$</th><th>$\alpha_s$</th><th>$\beta$</th><th>$\gamma$</th></tr></thead><tbody><tr><td>Single Link</td><td>1/2</td><td>1/2</td><td>0</td><td>-1/2</td></tr><tr><td>Complete Link</td><td>1/2</td><td>1/2</td><td>0</td><td>1/2</td></tr><tr><td>Unweighted pair group method average(UPGMA)</td><td>$\frac{n_r}{n_r+n_s}$</td><td>$\frac{n_s}{n_r+n_s}$</td><td>0</td><td>0</td></tr><tr><td>weighted pair group method average(WPGMA)</td><td>1/2</td><td>1/2</td><td>0</td><td>0</td></tr><tr><td>unweighted pair group method centroid(UPGMC)</td><td>$\frac{n_r}{n_r+n_s}$</td><td>$\frac{n_s}{n_r+n_s}$</td><td>$\frac{-n_sn_s}{(n_r+n_s)^2}$</td><td>0</td></tr><tr><td>weighted pair group method centroid(WPGMC)</td><td>1/2</td><td>1/2</td><td>-1/4</td><td>0</td></tr><tr><td>Ward’s method</td><td>$\frac{n_r+n_k}{n_r+n_s+n_k}$</td><td>$\frac{n_s+n_k}{n_r+n_s+n_k}$</td><td>$\frac{-n_k}{n_r+n_s+n_k}$</td><td>0</td></tr></tbody></table><p>最小距离层次聚类法也被称为单链聚类或者最近邻聚类。两个类之间的距离定义为两个类中对象之间距离的最小值。比如对于该例来说，最小的是D和E的距离0.50，选择先把D和E聚为一类(D,F)。然后其他单个样本之间的距离不变，计算(D,F)和其他样本之间的距离，采用最小距离法，得到的结果为：$d((D,F),A)=d(F,A)=3.20,d((D,F),B)=d(B,D)=2.92,d((D,F),C)$<br>$=d(D,C)=2.24,d((D,F),E)=d(D,E)=1.00$,然后可以与原来的样本间距离放在一起，发现0.71——A与B之间的距离最小，因此将A与B聚为一类，得到(A,B).依次类推，最终将所有的样本点聚成一个大类。总结一下计算过程如下：</p><p> 在一开始，我们有6个类，即A，B，C，D，E，F，</p><p> 将距离最小为0.5的D和F聚为一类(D,F)</p><p> 将距离最小为0.71的A和B聚为一类(A,B)</p><p> 将距离最小为1.00的(D,F)和E聚为一类(D,F,E)</p><p> 将距离最小为1.41的(D,F,E)和C聚为一类(D,F,E,C)</p><p> 将距离最小为2.50的(D,F,E,C)和(A,B)聚为一类(D,F,E,C,A,B)</p><p> 最后一个类包含了所有的样本点，聚类过程结束</p><p>根据上述过程，我们可以轻松的把层次聚类的树状图画出来。</p><p>那么，这个聚类的结果性能如何呢？有一个被称为共表相关系数的指标可以用来展示聚类结果的拟合优度。</p><p>为了计算层次聚类的共表相关系数，我们需要以下两个信息：</p><ul><li><p>距离矩阵</p></li><li><p>共表矩阵 </p></li></ul><p>我们在上文已经计算出来了样本之间的距离矩阵，现在只需要计算共表矩阵就行了。我们需要使用在合并类的过程中使用的最小距离来填充距离矩阵的下三角。上面我们已经将合并时所使用的距离进行了总结，利用上面的总结信息，可以得到如表所示的共表矩阵。</p><table><thead><tr><th>-</th><th>A</th><th>B</th><th>C</th><th>D</th><th>E</th><th>F</th></tr></thead><tbody><tr><td>A</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>B</td><td>0.71</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>C</td><td>2.50</td><td>2.50</td><td></td><td></td><td></td><td></td></tr><tr><td>D</td><td>2.50</td><td>2.50</td><td>1.41</td><td></td><td></td><td></td></tr><tr><td>E</td><td>2.50</td><td>2.50</td><td>1.41</td><td>1.00</td><td></td><td></td></tr><tr><td>F</td><td>2.50</td><td>2.50</td><td>1.41</td><td>0.50</td><td>1.00</td><td></td></tr></tbody></table><p>将距离矩阵和共表矩阵一一对应生成两个向量，计算这两个向量之间的相关系数，就会得到共表相关系数。在这里，最终CPCC=86.339%,可以看到，这个聚类的效果比较好。</p><p>这里以R中自带数据集iris作为样本，使用cluster包中的hclust函数进行层次聚类，分析聚类结果。</p><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="hljs-keyword">library</span>(cluster)</span><br><span class="line">&gt; iris_test &lt;- iris[,<span class="hljs-number">14</span>]</span><br><span class="line">&gt; d &lt;-dist(iris_test)</span><br><span class="line">&gt; hc1 &lt;- hclust(d,<span class="hljs-string">'complete'</span>)  <span class="hljs-comment">#最大距离法</span></span><br><span class="line">&gt; d1 &lt;- cophenetic(hc1)</span><br><span class="line">&gt; cor(d,d1)</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">0.7269857</span></span><br><span class="line">&gt; </span><br><span class="line">&gt; hc2 &lt;- hclust(d,<span class="hljs-string">'single'</span>)  <span class="hljs-comment">#最小距离法</span></span><br><span class="line">&gt; d2 &lt;- cophenetic(hc2)</span><br><span class="line">&gt; cor(d,d2)</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">0.8638787</span></span><br></pre></td></tr></table></figure><p>从上述R中的运行结果来看，single方法(最小距离法)的结果要优于complete法(最大距离法)。</p><h3 id="聚类个数的选择"><a href="#聚类个数的选择" class="headerlink" title="聚类个数的选择"></a>聚类个数的选择</h3><h4 id="轮廓系数-Silhouette-Coefficient-简称SC"><a href="#轮廓系数-Silhouette-Coefficient-简称SC" class="headerlink" title="轮廓系数(Silhouette Coefficient,简称SC)"></a>轮廓系数(Silhouette Coefficient,简称SC)</h4><p>对于数据集$x={x_1,x_2,…,x_n}$,使用某种聚类方法得到类的集合$C={C_1,C_2,…,C_k}$,每个类的样本量为$num={n_1,n_2,…,n_k}$}<br>$$a_i=\frac{1}{n_k-1}\sum <em>{x_i,x_j\in C_k}dist</em>{i\ne j}(x_i,x_j) $$<br>$$b_i=min(\frac{1}{n_l}\sum _{x_i\in C_k,x_j\in C_l,l\ne k}dist(x_i,x_j))$$<br>$$S_i=\frac{b_i-a_i}{max(b_i,a_i)}$$<br>$$SC=\frac{1}{n}\sum _{i=1}^n S_i$$<br>SC即为轮廓系数。该值处于-1-1之间，值越大，表示聚类效果越好。</p><h4 id="Gap统计量-Gap-Statistic-简称GS"><a href="#Gap统计量-Gap-Statistic-简称GS" class="headerlink" title="Gap统计量(Gap Statistic,简称GS)}"></a>Gap统计量(Gap Statistic,简称GS)}</h4><p>下面简单说一下Gap统计量原理及计算方法。}<br>Tibshirani等人在2001年提出了使用Gap统计量来确定聚类过程中最佳聚类个数k，这个统计量除了可以使用在最常用的k-means聚类方法上，对于任何聚类方法都适用。</p><p>假设有数据集${x_{ij}},i=1,2,3…,n,j=1,2,…,p$，该数据有n个观测点和p个特征。令$d_{ii’}$表示观测点$i$和观测点$i’$之间的距离，最常用的是欧氏距离。</p><p>假设我们已经将数据聚成$k$个类，分别是$C_1,C_2,…,C_k$，其中$C_r$表示第$r$个类的下标，令$n_r=|C_r|.$即$n_r$表示类$C_r$中的观测点的个数。令</p><p>$$D_r=\sum <em>{i,i’\in C_r}d</em>{ii’}=\sum _{i,i’\in C_r}|x_i-x_i’|^2=2n_r\sum _{i\in C_r}|xi-\bar x|^2$$<br>$$W_k=\sum _{r=1}^k\frac{1}{2n_r}D_r $$</p><p>也可以使用$W_k$来确定聚类个数$k$，寻找使得$W_k$突然变小的$k$.但是使用$W_k$来确定聚类个数有以下两个问题：}</p><p> 没有相关的类进行比较。<br> $W_k-W_{k-1}$的值没有进行标准化，以至于不同的模型无法进行比较。</p><p>Gap统计量将$W_k$进行了标准化，公式为<br>$$Gap_n(k)=E_n^+{log(W_k)}-log(W_k) ,$$<br>寻找使得$Gap(k)$达到最大的那个$k$，即为最佳的聚类个数。<br>其中，$E_n^+$表示根据某一相关分布构造的样本量为n的数据，其<br>$log(W_k)$的期望值。<br>通常，构造上述的样本具有以下两种方法，即：</p><p> 计算出每个特征（或变量）的取值范围，然后在该范围内随机生成n个服从均匀分布的数字，作为构造的样本观测值。<br> 如果$X$是一个$n\times p$的数值矩阵，假设每一列的均值都是0，然后对$X$进行奇异值分解，<br>$$X=UDV^T.$$<br>将$X$进行转换，得到$X’$<br>$$X’=XV.$$<br>然后利用方法1根据$X’$得到每一列的数值$Z’$。<br>最后，将$Z’$进行转换，<br>$$Z=Z’V^T$$<br>最终得到我们需要的数据$Z$。</p><p>计算Gap统计量的步骤如下。</p><p> 将数据聚类，类的个数分别取$k=1,2,…,K$，得出$W_k,k=1,2,…,K.$<br> 根据均匀分布，使用上面的方法生成B个数据集。计算出每个数据集的<br>$W_{kb}^+,b=1,2,…,B,k=1,2,…,K$，计算出估计的Gap统计量的值：</p><p>$$Gap(k)=(1/B)\sum <em>b log(W</em>{kb}^+)-log(W_k) $$</p><p>令</p><p>$$\bar l = (1/B)\sum <em>b log(W</em>{kb}^+)$$，</p><p>计算标准误差</p><p>$$sd_k=[(1/B)\sum <em>b {log(W</em>{kb}^+)-\bar l}^2]^{1/2}$$</p><p>定义$s_k=sd_k\sqrt {1+1/B}$，最后，计算出最佳的聚类个数$\hat k$，<br>$$\hat k=smallest\  k\  such\  that\  Gap(k) \le Gap(k+1)-s_{k+1} $$</p>]]></content>
      
      
        <tags>
            
            <tag> kmeans </tag>
            
            <tag> clustering </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lubridate包简介</title>
      <link href="/2018/04/01/Lubridate_notebook/"/>
      <url>/2018/04/01/Lubridate_notebook/</url>
      <content type="html"><![CDATA[<h2 id="lubridate包简介"><a href="#lubridate包简介" class="headerlink" title="lubridate包简介"></a>lubridate包简介</h2><p>该包的描述文档介绍道：<br>lubridate包包含了一些处理时间点和时间段的函数：<br>快速并且友好的分割、提取和更新时间的函数（比如从日期中提取年、月、日等信息），在日期上进行代数运算.lubridate包运用了一致并且易于记忆的语法，使得用户在使用该包的时候会感到非常简单和愉快.</p><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">library</span>(lubridate)</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="各种函数介绍"><a href="#各种函数介绍" class="headerlink" title="各种函数介绍"></a>各种函数介绍</h2><h3 id="add-with-rollback"><a href="#add-with-rollback" class="headerlink" title="add_with_rollback"></a>add_with_rollback</h3><p>add_with_rollback:Add and subtract months to a date without exceeding the last day of the new month.</p><p>add_with_rollback:将某个日期加上或者减取若干个月，不考虑新获得的月份的最后一天的实际值.比如2009-10-31，加上一个月就是2009-11-31，但是11月没有31号，所以会显示NA.如果使用%m+%，则会以自然月的方式相加.</p><ul><li>例子<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">may &lt;- ymd_hms(<span class="hljs-string">"1994-05-31 07:40:38"</span>)</span><br><span class="line">may + months(<span class="hljs-number">1</span>:<span class="hljs-number">3</span>)  <span class="hljs-comment">#六月31显示为NA</span></span><br><span class="line">may %m+% months(<span class="hljs-number">1</span>:<span class="hljs-number">3</span>)  <span class="hljs-comment">#不会有错误</span></span><br><span class="line"></span><br><span class="line">leap &lt;- ymd(<span class="hljs-string">"2012-02-29"</span>)</span><br><span class="line">leap + years(<span class="hljs-number">1</span>)  <span class="hljs-comment">#NA</span></span><br><span class="line">leap %m+% years(<span class="hljs-number">1</span>)  <span class="hljs-comment">#2013-02-28</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="am"><a href="#am" class="headerlink" title="am"></a>am</h3><p>am:Does date time occur in the am or pm?</p><p>am:判断是否是上午，显然，其需要的参数应该包含一个具体的一天中的时间信息，如果只提供日期信息，则默认为00:00:00，显示为上午时间.</p><ul><li>例子<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dt &lt;- ymd(<span class="hljs-string">"2012-02-02"</span>)</span><br><span class="line">am(dt)</span><br><span class="line">time &lt;- ymd_hm(<span class="hljs-string">"2012-02-01 13:20"</span>)</span><br><span class="line">am(time)</span><br></pre></td></tr></table></figure></li></ul><h3 id="as-duration"><a href="#as-duration" class="headerlink" title="as.duration"></a>as.duration</h3><p>as.duration:as.duration changes Interval, Period and numeric class objects to Duration objects. Numeric objects are changed to Duration objects with the seconds unit equal to the numeric value.</p><p>as.duration:将interval、period和数字转换为时间间隔——时间段.</p><ul><li>例子<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">span &lt;- interval(ymd(<span class="hljs-string">"1994-05-01"</span>),ymd(<span class="hljs-string">"1995-10-28"</span>))  <span class="hljs-comment">#interval</span></span><br><span class="line">as.duration(span)  <span class="hljs-comment">#47088000秒，约1.49年</span></span><br><span class="line">as.duration(<span class="hljs-number">10</span>)  <span class="hljs-comment">#10秒</span></span><br><span class="line">dur &lt;- duration(hours = <span class="hljs-number">10</span>,minutes = <span class="hljs-number">6</span>)  <span class="hljs-comment">#一段10分6秒的时间间隔</span></span><br><span class="line">as.numeric(dur,<span class="hljs-string">"hours"</span>)  <span class="hljs-comment">#10.1小时</span></span><br><span class="line">as.numeric(dur,<span class="hljs-string">"minutes"</span>)  <span class="hljs-comment">#606秒</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="as-interval"><a href="#as-interval" class="headerlink" title="as.interval"></a>as.interval</h3><p>as.intervel:Change an object to an interval.</p><p>as.interval:将对象转换为inteval</p><ul><li>例子<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">diff &lt;- make_difftime(days = <span class="hljs-number">31</span>) <span class="hljs-comment">#加上31天</span></span><br><span class="line">as.interval(diff, ymd(<span class="hljs-string">"2009-01-01"</span>))</span><br><span class="line">as.interval(diff, ymd(<span class="hljs-string">"2009-02-01"</span>))</span><br><span class="line"></span><br><span class="line">dur &lt;- duration(days = <span class="hljs-number">31</span>) <span class="hljs-comment">#duration</span></span><br><span class="line">as.interval(dur, ymd(<span class="hljs-string">"2009-01-01"</span>))</span><br><span class="line">as.interval(dur, ymd(<span class="hljs-string">"2009-02-01"</span>))</span><br><span class="line"></span><br><span class="line">per &lt;- period(months = <span class="hljs-number">1</span>) <span class="hljs-comment">#period</span></span><br><span class="line">as.interval(per, ymd(<span class="hljs-string">"2009-01-01"</span>))</span><br><span class="line">as.interval(per, ymd(<span class="hljs-string">"2009-02-01"</span>))</span><br><span class="line"></span><br><span class="line">as.interval(<span class="hljs-number">3600</span>,ymd(<span class="hljs-string">"2009-01-01"</span>)) <span class="hljs-comment">##numeric，在2009-01-01的基础上加上3600天</span></span><br><span class="line">as.interval(duration(hours = <span class="hljs-number">3600</span>), ymd(<span class="hljs-string">"2009-01-01"</span>))  <span class="hljs-comment">#加上3600小时</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="as-period"><a href="#as-period" class="headerlink" title="as.period"></a>as.period</h3><p>as.period:Change an object to a period.</p><p>as.period:将对象转换为period.</p><ul><li>例子<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">span &lt;- interval(as.POSIXct(<span class="hljs-string">"2009-01-01"</span>), as.POSIXct(<span class="hljs-string">"2010-02-02 01:01:01"</span>)) <span class="hljs-comment">#interval</span></span><br><span class="line">as.period(span)</span><br><span class="line">as.period(span, units = <span class="hljs-string">"day"</span>)</span><br></pre></td></tr></table></figure></li></ul><p>format of interval,duration and period:</p><ul><li><p>interval:</p><p>1990-01-01 UTC–1990-01-11 UTC</p></li><li><p>duration:</p></li></ul><p>47088000s (~1.49 years)</p><ul><li>period:</li></ul><p>1y 1m 1d 1H 1M 1S</p><h3 id="as-date"><a href="#as-date" class="headerlink" title="as_date"></a>as_date</h3><p>as_date:Convert an object to a date or date-time</p><p>as_date:将对象转换为date或者date-time类型</p><ul><li>例子<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dt_utc &lt;- ymd_hms(<span class="hljs-string">"2010-08-03 00:50:50"</span>)</span><br><span class="line">dt_europe &lt;- ymd_hms(<span class="hljs-string">"2010-08-03 00:50:50"</span>, tz = <span class="hljs-string">"Europe/London"</span>)</span><br><span class="line">c(as_date(dt_utc), as.Date(dt_utc))</span><br><span class="line">c(as_date(dt_europe), as.Date(dt_europe))</span><br><span class="line"><span class="hljs-comment">## need not suply origin，不需要提供初始值</span></span><br><span class="line">as_date(<span class="hljs-number">10</span>)  <span class="hljs-comment">#默认中1970-01-01年开始</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="as-datetime"><a href="#as-datetime" class="headerlink" title="as_datetime"></a>as_datetime</h3><p>as_datetime:Convert an object to a date or date-time</p><p>as_datetime:将对象转换为date或者date-time类型</p><ul><li>例子<br>同上.</li></ul><h3 id="date"><a href="#date" class="headerlink" title="date"></a>date</h3><p>date:Get/set Date component of a date-time.</p><p>date:获取/设置日期对象</p><ul><li>例子</li></ul><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x &lt;- as.POSIXct(<span class="hljs-string">"2012-03-26 23:12:13"</span>, tz = <span class="hljs-string">"Etc/GMT+8"</span>)</span><br><span class="line">x</span><br><span class="line">date(x)</span><br><span class="line">as.Date(x) <span class="hljs-comment">#as.Date()函数默认返回UTC时间</span></span><br><span class="line">as.Date(x,tz = <span class="hljs-string">'Etc/GMT+8'</span>)  <span class="hljs-comment">#返回Etc/GMT+8时区时间</span></span><br><span class="line">date(x) &lt;- as.Date(<span class="hljs-string">'2014-10-20'</span>)</span><br><span class="line">date(x)</span><br></pre></td></tr></table></figure><h3 id="DateTimeUpdate"><a href="#DateTimeUpdate" class="headerlink" title="DateTimeUpdate"></a>DateTimeUpdate</h3><p>DateTimeUpdate:Description of the classes “POSIXlt” and “POSIXct” representing calendar dates and times.</p><p>DateTimeUpdate:</p><h3 id="date-decimal"><a href="#date-decimal" class="headerlink" title="date_decimal"></a>date_decimal</h3><p>date_decimal:Converts a decimal to a date.</p><p>date_decimal:将一个小数转换为日期</p><ul><li>例子</li></ul><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">decimal &lt;- <span class="hljs-number">2017.12</span></span><br><span class="line">date_decimal(decimal)</span><br></pre></td></tr></table></figure><h3 id="day"><a href="#day" class="headerlink" title="day"></a>day</h3><p>day:Get/set days component of a date-time.</p><p>day:获取/设置日期中的具体日数据</p><ul><li>例子</li></ul><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x &lt;- as.Date(<span class="hljs-string">'2017-10-30'</span>)</span><br><span class="line">day(x) <span class="hljs-comment">#获取日期</span></span><br><span class="line">mday(x)  <span class="hljs-comment">#一个月中的第几天</span></span><br><span class="line">yday(x)  <span class="hljs-comment">#一年中的第几天</span></span><br><span class="line">wdayofdate &lt;- wday(x,label = <span class="hljs-literal">TRUE</span>,abbr = <span class="hljs-literal">FALSE</span>) </span><br><span class="line">levels(wdayofdate) &lt;- c(levels(wdayofdate)[<span class="hljs-number">2</span>:<span class="hljs-number">7</span>],levels(wdayofdate)[<span class="hljs-number">1</span>])</span><br><span class="line">wdayofdate</span><br><span class="line"><span class="hljs-comment">#一周中的第几天,以周日为第一天</span></span><br><span class="line"><span class="hljs-comment">#因为中国人默认周一为一周的第一天，所以可以更改一下因子的排序</span></span><br></pre></td></tr></table></figure><h3 id="days-in-month"><a href="#days-in-month" class="headerlink" title="days_in_month"></a>days_in_month</h3><p>days_in_month:Get the number of days in the month of a date-time.</p><p>days_in_month:某月份有几天,输入一个带年份的日期数据</p><ul><li>例子<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">md &lt;- ymd(<span class="hljs-string">'1994-05-01'</span>)</span><br><span class="line">days_in_month(md)</span><br></pre></td></tr></table></figure></li></ul><h3 id="decimal-date"><a href="#decimal-date" class="headerlink" title="decimal_date"></a>decimal_date</h3><p>decimal_date:Converts a date to a decimal of its year.</p><p>decimal_date:将日期转换成小数.</p><ul><li>例子</li></ul><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dt &lt;- ymd(<span class="hljs-string">'2017-10-31'</span>)</span><br><span class="line">decimal_date(dt)</span><br></pre></td></tr></table></figure><h3 id="dst"><a href="#dst" class="headerlink" title="dst"></a>dst</h3><p>dst:Get Daylight Savings Time indicator of a date-time.</p><p>dst:</p><h3 id="duration"><a href="#duration" class="headerlink" title="duration"></a>duration</h3><p>duration:Create a duration object.</p><p>duration:创建一个duration对象</p><h3 id="Duration-class"><a href="#Duration-class" class="headerlink" title="Duration_class"></a>Duration_class</h3><p>Duration_class:Fit a POSIXlt date-time to the timeline<br>Duration_class:</p><h3 id="fit-to-timeline"><a href="#fit-to-timeline" class="headerlink" title="fit_to_timeline"></a>fit_to_timeline</h3><h3 id="force-tz"><a href="#force-tz" class="headerlink" title="force_tz"></a>force_tz</h3><p>force_tz:Replace time zone to create new date-time</p><p>force_tz:将日期的tz换了，创建一个新日期对象</p><ul><li>例子</li></ul><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x &lt;- as.POSIXct(<span class="hljs-string">"2009-08-07 00:00:01"</span>, tz = <span class="hljs-string">"America/New_York"</span>)</span><br><span class="line">x</span><br><span class="line">force_tz(x, <span class="hljs-string">"GMT"</span>)</span><br></pre></td></tr></table></figure><h3 id="guess-formats"><a href="#guess-formats" class="headerlink" title="guess_formats"></a>guess_formats</h3><p>guess_formats:Guess formats from the supplied date-time character vector.</p><p>guess_formats:猜测提供的日期格式</p><ul><li>例子</li></ul><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dt &lt;- c(<span class="hljs-string">'1990-01'</span>,<span class="hljs-string">'1990 09 01'</span>,<span class="hljs-string">'1992 Jan 01'</span>,<span class="hljs-string">'May 1 1992'</span>)</span><br><span class="line">guess_formats(dt,orders = c(<span class="hljs-string">"ymd"</span>,<span class="hljs-string">"ym"</span>,<span class="hljs-string">"mdy"</span>),print_matches = <span class="hljs-literal">TRUE</span>)</span><br></pre></td></tr></table></figure><h3 id="hour"><a href="#hour" class="headerlink" title="hour"></a>hour</h3><p>hour；Get/set hours component of a date-time.<br>hour:获取/设置日期中的小时数据</p><ul><li>例子</li></ul><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dt &lt;- now()</span><br><span class="line">hour(dt)</span><br><span class="line">hour(dt) &lt;- <span class="hljs-number">1</span></span><br><span class="line">dt</span><br></pre></td></tr></table></figure><h3 id="interval"><a href="#interval" class="headerlink" title="interval"></a>interval</h3><p>interval:Utilities for creation and manipulation of Interval objects.</p><h3 id="interval-class"><a href="#interval-class" class="headerlink" title="interval_class"></a>interval_class</h3><h3 id="is-Date"><a href="#is-Date" class="headerlink" title="is.Date"></a>is.Date</h3><p>is.Date():Is x a Date object?</p><p>is.Date():x是否是一个日期对象</p><ul><li>例子</li></ul><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x &lt;- <span class="hljs-string">'1970-01-01'</span></span><br><span class="line">is.Date(x)</span><br><span class="line">x1 &lt;- as.Date(x)</span><br><span class="line">is.Date(x1)</span><br><span class="line">x2 &lt;- ymd(x)</span><br><span class="line">is.Date(x2)</span><br></pre></td></tr></table></figure><h3 id="is-difftime"><a href="#is-difftime" class="headerlink" title="is.difftime"></a>is.difftime</h3><p>is.difftime:Is x a difftime object?</p><p>is.difftime:x是否是一个difftime对象</p><ul><li>例子</li></ul><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">is.difftime(as.Date(<span class="hljs-string">'1999-09-09'</span>)) <span class="hljs-comment">#FALSE</span></span><br><span class="line">is.difftime(make_difftime(<span class="hljs-number">20</span>))  <span class="hljs-comment">#TRUE,默认20秒间隔</span></span><br></pre></td></tr></table></figure><h3 id="is-instant"><a href="#is-instant" class="headerlink" title="is.instant"></a>is.instant</h3><p>is.instant:Is x a date-time object?</p><p>is.instant:x是否为日期对象，同is.Date()</p><ul><li>例子</li></ul><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x &lt;- as.Date(<span class="hljs-string">'1999-09-09'</span>)</span><br><span class="line">is.instant(x)</span><br><span class="line">is.timepoint(x)</span><br><span class="line">is.timepoint(<span class="hljs-number">10</span>)</span><br></pre></td></tr></table></figure><h3 id="is-POSIXt"><a href="#is-POSIXt" class="headerlink" title="is.POSIXt"></a>is.POSIXt</h3><p>is.POSIXt:Is x a POSIXct or POSIXlt object?</p><p>is.POSIXt:x是否为POSIXct对象或者POSIXlt对象?</p><ul><li>例子</li></ul><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">is.POSIXt(as.Date(<span class="hljs-string">"1997-01-01"</span>))</span><br><span class="line">is.POSIXt(as.POSIXct(<span class="hljs-string">"1997-01-01"</span>))</span><br></pre></td></tr></table></figure><h3 id="is-timespan"><a href="#is-timespan" class="headerlink" title="is.timespan"></a>is.timespan</h3><p>is.timespan:Is x a length of time?</p><p>is.timespan:x是否时时间长度</p><ul><li>例子</li></ul><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">is.timespan(as.Date(<span class="hljs-string">"1999-09-09"</span>))</span><br><span class="line">is.timespan(duration(second = <span class="hljs-number">1</span>))</span><br><span class="line">is.timespan(interval(ymd(<span class="hljs-string">"1999-09-09"</span>),ymd(<span class="hljs-string">"2000-01-01"</span>)))</span><br><span class="line">is.timespan(period(<span class="hljs-number">10</span>,units = <span class="hljs-string">'month'</span>))</span><br></pre></td></tr></table></figure><h3 id="leap-year"><a href="#leap-year" class="headerlink" title="leap_year"></a>leap_year</h3><p>leap_year:Is a year a leap year?</p><p>leap_year:是否时闰年</p><ul><li>例子</li></ul><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">leap_year(<span class="hljs-number">2012</span>)</span><br><span class="line">leap_year(<span class="hljs-number">2013</span>)</span><br></pre></td></tr></table></figure><h3 id="local-time"><a href="#local-time" class="headerlink" title="local_time"></a>local_time</h3><p>local_time:</p><p>local_time:</p><h3 id="make-date"><a href="#make-date" class="headerlink" title="make_date"></a>make_date</h3><h3 id="make-datetime"><a href="#make-datetime" class="headerlink" title="make_datetime"></a>make_datetime</h3><p>make_datetime:Efficient creation of date-times from numeric representations</p><p>make_datetime:有效创建一个日期对象</p><ul><li>例子</li></ul><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dt &lt;- make_datetime(year = <span class="hljs-number">1999</span>,month = <span class="hljs-number">9</span>,day = <span class="hljs-number">10</span>)</span><br><span class="line">dt</span><br></pre></td></tr></table></figure><h3 id="make-difftime"><a href="#make-difftime" class="headerlink" title="make_difftime"></a>make_difftime</h3><p>make_difftime:Create a difftime object.</p><p>make_difftime:创建一个时间长度对象.</p><ul><li>例子</li></ul><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">make_difftime(<span class="hljs-number">1</span>)</span><br><span class="line">make_difftime(<span class="hljs-number">60</span>)</span><br><span class="line">make_difftime(<span class="hljs-number">3600</span>)</span><br><span class="line">make_difftime(<span class="hljs-number">3600</span>, units = <span class="hljs-string">"minute"</span>)</span><br><span class="line"><span class="hljs-comment"># Time difference of 60 mins</span></span><br><span class="line">make_difftime(second = <span class="hljs-number">90</span>)</span><br><span class="line"><span class="hljs-comment"># Time difference of 1.5 mins</span></span><br><span class="line">make_difftime(minute = <span class="hljs-number">1.5</span>)</span><br><span class="line"><span class="hljs-comment"># Time difference of 1.5 mins</span></span><br><span class="line">make_difftime(second = <span class="hljs-number">3</span>, minute = <span class="hljs-number">1.5</span>, hour = <span class="hljs-number">2</span>, day = <span class="hljs-number">6</span>, week = <span class="hljs-number">1</span>)</span><br><span class="line"><span class="hljs-comment"># Time difference of 13.08441 days</span></span><br><span class="line">make_difftime(hour = <span class="hljs-number">1</span>, minute = -<span class="hljs-number">60</span>)</span><br><span class="line"><span class="hljs-comment"># Time difference of 0 secs</span></span><br><span class="line">make_difftime(day = -<span class="hljs-number">1</span>)</span><br><span class="line"><span class="hljs-comment"># Time difference of -1 days</span></span><br><span class="line">make_difftime(second = <span class="hljs-number">120</span>,minute = <span class="hljs-number">10</span>,hour = <span class="hljs-number">10</span>,day = <span class="hljs-number">1</span>,week = <span class="hljs-number">1</span>, units = <span class="hljs-string">"hour"</span>)</span><br><span class="line">make_difftime(<span class="hljs-number">20</span>,day = <span class="hljs-number">6</span>,week = <span class="hljs-number">1</span>)</span><br><span class="line"><span class="hljs-comment"># Time differences in mins</span></span><br></pre></td></tr></table></figure><h3 id="minute"><a href="#minute" class="headerlink" title="minute"></a>minute</h3><p> minute:Get/set minutes component of a date-time.</p><p> minute:类似于hour.</p><h3 id="month"><a href="#month" class="headerlink" title="month"></a>month</h3><p>month:Get/set months component of a date-time.</p><p>month:类似于day.</p><h3 id="ms"><a href="#ms" class="headerlink" title="ms"></a>ms</h3><p>ms:Create a period with the specified hours, minutes, and seconds</p><p>ms:使用特定的小时、分钟和秒创建period.</p><ul><li>例子</li></ul><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#ms:代表分钟,s代表秒</span></span><br><span class="line"><span class="hljs-comment">#hms:h代表小时</span></span><br><span class="line">ms(<span class="hljs-string">"09 10"</span>)</span><br><span class="line">hms(<span class="hljs-string">"09:10:10"</span>)</span><br><span class="line">hms(<span class="hljs-string">"3:22:::2"</span>)</span><br></pre></td></tr></table></figure><h3 id="now"><a href="#now" class="headerlink" title="now"></a>now</h3><p>now:The current time</p><p>now:获取当前时间</p><p>+例子</p><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">now()</span><br></pre></td></tr></table></figure><h3 id="origin"><a href="#origin" class="headerlink" title="origin"></a>origin</h3><p>origin：1970-01-01 UTC</p><p>origin：1970-01-01 UTC，是一个常数，不是函数</p><ul><li>例子</li></ul><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">origin</span><br></pre></td></tr></table></figure><h3 id="parse-date-time"><a href="#parse-date-time" class="headerlink" title="parse_date_time"></a>parse_date_time</h3><p>parse_date_time:Parse character and numeric date-time vectors with user friendly order formats.</p><p>parse_date_time:将字符型或者数值型日期转换成用户友好的格式。</p><ul><li>例子</li></ul><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="poriod"><a href="#poriod" class="headerlink" title="poriod"></a>poriod</h3><h3 id="peroid-to-seconds"><a href="#peroid-to-seconds" class="headerlink" title="peroid_to_seconds"></a>peroid_to_seconds</h3><h3 id="pretty-dates"><a href="#pretty-dates" class="headerlink" title="pretty_dates"></a>pretty_dates</h3><h3 id="quarter"><a href="#quarter" class="headerlink" title="quarter"></a>quarter</h3><h3 id="roll-back"><a href="#roll-back" class="headerlink" title="roll_back"></a>roll_back</h3><h3 id="round-date"><a href="#round-date" class="headerlink" title="round_date"></a>round_date</h3><h3 id="second"><a href="#second" class="headerlink" title="second"></a>second</h3><h3 id="stamp"><a href="#stamp" class="headerlink" title="stamp"></a>stamp</h3><h3 id="timespan"><a href="#timespan" class="headerlink" title="timespan"></a>timespan</h3><h3 id="time-length"><a href="#time-length" class="headerlink" title="time_length"></a>time_length</h3><h3 id="today"><a href="#today" class="headerlink" title="today"></a>today</h3><h3 id="tz"><a href="#tz" class="headerlink" title="tz"></a>tz</h3><h3 id="week"><a href="#week" class="headerlink" title="week"></a>week</h3><h3 id="with-tz"><a href="#with-tz" class="headerlink" title="with_tz"></a>with_tz</h3><h3 id="year"><a href="#year" class="headerlink" title="year"></a>year</h3><h3 id="ymd"><a href="#ymd" class="headerlink" title="ymd"></a>ymd</h3><h3 id="ymd-hms"><a href="#ymd-hms" class="headerlink" title="ymd_hms"></a>ymd_hms</h3><h3 id="m"><a href="#m" class="headerlink" title="%m+%"></a>%m+%</h3><h3 id="within"><a href="#within" class="headerlink" title="%within%"></a>%within%</h3><h3 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h3><h2 id="三、应用实例"><a href="#三、应用实例" class="headerlink" title="三、应用实例"></a>三、应用实例</h2>]]></content>
      
      
        <tags>
            
            <tag> R </tag>
            
            <tag> Lubridate </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Selenium爬取MOOC网课程信息</title>
      <link href="/2018/04/01/selenium-%E7%BD%91%E6%98%93%E4%BA%91%E8%AF%BE%E5%A0%82/"/>
      <url>/2018/04/01/selenium-%E7%BD%91%E6%98%93%E4%BA%91%E8%AF%BE%E5%A0%82/</url>
      <content type="html"><![CDATA[<p>近期在写一份关于大数据相关的作业，需要搜索近年来市面上关于大数据的书籍信息和课程信息。其中一位同学负责在当当网上爬取书籍信息，我就负责爬取MOOC网的课程信息。</p><p>刚开始的时候，以为MOOC网作为一个公益性网站，安全性不会那么高，因此会比较好爬。然而我还是太天真了，网站上一大批JavaScript让我不知所措。好在经过一段时间的探索，终于能够成功爬取了。</p><a id="more"></a><h2 id="网站分析"><a href="#网站分析" class="headerlink" title="网站分析"></a>网站分析</h2><p>打开MOOC官网，在搜索框输入“大数据”关键词，发现返回了99条数据（当时的情况），也就是说，有99个关于大数据的课程。</p><p>但是，只有课程列表是不行的。就像爬取淘宝网站的时候，获取到了商品列表，还需要进入到商品的详情页面，然后抓取我们需要的信息。在这里，我们同样需要这样的方法。</p><p>但是，通过Google浏览器的检查功能可以发现，你几乎无法在课程页面获取什么东西——因为几乎都是动态变化的。我试图获取每个课程上面的超链接，然后进入到具体的详情页面，但是很显然直接使用requests方法是不行的。</p><p>后来经过同学指点发现此处需要通过post方法，获取到response，返回的response里面才具有我们需要的详情页面的信息（其实也就是每个课程的id，通过该id可以构造详情页面）</p><p><img src="/picture/mooc_1.png" alt="详情页"></p><h2 id="代码设计"><a href="#代码设计" class="headerlink" title="代码设计"></a>代码设计</h2><h3 id="获取课程id"><a href="#获取课程id" class="headerlink" title="获取课程id"></a>获取课程id</h3><p>经过上面的分析，我首先找到了商品id存储的页面，如下图所示,我发现当我点击下一页的时候，会多出图中红色方框部分的网址，说明该网址是我请求的response，点击preview查看预览也印证了我的猜测。</p><p><img src="/picture/mooc_2.png" alt="response"></p><p>问题搞清楚了，下面使用requests包的post函数发送请求，然后分析获取到的response。</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> requests</span><br><span class="line"><span class="hljs-keyword">import</span> urllib.parse <span class="hljs-keyword">as</span> up</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#准备进行搜索的关键词</span></span><br><span class="line">keywords = [<span class="hljs-string">'大数据'</span>,<span class="hljs-string">'机器学习'</span>,<span class="hljs-string">'数据挖掘'</span>,<span class="hljs-string">'数据科学'</span>,<span class="hljs-string">'人工智能'</span>]</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#转换成URL编码</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">quote</span><span class="hljs-params">(x)</span>:</span></span><br><span class="line">    <span class="hljs-keyword">return</span> up.quote(x)</span><br><span class="line"><span class="hljs-comment">#转换编码</span></span><br><span class="line">keywords = list(map(quote,keywords))</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#URL前缀</span></span><br><span class="line">startUrl = <span class="hljs-string">"http://www.icourse163.org/search.htm?search="</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#构造URL</span></span><br><span class="line">urls = []</span><br><span class="line"><span class="hljs-keyword">for</span> kws <span class="hljs-keyword">in</span> keywords:</span><br><span class="line">    urls.append(startUrl+kws)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#post的URL</span></span><br><span class="line">jsurl = <span class="hljs-string">"http://www.icourse163.org/dwr/call/plaincall/MocSearchBean.searchMocCourse.dwr"</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#请求头</span></span><br><span class="line">headers = &#123;</span><br><span class="line">        <span class="hljs-string">"Accept"</span>:<span class="hljs-string">"*/*"</span>,</span><br><span class="line">        <span class="hljs-string">"Accept-Encoding"</span>:<span class="hljs-string">"gzip,deflate"</span>,</span><br><span class="line">        <span class="hljs-string">"Accept-Language"</span>:<span class="hljs-string">"zh-CN,zh;q=0.9"</span>,</span><br><span class="line">        <span class="hljs-string">"Connection"</span>:<span class="hljs-string">"keep-alive"</span>,</span><br><span class="line">        <span class="hljs-string">"Content-Length"</span>:<span class="hljs-string">"522"</span>,</span><br><span class="line">        <span class="hljs-string">"Content-Type"</span>:<span class="hljs-string">"text/plain"</span>,</span><br><span class="line">        <span class="hljs-string">"Host"</span>:<span class="hljs-string">"www.icourse163.org"</span>,</span><br><span class="line">        <span class="hljs-string">"Origin"</span>:<span class="hljs-string">"http://www.icourse163.org"</span></span><br><span class="line">        <span class="hljs-comment">#Refere是我们查询的时候对应的URL，也需要根据不同的关键词进行调整</span></span><br><span class="line">        <span class="hljs-comment">#"Referer":"http://www.icourse163.org/search.htm?search=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"</span></span><br><span class="line">        &#125;</span><br><span class="line"><span class="hljs-comment">#发送的数据</span></span><br><span class="line">payload = &#123;</span><br><span class="line">    <span class="hljs-string">"callCount"</span>:<span class="hljs-string">"1"</span>,</span><br><span class="line">    <span class="hljs-string">"scriptSessionId"</span>:<span class="hljs-string">"$&#123;scriptSessionId&#125;190"</span>,</span><br><span class="line">    <span class="hljs-string">"httpSessionId"</span>:<span class="hljs-string">"907805e60a6540c4a268164e9e89ac4c"</span>,</span><br><span class="line">    <span class="hljs-string">"c0-scriptName"</span>:<span class="hljs-string">"MocSearchBean"</span>,</span><br><span class="line">    <span class="hljs-string">"c0-methodName"</span>:<span class="hljs-string">"searchMocCourse"</span>,</span><br><span class="line">    <span class="hljs-string">"c0-id"</span>:<span class="hljs-string">"0"</span>,</span><br><span class="line">    <span class="hljs-comment">#c0-e1的string是我们查询的关键词，需要根据不同的关键词进行更改</span></span><br><span class="line">    <span class="hljs-comment">#"c0-e1":"string:%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0",</span></span><br><span class="line">    <span class="hljs-comment">#c0-e2的number表示获取的是第几页数据，需要动态变化</span></span><br><span class="line">    <span class="hljs-comment">#"c0-e2":"number:1",</span></span><br><span class="line">    <span class="hljs-string">"c0-e3"</span>:<span class="hljs-string">"boolean:true"</span>,</span><br><span class="line">    <span class="hljs-string">"c0-e4"</span>:<span class="hljs-string">"null:null"</span>,</span><br><span class="line">    <span class="hljs-string">"c0-e5"</span>:<span class="hljs-string">"number:0"</span>,</span><br><span class="line">    <span class="hljs-string">"c0-e6"</span>:<span class="hljs-string">"number:30"</span>,</span><br><span class="line">    <span class="hljs-string">"c0-e7"</span>:<span class="hljs-string">"number:20"</span>,</span><br><span class="line">    <span class="hljs-string">"c0-param0"</span>:<span class="hljs-string">"Object_Object:&#123;keyword:reference:c0-e1,pageIndex:reference:c0-e2,highlight:reference:c0-e3,categoryId:reference:c0-e4,orderBy:reference:c0-e5,stats:reference:c0-e6,pageSize:reference:c0-e7&#125;"</span>,</span><br><span class="line">    <span class="hljs-string">"batchId"</span>:<span class="hljs-string">"1511830181483"</span></span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#构造一个空字典，用于存储课程列表中每一门课程的id</span></span><br><span class="line">courses = &#123;&#125;</span><br><span class="line"><span class="hljs-comment">#分析response</span></span><br><span class="line"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,len(urls)):</span><br><span class="line">    headers[<span class="hljs-string">"Referer"</span>] = urls[i]</span><br><span class="line">    string = <span class="hljs-string">"string:"</span> + keywords[i]</span><br><span class="line">    payload[<span class="hljs-string">"c0-e1"</span>] = string</span><br><span class="line">    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,<span class="hljs-number">20</span>):  <span class="hljs-comment">#大致查询了一下，课程数量不会超过20页</span></span><br><span class="line">        page = <span class="hljs-string">"number:"</span> + str(j)</span><br><span class="line">        payload[<span class="hljs-string">"c0-e2"</span>] = page</span><br><span class="line">        <span class="hljs-comment">#目前为止，上面请求的部分已经做完</span></span><br><span class="line">        response = requests.post(data=payload,url=jsurl,headers = headers)</span><br><span class="line">        courseid = re.findall(pattern=<span class="hljs-string">r'courseId=([0-9]&#123;0,20&#125;)'</span>,string=response.text)</span><br><span class="line">        <span class="hljs-keyword">if</span>(len(courseid) == <span class="hljs-number">0</span>):</span><br><span class="line">            <span class="hljs-keyword">break</span>;</span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            kw = up.unquote(keywords[i])</span><br><span class="line">            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> kw <span class="hljs-keyword">in</span> courses.keys():</span><br><span class="line">                courses[kw] = courseid</span><br><span class="line">            <span class="hljs-keyword">else</span>:</span><br><span class="line">                courses[kw].extend(courseid)</span><br></pre></td></tr></table></figure><h3 id="获取详情"><a href="#获取详情" class="headerlink" title="获取详情"></a>获取详情</h3><p>上面已经获取到了课程的id，我们只需要使用该id构造课程详情页的URL就行了。</p><p><img src="/picture/mooc_3.png" alt="URL"></p><p>上图展示了课程详情页的URL信息，总结可以发现，前面的部分”<a href="http://www.icourse163.org/course/“" target="_blank" rel="noopener">http://www.icourse163.org/course/“</a> 都是一样的，只有后面的大学简称和id是变化的。而且大学简称可以使用任何非空值……利用上面的信息，构造好需要的URL，然后就可以使用selenium进行爬取了。</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#使用无头浏览器phantomjs获取页面信息</span></span><br><span class="line">browser = webdriver.PhantomJS(<span class="hljs-string">'C:/phantomjs/bin/phantomjs.exe'</span>)</span><br><span class="line"><span class="hljs-comment">#data用来存储我们获取到的数据</span></span><br><span class="line">data = <span class="hljs-keyword">None</span></span><br><span class="line">data = pd.DataFrame(&#123;<span class="hljs-string">"course_name"</span>:<span class="hljs-string">""</span>,<span class="hljs-string">"start_times"</span>:<span class="hljs-string">""</span>,<span class="hljs-string">"lasting"</span>:<span class="hljs-string">""</span>,<span class="hljs-string">"start_date"</span>:<span class="hljs-string">""</span>,<span class="hljs-string">"end_date"</span>:<span class="hljs-string">""</span>,</span><br><span class="line">                     <span class="hljs-string">"rollnum"</span>:<span class="hljs-string">""</span>,<span class="hljs-string">"coursehrs"</span>:<span class="hljs-string">""</span>,<span class="hljs-string">"outline"</span>:<span class="hljs-string">""</span>,<span class="hljs-string">"key_word"</span>:<span class="hljs-string">""</span>&#125;,index=[<span class="hljs-string">"0"</span>])</span><br><span class="line"><span class="hljs-comment">#data frame的行索引</span></span><br><span class="line">index = <span class="hljs-number">0</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> courses.keys():  <span class="hljs-comment">#k是键</span></span><br><span class="line">    <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> courses[k]:  <span class="hljs-comment">#v是值</span></span><br><span class="line">        <span class="hljs-comment">#page是构造的课程详情页URL</span></span><br><span class="line">        page = <span class="hljs-string">"http://www.icourse163.org/course/ABC-"</span> + str(v)</span><br><span class="line">        <span class="hljs-comment">#get数据</span></span><br><span class="line">        browser.get(page)</span><br><span class="line">        <span class="hljs-comment">#每个页面之间停顿3秒，否则有可能还没有渲染成功，获取不到数据</span></span><br><span class="line">        <span class="hljs-comment">#这应该是一种隐式等待</span></span><br><span class="line">        time.sleep(<span class="hljs-number">3</span>)</span><br><span class="line">        <span class="hljs-comment">#info是我们需要的一系列信息，根据id(j-center)返回</span></span><br><span class="line">        info = browser.find_element_by_id(<span class="hljs-string">'j-center'</span>).text</span><br><span class="line">        info = re.sub(re.compile(<span class="hljs-string">"\n"</span>),<span class="hljs-string">""</span>,info)</span><br><span class="line">        info = re.sub(re.compile(<span class="hljs-string">r'[0-9]&#123;2&#125;:[0-9]&#123;2&#125;'</span>),<span class="hljs-string">""</span>,string=info)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment">#1.课程名称</span></span><br><span class="line">        course_name = browser.find_element_by_tag_name(<span class="hljs-string">'h1'</span>).text</span><br><span class="line">        <span class="hljs-comment">#2.第几次开课</span></span><br><span class="line">        start_times = re.search(pattern=<span class="hljs-string">"第([0-9])次开课"</span>,string=info)</span><br><span class="line">        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> start_times <span class="hljs-keyword">is</span> <span class="hljs-keyword">None</span>:</span><br><span class="line">            start_times = start_times.group(<span class="hljs-number">1</span>)</span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            start_times = <span class="hljs-string">"NA"</span></span><br><span class="line">        <span class="hljs-comment">#3.持续时长</span></span><br><span class="line">        lasting = re.search(pattern=<span class="hljs-string">"课程已进行至([0-9]&#123;0,2&#125;\/[0-9]&#123;0,2&#125;)周"</span>,string=info)</span><br><span class="line">        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> lasting <span class="hljs-keyword">is</span> <span class="hljs-keyword">None</span>:</span><br><span class="line">            lasting = lasting.group(<span class="hljs-number">1</span>)</span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            lasting = <span class="hljs-string">"NA"</span></span><br><span class="line">        <span class="hljs-comment">#4.开始日期</span></span><br><span class="line">        start_date = re.search(pattern= <span class="hljs-string">r"开课：([0-9]&#123;0,4&#125;[年]&#123;0,1&#125;[0-9]&#123;0,2&#125;月[0-9]&#123;0,2&#125;日)"</span>,string=info)</span><br><span class="line">        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> start_date <span class="hljs-keyword">is</span> <span class="hljs-keyword">None</span>:</span><br><span class="line">            start_date = start_date.group(<span class="hljs-number">1</span>)</span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            start_date = <span class="hljs-string">"NA"</span></span><br><span class="line">        <span class="hljs-comment">#5.结束日期</span></span><br><span class="line">        end_date = re.search(pattern = <span class="hljs-string">r"结束：([0-9]&#123;0,4&#125;[年]&#123;0,1&#125;[0-9]&#123;0,2&#125;月[0-9]&#123;0,2&#125;日)"</span>,string=info)</span><br><span class="line">        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> end_date <span class="hljs-keyword">is</span> <span class="hljs-keyword">None</span>:</span><br><span class="line">            end_date = end_date.group(<span class="hljs-number">1</span>)</span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            end_date = <span class="hljs-string">"NA"</span></span><br><span class="line">        <span class="hljs-comment">#6.参与人数</span></span><br><span class="line">        rollnum = re.search(pattern = <span class="hljs-string">r"([0-9]&#123;0,9&#125;)人参加"</span>,string = info)</span><br><span class="line">        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> rollnum <span class="hljs-keyword">is</span> <span class="hljs-keyword">None</span>:</span><br><span class="line">            rollnum = rollnum.group(<span class="hljs-number">1</span>)</span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            rollnum = <span class="hljs-string">"NA"</span></span><br><span class="line">        <span class="hljs-comment">#7.课程时长</span></span><br><span class="line">        coursehrs = re.search(pattern=<span class="hljs-string">r"课程时长(.*?)周"</span>,string=info)</span><br><span class="line">        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> coursehrs <span class="hljs-keyword">is</span> <span class="hljs-keyword">None</span>:</span><br><span class="line">            coursehrs = coursehrs.group(<span class="hljs-number">1</span>)</span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            coursehrs = <span class="hljs-string">"NA"</span></span><br><span class="line">        <span class="hljs-comment">#8.课程概述</span></span><br><span class="line">        outline = browser.find_element_by_id(<span class="hljs-string">'j-rectxt2'</span>).text</span><br><span class="line">        <span class="hljs-keyword">if</span> outline <span class="hljs-keyword">is</span> <span class="hljs-keyword">None</span>:</span><br><span class="line">            outline = <span class="hljs-string">"NA"</span></span><br><span class="line"></span><br><span class="line">        data.loc[index] = &#123;<span class="hljs-string">"course_name"</span>:course_name,<span class="hljs-string">"start_times"</span>:start_times,<span class="hljs-string">"lasting"</span>:lasting,<span class="hljs-string">"start_date"</span>:start_date,</span><br><span class="line">                       <span class="hljs-string">"end_date"</span>:end_date,<span class="hljs-string">"rollnum"</span>:rollnum,<span class="hljs-string">"coursehrs"</span>:coursehrs,<span class="hljs-string">"outline"</span>:outline,<span class="hljs-string">"key_word"</span>:k&#125;</span><br><span class="line"></span><br><span class="line">        index = index + <span class="hljs-number">1</span></span><br><span class="line"></span><br><span class="line">        print(<span class="hljs-string">"已经获取第%d个课程数据！"</span>%(index))</span><br></pre></td></tr></table></figure><h2 id="结果展示"><a href="#结果展示" class="headerlink" title="结果展示"></a>结果展示</h2><p>数据获取完毕以后，把存储在内存中的数据输出到Excel</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> pandas <span class="hljs-keyword">import</span>  ExcelWriter</span><br><span class="line">writer = ExcelWriter(<span class="hljs-string">"MOOC.xlsx"</span>)</span><br><span class="line">data.to_excel(writer,<span class="hljs-string">"mooc"</span>)</span><br><span class="line">writer.save()</span><br></pre></td></tr></table></figure><p>最终展示在Excel中的数据如下图：</p><p><img src="/picture/mooc_4.png" alt="result"></p>]]></content>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>爬取淘宝商品列表</title>
      <link href="/2018/04/01/%E6%B7%98%E5%AE%9D%E7%88%AC%E8%99%AB/"/>
      <url>/2018/04/01/%E6%B7%98%E5%AE%9D%E7%88%AC%E8%99%AB/</url>
      <content type="html"><![CDATA[<p>前段时间老师让我爬取淘宝的商品列表以及其商品详情数据，期间遇到了很多问题。最困难的就是淘宝的价格数据是以Ajax异步加载的，这些数据暂时还没有能力获取到。</p><p>下面介绍一下基本思路。</p><a id="more"></a><p>首先，通过抓取商品列表的商品ID获取商品的身份标识，然后根据商品ID跳转到具体的商品列表，对其他属性进行抓取。</p><p>观察两条商品列表的URL：</p><pre><code>https://s.taobao.com/search?q=帽子&amp;imgfile=&amp;commend=all&amp;ssid=s5-e&amp;search_type=item&amp;sourceId=tb.index&amp;spm=a21bo.50862.201856-taobao-item.1&amp;ie=utf8&amp;initiative_id=tbindexz_20170806&amp;bcoffset=7&amp;ntoffset=7&amp;p4ppushleft=1%2C48&amp;s=0https://s.taobao.com/search?q=帽子&amp;imgfile=&amp;commend=all&amp;ssid=s5-e&amp;search_type=item&amp;sourceId=tb.index&amp;spm=a21bo.50862.201856-taobao-item.1&amp;ie=utf8&amp;initiative_id=tbindexz_20170806&amp;bcoffset=4&amp;ntoffset=4&amp;p4ppushleft=1%2C48&amp;s=44https://s.taobao.com/search?q=帽子&amp;imgfile=&amp;commend=all&amp;ssid=s5-e&amp;search_type=item&amp;sourceId=tb.index&amp;spm=a21bo.50862.201856-taobao-item.1&amp;ie=utf8&amp;initiative_id=tbindexz_20170806&amp;bcoffset=4&amp;ntoffset=4&amp;p4ppushleft=1%2C48&amp;s=88</code></pre><p>这是前三个页面的URL，可以发现，除了”q=”和”s=”后面的数据不一样，其他的都是一样的，因此，可以把URL简化为：</p><p><a href="https://s.taobao.com/search?q=keyword&amp;s=pagenum" target="_blank" rel="noopener">https://s.taobao.com/search?q=keyword&amp;s=pagenum</a></p><p>其中，q代表搜索的关键词，s代表商品列表的页数，0代表第一页，44代表第二页，88代表第三页……</p><p>先以此URL抓取商品的ID等信息。</p><p>代码如下。</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> re</span><br><span class="line"><span class="hljs-keyword">import</span> requests</span><br><span class="line"><span class="hljs-keyword">from</span> pandas <span class="hljs-keyword">import</span> *</span><br><span class="line"><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> OrderedDict</span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getDetails</span><span class="hljs-params">(startpage,endpage)</span>:</span></span><br><span class="line"><span class="hljs-comment">#如果需要爬取具体的商品详情，页数过多可能会出现异常，此函数可以用来控制一次爬取的页数</span></span><br><span class="line">url_head=<span class="hljs-string">'https://s.taobao.com/search?q=帽子&amp;s='</span>   </span><br><span class="line"><span class="hljs-comment">#这是淘宝搜索列表的url前面的相同部分，q=''代表搜索的关键词，s=''代表第几页，s=0为第1页s=44为第二页，以此类推</span></span><br><span class="line"></span><br><span class="line">url_list=[url_head+str(i*<span class="hljs-number">44</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(startpage<span class="hljs-number">-1</span>,endpage)]  <span class="hljs-comment">#生成需要爬取的商品列表url</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#定义存储商品列表数据数据的列表</span></span><br><span class="line">nid_list=[]</span><br><span class="line">raw_title_list=[]</span><br><span class="line">view_price_list=[]</span><br><span class="line">view_sales_list=[]</span><br><span class="line">item_loc_list=[]</span><br><span class="line"><span class="hljs-keyword">for</span> url <span class="hljs-keyword">in</span> url_list:</span><br><span class="line">    resp=requests.get(url)</span><br><span class="line">    print(resp.url)</span><br><span class="line">    nid=re.findall(pattern=<span class="hljs-string">'"nid":"(.*?)"'</span>,string=resp.text)  <span class="hljs-comment">#商品id,唯一，可以此跳转到其商品详情页面，然后进行其他信息的抓取</span></span><br><span class="line">    raw_title=re.findall(pattern=<span class="hljs-string">'"raw_title":"(.*?)"'</span>,string=resp.text)  <span class="hljs-comment">#商品名称</span></span><br><span class="line">    view_price=re.findall(pattern=<span class="hljs-string">'"view_price":"(.*?)"'</span>,string=resp.text)  <span class="hljs-comment">#商品价格</span></span><br><span class="line">    view_sales=re.findall(pattern=<span class="hljs-string">'"view_sales":"(.*?)"'</span>,string=resp.text)  <span class="hljs-comment">#商品销量</span></span><br><span class="line">    item_loc=re.findall(pattern=<span class="hljs-string">'"item_loc":"(.*?)"'</span>,string=resp.text)  <span class="hljs-comment">#发货地址</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment">#逐个存储</span></span><br><span class="line">    nid_list.extend(nid)</span><br><span class="line">    raw_title_list.extend(raw_title)</span><br><span class="line">    view_price_list.extend(view_price)</span><br><span class="line">    view_sales_list.extend(view_sales)</span><br><span class="line">    item_loc_list.extend(item_loc)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment">#生成数据框</span></span><br><span class="line">dt=&#123;<span class="hljs-string">'商品id'</span>:nid_list,<span class="hljs-string">'商品名称'</span>:raw_title_list,<span class="hljs-string">'商品价格'</span>:view_price_list,</span><br><span class="line"><span class="hljs-string">'商品销量'</span>:view_sales_list,<span class="hljs-string">'商品发货地址'</span>:item_loc_list&#125;</span><br><span class="line"></span><br><span class="line">df=DataFrame(dt)  <span class="hljs-comment">#根据字典生成数据框</span></span><br><span class="line"><span class="hljs-comment">#写入Excel</span></span><br><span class="line">writer1=ExcelWriter(<span class="hljs-string">"taobao_details.xlsx"</span>)  <span class="hljs-comment">#新建一个空白Excel工作簿</span></span><br><span class="line">df.to_excel(writer1,<span class="hljs-string">"Sheet1"</span>)  <span class="hljs-comment">#将df写入Sheet1工作表</span></span><br><span class="line">writer1.save()</span><br></pre></td></tr></table></figure><p>上述代码可以获取到商品ID、商品名称、商品价格、商品销量、发货地址信息。接下来，利用商品ID信息，跳转到具体的商品详情页面，对其他属性进行抓取。</p><p>对其他属性的抓取，主要着眼于两个方面。一个是商品的描述、服务于物流评分，这个虽然淘宝与天猫的HTML页面有所差别，但是获取方式大同小异。另外，所有的商品详情都放在了一个列表里面，因此用beautifulsoup提取十分方便。</p><p>抓取淘宝商品详情的代码如下：</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getTaoBaoDetails</span><span class="hljs-params">(url)</span>:</span></span><br><span class="line"><span class="hljs-keyword">import</span> requests</span><br><span class="line"><span class="hljs-keyword">import</span> re</span><br><span class="line"><span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="hljs-comment"># from pandas import DataFrame</span></span><br><span class="line"><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> OrderedDict</span><br><span class="line">res=requests.get(url)</span><br><span class="line">soup=BeautifulSoup(res.text,<span class="hljs-string">"html.parser"</span>)</span><br><span class="line"></span><br><span class="line">dd=soup.select(<span class="hljs-string">".tb-shop-rate dd"</span>) <span class="hljs-comment">#获取描述、服务、物流的数字信息，该信息存放在一个列表，需要使用正则表达式提取</span></span><br><span class="line">dd_value=[]</span><br><span class="line"><span class="hljs-keyword">if</span> len(dd)&gt;<span class="hljs-number">0</span>:</span><br><span class="line"><span class="hljs-keyword">try</span>:</span><br><span class="line"><span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,<span class="hljs-number">3</span>):</span><br><span class="line">dd_value.append(re.search(pattern=<span class="hljs-string">r'[\s]*([0-9]\.[0-9])[\s]*'</span>,string=dd[d].text).group(<span class="hljs-number">1</span>))</span><br><span class="line"><span class="hljs-keyword">except</span> IndexError <span class="hljs-keyword">as</span> err:</span><br><span class="line">print(res.url)</span><br><span class="line"><span class="hljs-comment">#下面的语句获取属性列表</span></span><br><span class="line">attrs=soup.select(<span class="hljs-string">".attributes-list li"</span>)</span><br><span class="line">attrs_name=[]</span><br><span class="line">attrs_value=[]</span><br><span class="line"><span class="hljs-keyword">for</span> attr <span class="hljs-keyword">in</span> attrs:</span><br><span class="line">attrs_name.append(re.search(<span class="hljs-string">r'(.*?):[\s]*(.*)'</span>,attr.text).group(<span class="hljs-number">1</span>))</span><br><span class="line">attrs_value.append(re.search(<span class="hljs-string">r'(.*?):[\s]*(.*)'</span>,attr.text).group(<span class="hljs-number">2</span>))</span><br><span class="line"></span><br><span class="line">allattrs=OrderedDict() <span class="hljs-comment">#存放该产品详情页面所具有的属性</span></span><br><span class="line"><span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,len(attrs_name)):</span><br><span class="line">allattrs[attrs_name[k]]=attrs_value[k]</span><br><span class="line"></span><br><span class="line">info=OrderedDict()  <span class="hljs-comment">#存放该商品所具有的全部信息</span></span><br><span class="line"><span class="hljs-comment">#下面三条语句获取描述、服务、物流的评分信息</span></span><br><span class="line"><span class="hljs-keyword">if</span> len(dd_value)&gt;<span class="hljs-number">0</span>:</span><br><span class="line">info[<span class="hljs-string">'描述'</span>]=dd_value[<span class="hljs-number">0</span>]</span><br><span class="line">info[<span class="hljs-string">'服务'</span>]=dd_value[<span class="hljs-number">1</span>]</span><br><span class="line">info[<span class="hljs-string">'物流'</span>]=dd_value[<span class="hljs-number">2</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'描述'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line">info[<span class="hljs-string">'服务'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line">info[<span class="hljs-string">'物流'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#下面的语句用来判断该商品具有哪些属性，如果具有该属性，将属性值插入有序字典，否则，该属性值为空</span></span><br><span class="line"><span class="hljs-comment">#适用场景</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'适用场景'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'适用场景'</span>]=allattrs[<span class="hljs-string">'适用场景'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'适用场景'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#适用对象</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'适用对象'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'适用对象'</span>]=allattrs[<span class="hljs-string">'适用对象'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'适用对象'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#款式</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'款式'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'款式'</span>]=allattrs[<span class="hljs-string">'款式'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'款式'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#尺码</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'尺码'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'尺码'</span>]=allattrs[<span class="hljs-string">'尺码'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'尺码'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#帽顶款式</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'帽顶款式'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'帽顶款式'</span>]=allattrs[<span class="hljs-string">'帽顶款式'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'帽顶款式'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#帽檐款式</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'帽檐款式'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'帽檐款式'</span>]=allattrs[<span class="hljs-string">'帽檐款式'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'帽檐款式'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#檐形</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'檐形'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'檐形'</span>]=allattrs[<span class="hljs-string">'檐形'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'檐形'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#主要材质</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'主要材质'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'主要材质'</span>]=allattrs[<span class="hljs-string">'主要材质'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'主要材质'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#人群</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'人群'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'人群'</span>]=allattrs[<span class="hljs-string">'人群'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'人群'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#品牌</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'品牌'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'品牌'</span>]=allattrs[<span class="hljs-string">'品牌'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'品牌'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#风格</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'风格'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'风格'</span>]=allattrs[<span class="hljs-string">'风格'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'风格'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#款式细节</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'款式细节'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'款式细节'</span>]=allattrs[<span class="hljs-string">'款式细节'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'款式细节'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#颜色分类</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'颜色分类'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'颜色分类'</span>]=allattrs[<span class="hljs-string">'颜色分类'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'颜色分类'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#适用季节</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'适用季节'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'适用季节'</span>]=allattrs[<span class="hljs-string">'适用季节'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'适用季节'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#适用年龄</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'适用年龄'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'适用年龄'</span>]=allattrs[<span class="hljs-string">'适用年龄'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'适用年龄'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">return</span> info</span><br></pre></td></tr></table></figure><p>　　抓取天猫商品详情的代码如下：</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getTmallDetails</span><span class="hljs-params">(url)</span>:</span></span><br><span class="line"><span class="hljs-keyword">import</span> requests</span><br><span class="line"><span class="hljs-keyword">import</span> re</span><br><span class="line"><span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="hljs-comment"># from pandas import DataFrame</span></span><br><span class="line"><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> OrderedDict</span><br><span class="line">res=requests.get(url)</span><br><span class="line">soup=BeautifulSoup(res.text,<span class="hljs-string">"html.parser"</span>)</span><br><span class="line"><span class="hljs-comment"># dt=soup.select(".tb-shop-rate dt") #获取描述、服务、物流的文本信息，该信息存放在一个列表，需要使用正则表达式提取</span></span><br><span class="line">dd=soup.select(<span class="hljs-string">".shop-rate ul li"</span>) <span class="hljs-comment">#获取描述、服务、物流的数字信息，该信息存放在一个列表，需要使用正则表达式提取</span></span><br><span class="line"><span class="hljs-comment"># dt_name=[]</span></span><br><span class="line">dd_value=[]</span><br><span class="line"><span class="hljs-comment"># for t in range(0,3):</span></span><br><span class="line"><span class="hljs-comment"># dt_name.append(dt[t].text)</span></span><br><span class="line"><span class="hljs-keyword">if</span> len(dd)&gt;<span class="hljs-number">0</span>:</span><br><span class="line"><span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> dd:</span><br><span class="line">dd_value.append(re.search(<span class="hljs-string">r'([0-9][.][0-9])'</span>,d.text).group())</span><br><span class="line"><span class="hljs-comment">#下面的语句获取属性列表</span></span><br><span class="line">attrs=soup.select(<span class="hljs-string">'#J_AttrUL li'</span>)</span><br><span class="line">attrs_name=[]</span><br><span class="line">attrs_value=[]</span><br><span class="line"><span class="hljs-keyword">for</span> attr <span class="hljs-keyword">in</span> attrs:</span><br><span class="line">attrs_name.append(re.search(<span class="hljs-string">r'(.*?):[\s]*(.*)'</span>,attr.text).group(<span class="hljs-number">1</span>))</span><br><span class="line">attrs_value.append(re.search(<span class="hljs-string">r'(.*?):[\s]*(.*)'</span>,attr.text).group(<span class="hljs-number">2</span>))</span><br><span class="line"></span><br><span class="line">allattrs=OrderedDict() <span class="hljs-comment">#存放该产品详情页面所具有的属性</span></span><br><span class="line"><span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,len(attrs_name)):</span><br><span class="line">allattrs[attrs_name[k]]=attrs_value[k]</span><br><span class="line"></span><br><span class="line">info=OrderedDict()  <span class="hljs-comment">#存放该商品所具有的全部信息</span></span><br><span class="line"><span class="hljs-comment">#下面三条语句获取描述、服务、物流的评分信息</span></span><br><span class="line"><span class="hljs-keyword">if</span> len(dd_value)&gt;<span class="hljs-number">0</span>:</span><br><span class="line">info[<span class="hljs-string">'描述'</span>]=dd_value[<span class="hljs-number">0</span>]</span><br><span class="line">info[<span class="hljs-string">'服务'</span>]=dd_value[<span class="hljs-number">1</span>]</span><br><span class="line">info[<span class="hljs-string">'物流'</span>]=dd_value[<span class="hljs-number">2</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'描述'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line">info[<span class="hljs-string">'服务'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line">info[<span class="hljs-string">'物流'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#下面的语句用来判断该商品具有哪些属性，如果具有该属性，将属性值插入有序字典，否则，该属性值为空</span></span><br><span class="line"><span class="hljs-comment">#适用场景</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'适用场景'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'适用场景'</span>]=allattrs[<span class="hljs-string">'适用场景'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'适用场景'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#适用对象</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'适用对象'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'适用对象'</span>]=allattrs[<span class="hljs-string">'适用对象'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'适用对象'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#款式</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'款式'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'款式'</span>]=allattrs[<span class="hljs-string">'款式'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'款式'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#尺码</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'尺码'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'尺码'</span>]=allattrs[<span class="hljs-string">'尺码'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'尺码'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#帽顶款式</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'帽顶款式'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'帽顶款式'</span>]=allattrs[<span class="hljs-string">'帽顶款式'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'帽顶款式'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#帽檐款式</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'帽檐款式'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'帽檐款式'</span>]=allattrs[<span class="hljs-string">'帽檐款式'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'帽檐款式'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#檐形</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'檐形'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'檐形'</span>]=allattrs[<span class="hljs-string">'檐形'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'檐形'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#主要材质</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'主要材质'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'主要材质'</span>]=allattrs[<span class="hljs-string">'主要材质'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'主要材质'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#人群</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'人群'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'人群'</span>]=allattrs[<span class="hljs-string">'人群'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'人群'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#品牌</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'品牌'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'品牌'</span>]=allattrs[<span class="hljs-string">'品牌'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'品牌'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#风格</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'风格'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'风格'</span>]=allattrs[<span class="hljs-string">'风格'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'风格'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#款式细节</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'款式细节'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'款式细节'</span>]=allattrs[<span class="hljs-string">'款式细节'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'款式细节'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#颜色分类</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'颜色分类'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'颜色分类'</span>]=allattrs[<span class="hljs-string">'颜色分类'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'颜色分类'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#适用季节</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'适用季节'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'适用季节'</span>]=allattrs[<span class="hljs-string">'适用季节'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'适用季节'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"><span class="hljs-comment">#适用年龄</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">'适用年龄'</span> <span class="hljs-keyword">in</span> attrs_name:</span><br><span class="line">info[<span class="hljs-string">'适用年龄'</span>]=allattrs[<span class="hljs-string">'适用年龄'</span>]</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">info[<span class="hljs-string">'适用年龄'</span>]=<span class="hljs-string">'NA'</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">return</span> info</span><br></pre></td></tr></table></figure><p>最后，将所获取的信息合并在一起，输出为Excel：</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">url_start=<span class="hljs-string">'https://item.taobao.com/item.htm?id='</span></span><br><span class="line">info_df=DataFrame()</span><br><span class="line"><span class="hljs-keyword">for</span> id <span class="hljs-keyword">in</span> nid_list:</span><br><span class="line">url_detail=url_start+str(id)</span><br><span class="line">res=requests.get(url_detail)</span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> isnull(re.search(<span class="hljs-string">'tmall'</span>,res.url)):</span><br><span class="line">detial=getTmallDetails(url_detail)</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">detial=getTaoBaoDetails(url_detail)</span><br><span class="line">detial[<span class="hljs-string">'商品id'</span>]=id</span><br><span class="line">info_df=info_df.append(detial,ignore_index=<span class="hljs-keyword">True</span>)</span><br><span class="line">writer2=ExcelWriter(<span class="hljs-string">"detail.xlsx"</span>)</span><br><span class="line">info_df.to_excel(writer2,<span class="hljs-string">"Sheet1"</span>)</span><br><span class="line">writer2.save()</span><br></pre></td></tr></table></figure><p>上述代码应该与第一部分的代码合并在一起，这样会同时输出两个Excel，如果每10个页面输出一次，则需要对输出的Excel重命名，否则下一次输出会覆盖前一次的数据。</p>]]></content>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>MapReduce计算线性回归的系数</title>
      <link href="/2018/04/01/mapreduce-LR/"/>
      <url>/2018/04/01/mapreduce-LR/</url>
      <content type="html"><![CDATA[<h2 id="先修知识"><a href="#先修知识" class="headerlink" title="先修知识"></a>先修知识</h2><p>设多元线性回归方程的模型为</p><p>$$Y=\beta_0+\beta_1X_1+\beta_2X_2+\cdots+\beta_pX_p $$</p><p>可令$X_0=1$，则模型可写做：</p><p>$$Y=\beta_0X_0+\beta_1X_1+\beta_2X_2+\cdots+\beta_pX_p $$</p><p>表示成矩阵形式为：</p><p>$$Y=\beta X$$</p><a id="more"></a><p>其中，</p><p>$$\beta =<br>\left[\begin{matrix}<br>\beta_0 \<br>\beta_1 \<br>\beta_2 \<br>\vdots \<br>\beta_p<br>\end{matrix}<br>\right]</p><p>X=<br>\left[\begin{matrix}<br>&amp;x_{10} &amp;x_{11} &amp;\cdots &amp;x_{1p} \<br>&amp;x_{20} &amp;x_{21} &amp;\cdots &amp;x_{2p} \<br>&amp;\vdots &amp;\vdots &amp;\ddots &amp;\vdots \<br>&amp;x_{n0} &amp;x_{n1} &amp;\cdots &amp;x_{np}<br>\end{matrix}\right]<br>$$</p><p>则线性回归方程的最小二乘系数估计值为:</p><p>$$\hat{\beta} = (X’X)^{-1}X’Y$$</p><h2 id="编程思路"><a href="#编程思路" class="headerlink" title="编程思路"></a>编程思路</h2><p>利用MapReduce计算系数是，由于输入数据是一行一行进行读取的，因此在计算的时候，不可能直接利用矩阵乘法进行计算。这里，我们假设输入的数据格式为：</p><p>$$x_0\ x_1\ x_2\ \cdots x_p\ y$$</p><p>即</p><p>$$1\  x_1\ x_2\ \cdots x_p\ y$$</p><p>首先，考虑最简单的，计算$X’Y$，即计算</p><p>$$<br>\left[\begin{matrix}<br>&amp;x_{10} &amp;x_{20} &amp;x_{30} &amp;\cdots &amp;x_{n0} \<br>&amp;x_{11} &amp;x_{21} &amp;x_{31} &amp;\cdots &amp;x_{n1} \<br>&amp;x_{12} &amp;x_{22} &amp;x_{32} &amp;\cdots &amp;x_{n2} \<br>&amp;\vdots &amp;\vdots &amp;\vdots &amp;\ddots &amp;\vdots \<br>&amp;x_{1p} &amp;x_{2p} &amp;x_{3p} &amp;\cdots &amp;x_{np}<br>\end{matrix}\right] \times<br>\left[\begin{matrix}<br>        y_1 \<br>        y_2 \<br>        y_3 \<br>        \vdots \<br>        y_n<br>    \end{matrix}\right]=\left[\begin{matrix}<br>        &amp;x_{10}y_1+&amp;x_{20}y_2+&amp;x_{30}y_3+&amp;\cdots+&amp;x_{n0}y_n \<br>        &amp;x_{11}y_1+&amp;x_{21}y_2+&amp;x_{31}y_3+&amp;\cdots+&amp;x_{n1}y_n \<br>        &amp;x_{12}y_1+&amp;x_{22}y_2+&amp;x_{32}y_3+&amp;\cdots+&amp;x_{n2}y_n \<br>        &amp;\vdots &amp;\vdots &amp;\vdots &amp;\ddots &amp;\vdots \<br>        &amp;x_{1p}y_1+&amp;x_{2p}y_2+&amp;x_{3p}y_3+&amp;\cdots+&amp;x_{np}y_n<br>        \end{matrix}\right]<br>$$</p><p>观察右侧矩阵可以发现，$y_1$总是与第一列相乘，$y_2$总是与第二列相乘，……，以此类推。而第一列实际上就是我们读取数据的第一行，第二列是读取数据的第二行……。根据这种规律，我们每读取一行，就让当前的$y$与所有的自变量相乘，然后把所有的结果累加求和，即为我们想要的结果。</p><p>上面已经解决了矩阵自变量矩阵的转置与因变量向量的乘积，即：</p><p>$$part1 = X’Y$$</p><p>那么，矩阵转置与矩阵的乘积$X’X$仍然可以仿照上述的方法进行。</p><p>$$X’X = \left[\begin{matrix}<br>&amp;x_{10} &amp;x_{20} &amp;x_{30} &amp;\cdots &amp;x_{n0} \<br>&amp;x_{11} &amp;x_{21} &amp;x_{31} &amp;\cdots &amp;x_{n1} \<br>&amp;x_{12} &amp;x_{22} &amp;x_{32} &amp;\cdots &amp;x_{n2} \<br>&amp;\vdots &amp;\vdots &amp;\vdots &amp;\ddots &amp;\vdots \<br>&amp;x_{1p} &amp;x_{2p} &amp;x_{3p} &amp;\cdots &amp;x_{np}<br>\end{matrix}\right]<br>\times\left[\begin{matrix}<br>&amp;x_{10} &amp;x_{11} &amp;\cdots &amp;x_{1p} \<br>&amp;x_{20} &amp;x_{21} &amp;\cdots &amp;x_{2p} \<br>&amp;x_{30} &amp;x_{31} &amp;\cdots &amp;x_{2p} \<br>&amp;\vdots &amp;\vdots &amp;\ddots &amp;\vdots \<br>&amp;x_{n0} &amp;x_{n1} &amp;\cdots &amp;x_{np}<br>\end{matrix}<br>\right]<br>$$</p><p>$$=\left[\begin{matrix}<br>&amp;x_{10} &amp;x_{20} &amp;x_{30} &amp;\cdots &amp;x_{n0} \<br>&amp;x_{11} &amp;x_{21} &amp;x_{31} &amp;\cdots &amp;x_{n1} \<br>&amp;x_{12} &amp;x_{22} &amp;x_{32} &amp;\cdots &amp;x_{n2} \<br>&amp;\vdots &amp;\vdots &amp;\vdots &amp;\ddots &amp;\vdots \<br>&amp;x_{1p} &amp;x_{2p} &amp;x_{3p} &amp;\cdots &amp;x_{np}<br>\end{matrix}\right]\times\left[\begin{matrix}<br>x_0, x_1, x_2, \cdots, x_p<br>\end{matrix}\right]<br>$$</p><p>可以把上述既然课程看作是进行了$p+1$次的矩阵转置与向量乘积过程。当读取第一行时，我们可以得到一个矩阵：</p><p>$$<br>step1 = \left[\begin{matrix}<br>        &amp;x_{10}x_{10} &amp;x_{11}x_{10} &amp;\cdots &amp;x_{1p}x_{10} \<br>        &amp;x_{10}x_{11} &amp;x_{11}x_{11} &amp;\cdots &amp;x_{1p}x_{11} \<br>        &amp;x_{10}x_{12} &amp;x_{11}x_{12} &amp;\cdots &amp;x_{1p}x_{12} \<br>        &amp;\vdots &amp;\vdots &amp;\ddots &amp;\vdots                   \<br>        &amp;x_{10}x_{1p} &amp;x_{11}x_{1p} &amp;\cdots &amp;x_{1p}x_{1p}<br>    \end{matrix}\right]$$<br>同理，当读取第二行的时候，同样可以得到一个矩阵：</p><p>$$<br>step2 = \left[<br>    \begin{matrix}<br>        &amp;x_{20}x_{20} &amp;x_{21}x_{20} &amp;\cdots &amp;x_{2p}x_{20} \<br>        &amp;x_{20}x_{21} &amp;x_{21}x_{21} &amp;\cdots &amp;x_{2p}x_{21} \<br>        &amp;x_{20}x_{22} &amp;x_{21}x_{12} &amp;\cdots &amp;x_{2p}x_{12} \<br>        &amp;\vdots &amp;\vdots &amp;\ddots &amp;\vdots                   \<br>        &amp;x_{20}x_{2p} &amp;x_{21}x_{2p} &amp;\cdots &amp;x_{2p}x_{2p}<br>    \end{matrix}\right]$$</p><p>此时，将$step1\ step2$对应元素相加，即得到我们的更新矩阵。迭代下去，最终读取完所在的数据，即为$X’X$的结果。</p><p>当数据量很大的时候，用MapReduce方法进行计算，hadoop会将数据按照block的大小切分成若干块，每一块都执行mapper函数，在reducer里面把所有的mapper结果加起来，最后计算$(X’X)^{-1}X’Y$.</p><h2 id="编程实现"><a href="#编程实现" class="headerlink" title="编程实现"></a>编程实现</h2><p>由于矩阵的输出不容易处理，因此在得到矩阵的时候，可以将其拉长为向量，这样就可以使用标准化输出函数print将计算结果输出，以便于reducer使用。</p><p>mapper函数如下：</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#! /usr/bin/anaconda2/bin/python</span></span><br><span class="line"><span class="hljs-comment"># -*- coding:UTF-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">import</span> sys</span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">read_input</span><span class="hljs-params">(file)</span>:</span></span><br><span class="line">    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> file:</span><br><span class="line">        <span class="hljs-keyword">yield</span> line.strip()</span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">matmulti</span><span class="hljs-params">()</span>:</span></span><br><span class="line">    </span><br><span class="line">    input = read_input(sys.stdin)</span><br><span class="line"></span><br><span class="line">    innerLength = <span class="hljs-number">0</span></span><br><span class="line">    length = <span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> input:</span><br><span class="line">        fields = line.split(<span class="hljs-string">","</span>)</span><br><span class="line">        <span class="hljs-keyword">if</span> innerLength == <span class="hljs-number">0</span>:</span><br><span class="line">            innerLength = len(fields) - <span class="hljs-number">1</span></span><br><span class="line">            data1 = np.array([<span class="hljs-number">0.0</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(innerLength)])</span><br><span class="line">        temp = np.array(fields,float)[:innerLength]*float(fields[innerLength])</span><br><span class="line">        data1 = data1 + temp</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-keyword">if</span> length == <span class="hljs-number">0</span>:</span><br><span class="line">            length = len(fields) - <span class="hljs-number">1</span></span><br><span class="line">            data2 = np.diag(np.zeros(length))</span><br><span class="line">        <span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> range(length):</span><br><span class="line">            data2[index] += np.array(fields[:length],dtype = float)*float(fields[index])</span><br><span class="line">            </span><br><span class="line">    <span class="hljs-keyword">return</span> data1,data2</span><br><span class="line"></span><br><span class="line">data1,data2 = matmulti()</span><br><span class="line">data1 = list(data1)</span><br><span class="line">length = len(data2)</span><br><span class="line">data2 = data2.reshape(<span class="hljs-number">1</span>,length**<span class="hljs-number">2</span>)</span><br><span class="line">data2 = list(data2[<span class="hljs-number">0</span>])</span><br><span class="line">data = data1 + data2</span><br><span class="line">print(<span class="hljs-string">"\t"</span>.join(str(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> data))</span><br></pre></td></tr></table></figure><p>reducer函数如下：</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#! /usr/bin/anaconda2/bin/python</span></span><br><span class="line"><span class="hljs-comment"># -*- coding:UTF-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">import</span> sys</span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">read_input</span><span class="hljs-params">(file)</span>:</span></span><br><span class="line">    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> file:</span><br><span class="line">        <span class="hljs-keyword">yield</span> line.strip()</span><br><span class="line"></span><br><span class="line">input = read_input(sys.stdin)</span><br><span class="line"></span><br><span class="line">length = <span class="hljs-number">0</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> input:</span><br><span class="line">    fields = line.split(<span class="hljs-string">"\t"</span>)</span><br><span class="line">    <span class="hljs-keyword">if</span> length == <span class="hljs-number">0</span>:</span><br><span class="line">        length = len(fields)</span><br><span class="line">        data = np.array([<span class="hljs-number">0.0</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(length)])</span><br><span class="line">    fields = np.array(fields,dtype = float)</span><br><span class="line">    data += fields</span><br><span class="line"></span><br><span class="line">lenght = len(data)</span><br><span class="line">varnums = int((<span class="hljs-number">-1</span>+math.sqrt(<span class="hljs-number">1</span>+<span class="hljs-number">4.0</span>*length))/<span class="hljs-number">2.0</span>)</span><br><span class="line"></span><br><span class="line">part1 = np.mat(data[:varnums])</span><br><span class="line">part1 = part1.T</span><br><span class="line"></span><br><span class="line">part2 = data[varnums:]</span><br><span class="line">part2 = part2.reshape(varnums,varnums)</span><br><span class="line">part2 = np.mat(part2)</span><br><span class="line">part2 = part2.I</span><br><span class="line"></span><br><span class="line">result = part2*part1</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">print</span> result</span><br></pre></td></tr></table></figure><p>用R随机生成一份样本量为100万的数据，R回归结果为：</p><figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Coefficients:</span><br><span class="line">              Estimate Std. Error t value Pr(&gt;|t|)    </span><br><span class="line">(Intercept) <span class="hljs-number">19.9995945</span>  <span class="hljs-number">0.0602316</span> <span class="hljs-number">332.045</span>   &lt;<span class="hljs-number">2e-16</span> ***</span><br><span class="line">V2          -<span class="hljs-number">0.0012168</span>  <span class="hljs-number">0.0009998</span>  -<span class="hljs-number">1.217</span>   <span class="hljs-number">0.2236</span>    </span><br><span class="line">V3          -<span class="hljs-number">0.0018043</span>  <span class="hljs-number">0.0009990</span>  -<span class="hljs-number">1.806</span>   <span class="hljs-number">0.0709</span> .  </span><br><span class="line">V4           <span class="hljs-number">0.0017635</span>  <span class="hljs-number">0.0009990</span>   <span class="hljs-number">1.765</span>   <span class="hljs-number">0.0775</span> .  </span><br><span class="line">V5          -<span class="hljs-number">0.0021161</span>  <span class="hljs-number">0.0009987</span>  -<span class="hljs-number">2.119</span>   <span class="hljs-number">0.0341</span> *  </span><br><span class="line">V6          -<span class="hljs-number">0.0002982</span>  <span class="hljs-number">0.0009997</span>  -<span class="hljs-number">0.298</span>   <span class="hljs-number">0.7655</span>    </span><br><span class="line">V7           <span class="hljs-number">0.0008608</span>  <span class="hljs-number">0.0009998</span>   <span class="hljs-number">0.861</span>   <span class="hljs-number">0.3892</span>    </span><br><span class="line">V8           <span class="hljs-number">0.0007678</span>  <span class="hljs-number">0.0009995</span>   <span class="hljs-number">0.768</span>   <span class="hljs-number">0.4423</span>    </span><br><span class="line">V9           <span class="hljs-number">0.0009089</span>  <span class="hljs-number">0.0009991</span>   <span class="hljs-number">0.910</span>   <span class="hljs-number">0.3630</span>    </span><br><span class="line">V10          <span class="hljs-number">0.0009761</span>  <span class="hljs-number">0.0009991</span>   <span class="hljs-number">0.977</span>   <span class="hljs-number">0.3286</span></span><br></pre></td></tr></table></figure><p>MapReduce的输出结果为：</p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[[  <span class="hljs-number">1.99995945e+01</span>]</span><br><span class="line"> [ <span class="hljs-number">-1.21684122e-03</span>]</span><br><span class="line"> [ <span class="hljs-number">-1.80428969e-03</span>]</span><br><span class="line"> [  <span class="hljs-number">1.76352620e-03</span>]</span><br><span class="line"> [ <span class="hljs-number">-2.11610460e-03</span>]</span><br><span class="line"> [ <span class="hljs-number">-2.98208998e-04</span>]</span><br><span class="line"> [  <span class="hljs-number">8.60846240e-04</span>]</span><br><span class="line"> [  <span class="hljs-number">7.67848910e-04</span>]</span><br><span class="line"> [  <span class="hljs-number">9.08866936e-04</span>]</span><br><span class="line"> [  <span class="hljs-number">9.76052909e-04</span>]]</span><br></pre></td></tr></table></figure><p>二者的计算结果是一样的。</p>]]></content>
      
      
        <tags>
            
            <tag> MapReduce </tag>
            
            <tag> 线性回归 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Introduction to ETL</title>
      <link href="/2018/03/30/Introduction-to-ETL/"/>
      <url>/2018/03/30/Introduction-to-ETL/</url>
      <content type="html"><![CDATA[<a id="more"></a>]]></content>
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ETL </tag>
            
            <tag> 数据仓库 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Word2Vec-语言模型的前世今生</title>
      <link href="/2018/03/27/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
      <url>/2018/03/27/%E8%AF%8D%E5%90%91%E9%87%8F/</url>
      <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p><img src="/picture/feeding.jpg" alt="picture"></p><p>在机器学习领域，语言识别和图像识别都比较容易做到。语音识别的输入数据可以是音频频谱序列向量所构成的matrix，图像识别的输入数据是像素点向量构成的矩阵。但是文本是一种抽象的东西，显然不能直接把文本数据喂给机器当做输入，因此这里就需要对文本数据进行处理。</p><p>现在，有这么一个有趣的例子，我接下来要讲的模型就可以做到。</p><a id="more"></a><ul><li>首先给出一个例子，Paris - France + America = ?</li></ul><p>从我们人的角度来看,Paris是法国的首都，那么首都减去国家再加上一个国家，很可能表示的就是另一个国家的首都。因此这里的结果就是华盛顿Washington.机器想做到这一点，并不容易。</p><ul><li><p>众所周知，只有标量或者向量可以应用加减法，抽象的自然语言该如何做到呢？</p></li><li><p>一个很自然的想法就是，自然语言能否表示成数学的形式，这样就可以更加方便地研究其规律了。</p></li><li><p>答案是肯定的。</p></li></ul><p>现在我们可以进行思考，如何将文本中的词语用数学的形式表达出来,也就是说，文本中藏着哪些数学形式需要我们去挖掘。</p><ol><li><p>文本中各个词语出现的频数是有限的，这是一个可以提取的数学形式</p></li><li><p>从逻辑的角度出发，词语之间不可能是独立的，一个词语的出现肯定与另一个或者若干个词语有关系。这就涉及到词语共现的层面了。</p></li></ol><p>统计语言模型和大多数的词向量表示都是基于以上两点考虑的。</p><p>词向量的表现形式主要分为两种，一种是one-hot(one-hot representation)表示方式，将词表示成一个很长的向量，向量的长度就是词典的长度；另一种表示方法是分布式表示(distributed representation).同时，分布式表示方法又可以分为基于矩阵的表示方法、基于聚类的表示方法和基于神经网络的表示方法。</p><p>首先，最简单的就是one-hot表示方法，将词表示成一个很长的向量，向量的分量只有一个1，其他全为0，1所对应的位置就是该词在词汇表中的索引。</p><p>这样表示有两个缺点：</p><ol><li><p>容易受维度灾难(the curse of dimentionality)的困扰；</p></li><li><p>没有考虑到词之间的关系(similarity)。</p></li></ol><p>现在主要应用的都是分布式表示形式了。下面介绍一种简单的分布式表示形式——基于矩阵的表示形式。</p><p>如下表所示：</p><table><thead><tr><th>Probability and Ratio</th><th>k = solid</th><th>k = gas</th><th>k = water</th><th>k = fashion</th></tr></thead><tbody><tr><td>$p(k\vert ice)$</td><td>$1.9\times 10^{-4}$</td><td>$6.6\times 10^{-5}$</td><td>$3.0\times 10^{-3}$</td><td>$1.7\times 10^{-5}$</td></tr><tr><td>$p(k\vert steam$)</td><td>$2.2\times 10^{-5}$</td><td>$7.8\times 10^{-4}$</td><td>$2.2\times 10^{-3}$</td><td>$1.8\times 10^{-5}$</td></tr><tr><td>$p(k\vert ice)/(p(k\vert steam)$</td><td>$8.9$</td><td>$8.5\times 10^{-2}$</td><td>$1.36$</td><td>$0.96$</td></tr></tbody></table><p>简单说一下上面的矩阵。</p><p>假设我们对一些热力学短语或者词语的概念感兴趣，我们选择i=ice，k=steam，我们想看看ice和steam的关系，可以通过他们与其他词语的共现频率来研究。这些其他词语我们称之为探测词。这里我们选择探测词k为solid，gas，water和fashion。显然，ice与solid的相关性较高，但是与steam相关性较低，因此我们期望看到的是$p_{ik}/p_{jk}$比值比较大。对于探测词gas，我们期望看到的是$p_{ik}/p_{jk}$比值比较小。而water和fashion与ice和steam的关系要么都十分密切，要么都不怎么密切，因此对于这两个探测词，$p_{ik}/p_{jk}$应该接近于1.</p><p>上表是基于一个很大的语料库统计得出的，符合我们的预期。相比于单独使用原始概率，概率比值可以更好的区分相关词语和不相关词语，比如solid和gas与water和fashion；也可以很容易区分两个相关词。</p><p>那么，在正式介绍自然语言处理，或者说wrod2vec之前，有必要介绍以下统计语言模型。它是现在所有语言模型的基础。</p><p>第二个需要讲的分布式表示方式是基于神经网络的表示方法。在此之前，有必要讲一下传统的统计语言模型，毕竟它对语言模型影响深远。</p><hr><h2 id="统计语言模型"><a href="#统计语言模型" class="headerlink" title="统计语言模型"></a>统计语言模型</h2><h3 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h3><p>给出以下三个句子：</p><pre><code>美联储主席本·伯南克昨天告诉媒体7000亿美元的救助资金将借给上百家银行、保险公司和汽车公司美主席联储本·伯南克告诉昨天媒体7000亿美元的资金救助将借给百上家银行、保险公司和汽公车司美主车席联储本·克告诉昨天公司媒体7000伯南亿美行元的金将借给百救助上家资银、保险公司和汽</code></pre><p>对于第一个句子，语句通畅，意思也很不明白；对于第二个句子，虽然个别词语调换了位置，但也不影响阅读，我们仍然能够知道表达的是什么意思；对于第三个句子，我们就很难知道具体表示什么意思了。</p><p>如果问你为什么第三个句子不知道表达什么，你可能会说句子混乱，语义不清晰。在上个世纪70年代的时候，科学家们也是这样想的，并且试图让计算机去判断一个句子的语义是否清晰，然而，这样的方法是走不通的。</p><p><strong>贾里尼克</strong>想到了一种很好的统计模型来解决上述问题。判断一个句子是否合理，只需要看它在所有句子中出现的概率就行了。第一个句子出现的概率大概是$10^{-20}$,第二个句子出现的概率大概是$10^{-25}$，第三个句子出现的概率大概是$10^{-70}$，第一个句子出现的可能性最大，因此这个句子最为合理。</p><p>那么，如何计算一个句子出现的概率呢，我们可以把有史以来人类说过的话都统计一遍，这样就能很方便的计算概率了。然而，你我都知道这条路走不通。</p><p>假设想知道S在文本中出现的可能性，也就是数学上所说的S的概率，既然$S=w_1,w_2,…,w_n$,那么不妨把S展开表示，</p><p>$$P(S)=P(w_1,w_2,…,w_n)$$</p><p>利用条件概率的公式，S这个序列出现的概率等于每一个词出现的条件概率的乘乘积，展开为：</p><p>$$<br>P(w_1,w_2,…,w_n)=P(w_1)P(w_2\vert w_1)P(w_3\vert w_1,w_2)\cdots P(w_n\vert w_1,w_2,\cdots w_{n-1})<br>$$</p><p>计算$P(w_1)$很容易，$P(w_2\vert w_1)$也还能算出来，$P(w_3\vert w_1,w_2)$已经非常难以计算了。</p><h3 id="2-偷懒的马尔科夫-Andrey-Markov"><a href="#2-偷懒的马尔科夫-Andrey-Markov" class="headerlink" title="2. 偷懒的马尔科夫(Andrey Markov)"></a>2. 偷懒的<strong>马尔科夫</strong>(Andrey Markov)</h3><p>假设上面的n不取很长，而只取2个，那么就可以大大减少计算量。即在此时，假设一个词$w_i$出现的概率只与它前面的$w_{i-1}$有关，这种假设称为1阶马尔科夫假设。</p><p>现在，S的概率就变得简单了：<br>$$<br>P(w_1,w_2,…,w_n) \approx P(w_1)P(w_2\vert w_1)<br>$$ </p><p>那么，接下来的问题就变成了估计条件概率$P(w_i\vert w_{i-1})$,根据它的定义，</p><p>$$<br>P(w_i\vert w_{i-1}) = \frac{P(w_i,w_{i-1})}{P(w_{i-1})}<br>$$<br>,</p><p>当样本量很大的时候，基于大数定律，一个短语或者词语出现的概率可以用其频率来表示，即<br>$$<br>P(w_i,w_{i-1})\approx \frac{count(w_i,w_{i-1})}{count(*)}<br>$$</p><p>$$<br>P(w_{i-1}) \approx \frac{count(w_{i-1})}{count(*)}<br>$$</p><p>其中，$count(i)$表示词$i$出现的次数，$count$表示语料库的大小。</p><p>那么</p><p>$$<br>P(w_i\vert w_{i-1}) = \frac{P(w_i,w_{i-1})}{P(w_{i-1})} \approx \frac{count(w_i,w_{i-1})}{count(w_{i-1})}<br>$$</p><h3 id="3-高阶语言模型"><a href="#3-高阶语言模型" class="headerlink" title="3. 高阶语言模型"></a>3. 高阶语言模型</h3><p>在前面的模型中，每个词只与前面1个词有关，和更前面的词就没有关系了，这似乎简单的有点过头了。那么，假定每个词$w_i$都与前面的N-1个词有关，而与更前面的词无关，这样，当前词的概率只取决于前面N-1个词的联合概率，即</p><p>$$<br>P(w_1\vert w_1,w_2,\cdots w_{i-1}) \approx P(w_1\vert w_{i-N+1},w_{i-N+2},\cdots w_{i-1})<br>$$<br>,</p><p>上面这种假设被称为n-1阶马尔科夫假设，对应的模型称为N元模型。N=2就是二元模型，N=1其实就是上下文无关的模型，基本不怎么使用。</p><p>上面的模型看起来已经很完美了，但是考虑以下两个问题，对于二元模型：</p><p>如果此时$count(w_i,w_{i-1})=0$，是否可以说$P(w_i\vert w_{i-1})=0$ ?</p><p>如果此时$count(w_i,w_{i-1})=count(w_{i-1})$，是否可以说$P(w_i\vert w_{i-1})=1$ ?</p><p>显然，不能这么武断。</p><p>但是，实际上上述两种情况肯定是会出现的，尤其是语料足够大的时候，那么，我们怎么解决上述问题呢？</p><p>古德和图灵给出了一个很漂亮的重新估计概率的公式，这个公式后来被称为古德-图灵估计。</p><p>古德图灵的原理是：</p><pre><code>对于没有看见的事件，我们不能认为他发生的概率就是0，因此从概率的总量中，分配一个很小的比例给这些没有看见的事件。这样一来，看见的那些事件的概率就要小于1了，因此，需要将所有看见的事件的概率调小一点。至于小多少，要根据“越是不可信的统计折扣越多”的方法进行。</code></pre><p>假定在语料库中出现$r$次的词有$N_r$个，特别的，未出现的词数量为$N_0$，语料库大小为$N$，那么，很显然，</p><p>$$<br>N = \sum _{r=1}^ \infty rN_r<br>$$<br>,</p><p>出现$r$次的词在整个语料库中的相对频度则是${rN_r}/{N}$，如果不做任何处理，这个相对频度作为这些词的概率。但是当$r$比较小的时候，统计上可能不可靠，因此需要使用一个更小的次数$d_r$来表示，古德-图灵按照如下公式计算$d_r$:</p><p>$$<br>d_r=(r+1)\cdot N_{r+1}/N_r<br>$$<br>,</p><p>显然</p><p>$$<br>\sum_r d_r\cdot N_r = \sum_r (r+1)\cdot N_{r+1} = N<br>$$<br>,</p><p>此时，</p><p>$$<br>d_0 = (0+1)\cdot N_1/N_0 = \frac{N_1}{N_0} &gt; 0<br>$$</p><p>在实际处理的时候，一般对出现次数超过某个阈值的词，频率不下调。</p><p>基于这种思想，估计二元模型概率的公式如下：</p><p>$$<br>P(w_i\vert w_{i-1})=<br>\begin{cases}<br>f(wi\vert w_{i-1}) &amp; {if\quad count(w_{i-1},w_i)} \ge T \<br>f_{gt}(w_i\vert w_{i-1})&amp; if\quad 0 \le count(w_{i-1},w_i) &lt; T \<br>Q(w_{i-1})\cdot f(w_i) &amp; otherwise<br>\end{cases}<br>$$<br>,</p><p>其中, $f(\cdot)$ 表示相对频度，即频率。</p><p>$$<br>Q(w_{i-1})=\frac{1-\sum_{w_i\quad seen}P(w_i \vert w_{i-1})}{\sum _{wi\quad unseen}f(w_i)}<br>$$</p><p>上面这种方法称为卡茨退避法。</p><p>n-gram模型的作用就是，基于语料库计算出各种词串出现的概率，遇到一个句子的时候，可以直接使用上面所计算的概率，把所有的概率连乘，就得到了整个句子的概率。</p><h3 id="4-机器学习的思想"><a href="#4-机器学习的思想" class="headerlink" title="4. 机器学习的思想"></a>4. 机器学习的思想</h3><p>机器学习的套路是，对所研究的问题建模，构造一个目标函数，然后优化参数，最后用这个目标函数进行预测。</p><p>对于统计语言模型，常使用最大对数似然作为目标函数，即</p><p>$$<br>L = \sum_{w\in C}logP(w\vert context(w))<br>$$<br>,</p><p>在n-gram模型中，$context(w_i) = (w_{i-n+1},w_{i-n+2},\cdots,w_{i-1})$,</p><p>由此可见，概率$P(w_i\vert context(w_i))$是关于$w$和$context(w)$的函数，即</p><p>$$<br>P(w\vert context(w)) = F(w,context(w),\theta)<br>$$<br>,</p><p>一旦$F$确定下来了，任何概率都可以使用这个函数进行计算了。</p><p><strong>似乎到这里，我们仍然不知道词向量是什么，因为n-gram模型中根本没有用到词向量。那么，接下来将要介绍的神经网络语言模型则是实实在在用到了词向量。之所以提到统计语言模型，是因为它是其他语言模型的基础，我们得知道语言模型是干嘛的，然后再对语言模型进行展开。</strong></p><hr><h2 id="神经网络语言模型"><a href="#神经网络语言模型" class="headerlink" title="神经网络语言模型"></a>神经网络语言模型</h2><p>上面介绍的n-gram模型相信我们已经十分清楚了，但是n-gram模型的一个突出的确定就是，n的设置不宜过大，n从2到3提升效果显著，但是从3-4提升的效果就没那么好了。而且随着n的增大，参数的数量是以几何形势增长的。</p><p>因此，n-gram模型只能提取某个词前面两到三个词的信息，而不能提取更多的信息了。然而很明显的是，整文本序列中，包含更前面的词能够提供比仅仅2到3个词更多的信息，这也是神经网络语言模型着重要解决的问题之一。</p><p>神经网络模型主要在以下两点上寻求更大的进步：</p><ol><li><p>n-gram模型没有考虑上下文中更多的词提供的信息</p></li><li><p>n-gram模型没有考虑词与此之间的相似性。</p></li></ol><p><strong>举个栗子：</strong></p><p>如果在一个语料库中，”the cat is walking in the bedroom”出现了5000次，而”a dog is running in the room”只出现了5次，n-gram模型得出的结果是前面一个句子的可能性会比后面一个句子大得多。但是实际上，这两个句子是相似的，他们在真实的情况下出现的概率也应该是相仿的。</p><p>神经网络语言模型可以概括为以下三点：</p><ol><li><p>将词汇表中的每个词表示成一个在m维空间里的实数形式的分布式特征向量</p></li><li><p>使用序列中词语的分布式特征向量来表示连接概率函数</p></li><li><p>同时学习特征向量和概率函数的参数</p></li></ol><p>特征向量表示词的不同特征：每一个词都是向量空间内的一个点。特征的个数通常都比较小，比如30，60或者100，远远小于词汇表的长度。概率函数是在给定一个词前面的若干词的情况下，该词出现的条件概率。调整概率函数的参数，使得训练集的对数似然达到最大。每个词的特征向量是通过训练得到的，也可以用先验知识进行初始化。</p><ul><li><p>训练集是词序列$W_1,W_2,…,W_T,W_T\in V$,$V$是词汇表，是一个很大但是有限的集合。</p></li><li><p>目标是找到一个好的模型，使得$f(W_t,…,W_{t-n+1})=\hat{P}(W_t|W_1^{n-1})$</p></li><li><p>唯一的约束条件是$\sum <em>{i=1}^{|V|}f(i,W</em>{t-1},…,W_t-n+1)=1,f&gt;0$</p></li></ul><p><img src="/picture/nnlm.png" alt="photo"> </p><p>我们将函数$f(W_t,…,W_{t-n+1})$分解为以下两个部分：</p><ol><li><p>映射$C$，将$V$中的所有元素映射为真实的向量$C(i)\in R^m$，$C(i)$代表词汇表中的每个词的分布式特征向量，实际上，$C$是一个由自由参数构成的$|V|\times m$矩阵。其中$|V|$代表词汇表的大小，也就是词汇表中的词数量。</p></li><li><p>每个词的概率函数是由$C$来表示的：函数$g$将输入的词特征向量$(C(W_{t-n+1}),…,C(W_{t-1}))$映射为词$W_t$前面$n-1$个词的条件概率分布。</p></li></ol><p>$$<br>f(i,W_{t-1},…,W_{t-n+1})=g(i,C(W_{t-1}),…,C(W_{t-n+1}))$$</p><p>函数$f$是由两个映射组成的($C&amp;g$),$C$是所有词共享的。每一个部分都与一些参数有关。</p><p>映射$C$的参数实际上就是特征向量本身，表示为一个$|V|\times m$矩阵，矩阵的每一行代表词$i$的特征向量$C(i)$.</p><p>函数$g$的参数是$\omega$，所有的参数就是$\theta = (C,\omega)$.</p><p>当寻找到使得带惩罚项的训练语料库的对数似然率最大的$\theta$，那么训练完成。</p><p>$$<br>L = \frac{1}{T}\sum <em>t log f(W_t,W</em>{t-1},…,W_t-n+1)+R(\theta)<br>$$</p><p>$R(\theta)$是惩罚项，只作用于神经网络的权重和矩阵$C$。自由参数的规模是$V$的线性函数，也是$n$的线性函数。</p><p>在下面的大多数试验中，神经网络只有一个隐藏层，外加一个映射层。还有一个可选的直连层。所以说实际上有两个隐藏层，但是由于影射层只是做了一个线性变换，并没有添加新的信息，因此不能视为真正的隐藏层。所以真正的隐藏层就只有一个。</p><p>从图中可以看出，最底层实际上就是一些单一的词，表示成one-hot形式，即长度为词汇表的长度。然后，每个one-hot向量分别与投影矩阵C相乘，则原来长度为$|V|$的one-hot向量，经过线性变换以后，缩短为一个长度为$m$的向量，其中m就是我们设置的特征的个数，一般在2个数量级。投影完成以后，将所有的特征向量按照顺序首尾相连，形成一个长度为$m(n-1)$的向量，以词向量作为隐藏层的输入，隐藏层的激活函数为双曲正切函数$tanh$。输出层接受隐藏层的输出作为输入，经过一个softmax函数进行转换，得到最终的输出P.</p><p>即</p><p>$$<br>\hat{P}(w_t\vert w_{t-1},…,w_{t-n+1})=\frac{e^{y_{w_t}}}{\sum_i e^{y_i}}<br>$$<br>,</p><p>其中</p><p>$$<br>y = b+Wx + Utanh(d+Hx)<br>$$</p><p>双曲正切函数逐个应用于隐藏层的各个单元。当没有直连的时候，$W=0$，$x$是首尾相连的特征向量：</p><p>$$<br>x = (C(w_{t-1}),C(w_{t-2}),…,C(w_{t-n+1}))<br>$$</p><p>令$h$是隐藏层的单元数，$m$是特征向量的长度，当没有直连的时候，$W=0$。那么，所有的自由参数就是：</p><ul><li><p>输出层的偏置$b$，长度为|V|</p></li><li><p>隐藏层的偏置$d$，长度为$h$</p></li><li><p>隐藏层到输出层的权重矩阵$U$,是一个$|V|\times h$矩阵</p></li><li><p>词向量到输出层的权重矩阵$W$，是一个$|V|\times (n-1)m$矩阵</p></li><li><p>隐藏层权重$H$，是一个$h\times (n-1)m$矩阵</p></li><li><p>特征矩阵$C$，是一个$|V|\times m$矩阵</p></li></ul><p><strong>此时的输出$y$实际上是一个长度为$|V|$的向量，那么分量$y_{wt}$不能表示给出前(n-1)个词的情况下$w_t$的概率，因此需要进行一次归一化。</strong></p><p>那么，我们所有的参数如下：</p><p>$$<br>\theta = (b,d,W,U,H,C)<br>$$</p><p>主要的计算量都集中在隐藏层到输出层的以及输出层的归一化计算。</p><p>使用随机梯度下降进行参数求解：</p><p>$$<br>\theta \leftarrow \theta + \epsilon\frac{\partial log \hat{P}(w_t\vert w_{t-1},w_{t-2},w_{t-n+1})}{\partial \theta}<br>$$</p><p>其中，$\epsilon$是学习率。</p><p>当训练结束以后，矩阵$C$就是我们需要的词向量，每一行代表该位置的词的向量。得到了词向量，就可以进行许多有趣的分析。</p><p>比如：</p><ul><li><p>文本聚类</p></li><li><p>计算文本相似度</p></li><li><p>其他应用</p></li></ul><hr><h2 id="Skip-Gram-模型"><a href="#Skip-Gram-模型" class="headerlink" title="Skip-Gram 模型"></a>Skip-Gram 模型</h2><p>基本版的skip-gram模型是十分简单的。我们将会训练一个只有一个隐藏层的简单的神经网络来完成我们的任务，但是当神经网络训练完成以后，我们实际上并不使用这个网络做什么，而是获得隐藏层的权重矩阵，这个矩阵实际上就是我们需要的词向量(word vectors)</p><p>skip-gram模型需要完成这样的工作：给定一个词，预测其周围的词，或者说在其附近的词。这个神经网络将会计算出我们从词汇表中选出的每个“候选”邻居词的概率。</p><p>这里的附近需要说明一下，实际上在算法里有一个”window size”参数，用来控制窗口大小，如果选择参数的值为5，则会预测该词前后各5个词的概率。</p><p>skip-gram模型的输入需要经过特殊的调整，不同于上述神经网络语言模型，应该首先将这里的语料整理成词对(word-pair)。</p><p>比如对于语句”The quick brown fox jumps over the lazy dog.”此处我们选择window size = 2.</p><p><img src="/picture/3.png" alt="input of skip gram"></p><p>具体操作如图所示：</p><ul><li><p>对于单词”The”，取其前后两个词与其凑成词对，这里”The”前面没有单词，因此取后面两个，凑成两个词对，分别是(the,quick),(the,brown)。</p></li><li><p>对于单词quick，其前面有1个单词，后面有2个单词，可以凑成3对，分别为：(quick,the),(quick,brown),(quick,fox)。</p></li><li><p>对于单词brown，其前面有两个单词，后面有两个单词，可以凑成4对，分别是：(brown,the),(brown,quick),(browm,fox),(brown,fox)。</p></li></ul><p>以此类推，可以把语料库中的所有文本调整成上述词对。</p><p>所有的词对都应该是(input,output)形式。</p><p>有了词对，接下来看一下skip-gram的最简单的模型长什么样子。</p><p><img src="/picture/4.png" alt="skip-gram  diagram"></p><p>在上图中，可以清晰地看出，skip-gram模型是一个简单的神经网络，有一个隐藏层（实际上在作者的论文中是以投影层的形式表述的），该隐藏层并没有对应的激活函数。</p><p>输入数据仍然是one-hot向量，向量只有一个分量是1，其他全为0.向量的长度为词汇表的长度。</p><p>对下面所要用的符号进行说明：</p><ul><li><p>$i$：输入的one-hot向量，长度为|V|</p></li><li><p>$|V|$：词汇表的长度</p></li><li><p>$P$：输入层到投影层的$|V|\times m$权重矩阵</p></li><li><p>$W$：投影层到输出层的$m\times |V|$权重矩阵</p></li><li><p>$y$：输出层输出结果</p></li></ul><p>由于这个神经网络没有激活函数，因此看起来比较简单。</p><p>神经网络的计算过程如下：</p><ul><li><p>首先，将输入词向量(one-hot)投影到隐藏层(投影层)，即$$<br>i^T\times P=1\times |V|\times |V| \times m=1\times m<br>$$</p></li><li><p>将隐藏层的结果乘以隐藏层到输出层的权重矩阵$W$，即$$<br>1\times m \times m\times |V|=1\times |V|<br>$$</p></li><li><p>将输出层的输出结果进行softmax归一化，即</p></li></ul><p>$$y=\frac{e^{y}}{\sum_{i\in y}e^i }$$</p><p>此时$y$是一个$1\times |V|$向量，向量的每个分量代表给定词$w_i$的情况下，其相邻词是词汇表中对应词的概率。将$y$与output的one-hot向量相乘，就可以得到给定$w_i$的情况下，其相邻词是output的概率，即</p><p>$$<br>P(w_o\vert w_i)=y\times w_o<br>$$<br>,</p><p>其中，$w_o$是词output的向量形式。</p><p><strong>以上矩阵相乘只是为了说明维度变化，并不是实际上的矩阵乘法。</strong></p><p>用矩阵说明如下：</p><p>假定我们的词库大小为10，输出层到投影层的权重矩阵为$10\times 4$矩阵。</p><p>我们的输入样本是词对(brown,quick).其中，brown的one-hot表示形式如图中的蓝色向量所示，quick的one-hot表示形式如图中的浅绿色向量所示。</p><ul><li><p>蓝色向量是我们的输入向量，即one-hot向量</p></li><li><p>黄色矩阵是输入层到投影层的权重矩阵</p></li><li><p>绿色向量是输入向量与第一个权重矩阵的向量乘积</p></li><li><p>棕色矩阵是投影层到输出层的权重矩阵</p></li><li><p>灰色向量是绿色向量与棕色矩阵的矩阵乘积</p></li><li><p>红色向量是对灰色向量进行了一次$softmax$归一化计算</p></li><li><p>浅绿色矩阵是词对中的输出词对应的one-hot向量</p></li><li><p>深红色数值是我们计算出的输出词对应的概率</p></li></ul><p><strong>skip-gram的目标是对于训练样本$w_1,w_2,\cdots,w_T$,最大化如下平均对数似然概率：</strong></p><p><img src="/picture/5.jpg" alt="matrix2"></p><p>$$<br>\frac{1}{T}\sum <em>{t=1}^T\sum</em>{-c\le i\le c,j\ne 0}log p(w_{t+j}\vert w_t)<br>$$</p><p>实际上，以上模型是难以实现的，因为计算 $\nabla logp(w_O\vert w_I)$的代价随着$W$的增大而增大(W表示词汇表的长度),经常达到$10^5-10^7$数量级。</p><h2 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h2><h3 id="哈夫曼树"><a href="#哈夫曼树" class="headerlink" title="哈夫曼树"></a><strong>哈夫曼树</strong></h3><p>哈夫曼树是一种最优二叉树，它是这样定义的：</p><pre><code>给定n个权值作为n个叶子结点，构造一棵二叉树，若带权路径长度达到最小，称这样的二叉树为最优二叉树。</code></pre><p>哈夫曼树的构造方法如下：</p><p>(1) 将$w_1,w_2,\cdots,w_n$看成是有n 棵树的森林(每棵树仅有一个结点)；</p><p>(2) 在森林中选出两个根结点的权值最小的树合并，作为一棵新树的左、右子树，且新树的根结点权值为其左、右子树根结点权值之和；</p><p>(3)从森林中删除选取的两棵树，并将新树加入森林；</p><p>(4)重复(2)、(3)步，直到森林中只剩一棵树为止，该树即为所求得的哈夫曼树</p><p>假设我们的权值为：26    24    15    10    17    18    10    27</p><p>首先对上面的权值按照从小到大排序：</p><pre><code>排序后的权值为：10 10 15 17 18 24 26 27</code></pre><p>然后。我们按照上述步骤构建哈夫曼树。</p><p><strong>此处我们约定将大的权值放在左子树。</strong></p><p>—Begin</p><ul><li><p>选择最小的两个权值10，10，合并成一个新树的左右子树。新树的权值为20，删除合并的两个权值，将20加入到森林，此时的权值为（仍然进行排序）：15 17 18 <strong>20</strong> 24 26 27</p></li><li><p>选择最小的15和17合并，删除15和17，将32加入到森林，此时的权值为：18 20 24 26 27 <strong>32</strong></p></li><li><p>选择最小的18和20合并，删除18和20，将38加入到森林，此时的权值为：24 26 27 32 <strong>38</strong></p></li><li><p>选择最小的24和26合并，删除24和26，将20加入到森林，此时的权值为：27 32 38 <strong>50</strong></p></li><li><p>选择最小的27和32合并，删除27和32，将59加入到森林，此时的权值为：38 50 <strong>59</strong></p></li><li><p>选择最小的38和50合并，删除38和50，将88加入到森林，此时的权值为：59 <strong>88</strong></p></li><li><p>选择最小的59和88合并，删除59和88，将147加入到森林，此时的权值为：<strong>147</strong></p></li></ul><p>—End</p><p>根据上述权值构造的哈夫曼树如下：</p><p><img src="/picture/6.png" alt="Huffman Tree"></p><p><strong>背景为黄色的是新生成的树</strong></p><h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a><strong>逻辑回归</strong></h3><p>逻辑回归通常用来处理二分类问题，因变量通常只有两个可能的取值，自变量既可以是连续型变量，也可以是分类变量。</p><p>利用sigmoid函数，对于任意的样本$x=(x_1,x_2,\cdots,x_n)^T$，可将二分类问题的h函数(hypothesis)写成如下形式：</p><p>$$<br>h_\theta(x)=\sigma(\theta_0+\theta_1x_1+\cdots+\theta_nx_n)$$<br>,</p><p>其中$\theta =(\theta_0,\theta_1,\cdots,\theta_n)$为待定参数，为了简化起见，可以引入$x_0=1$将$x$扩展为$(x_0,x_1,\cdots,x_n)$,于是，$h_\theta$可简写为</p><p>$$<br>h_\theta(x)=\sigma(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}<br>$$</p><p>sigmoid函数图像如下：</p><p><img src="/picture/7.png" alt="sigmoid"></p><p><strong>函数$h_\theta(x)$的值有特殊的含义，它表示结果取1的概率，因此对于输入$x$,分类结果为类别1和类别0的概率分别为：</strong></p><p>$$<br>P(y=1\vert x;\theta)=h_\theta(x)<br>$$</p><p>$$<br>P(y=0\vert x;\theta)=1-h_\theta(x)<br>$$</p><p>上式也可以写成如下综合形式：</p><p>$$<br>P(y\vert x;\theta)=(h_\theta(x))^y(1-h_\theta(x))^{1-y}<br>$$</p><p><strong>sigmoid函数具有很好的导数特征</strong></p><p>已知</p><p>$$<br>\sigma(x) = \frac{1}{1+e^{-x}}<br>$$</p><p>则</p><p>$$<br>\sigma(x)^{‘}=\frac{e^{-x}}{(1+e^{-x})^2}=\sigma(x)(1-\sigma(x))<br>$$</p><p>另外</p><p>$$<br>[log\sigma(x)]^{‘}=1-\sigma(x),[log(1-\sigma(x))^{‘}=- \sigma(x)<br>$$</p><h3 id="Hierarchical-Softmax-1"><a href="#Hierarchical-Softmax-1" class="headerlink" title="Hierarchical Softmax"></a><strong>Hierarchical Softmax</strong></h3><p>hierarchical softmax使用一颗二叉树的叶子结点来代表输出层对应词汇表中每个词的输出结果。每个结点都代表了它的孩子结点的相关概率。这种方式定义了一种给词分配概率的随机游走解决方案。</p><p>准确地说，从根结点出发，每一个词都能以一条确定的路径抵达。对所使用的符号作如下阐述：</p><ul><li><p>$w$：代表词汇表中的词，用叶子结点表示</p></li><li><p>$n(w,j)$：从根结点到达某个叶子结点的路径上的第$j$个结点，根结点为第一个结点</p></li><li><p>$L(w)$：路径的长度，也就是结点的个数</p></li></ul><p>因此，$n(w,1)=root,n(w,L(w))=w$</p><ul><li><p>$n$：非叶子结点</p></li><li><p>$ch(n)$：非叶子结点的孩子结点</p></li><li><p>$[[x]]$：if x is true,than 1,else than -1</p></li></ul><p>hierarchical softmax定义$P(w_o\vert w_i)$如下：</p><p>$$<br>P(w\vert w_i)=\prod_{j=1}^{L(w)-1}\sigma([[n(w,j+1)=ch(n(w,j))]]\cdot (v^{‘}<em>{n(w,j)})^T v</em>{w_i})<br>$$</p><p>其中</p><p>$$<br>\sigma(x)=1/(1+e^{-x})<br>$$</p><p>可以证明：</p><p>$$<br>\sum _{i=1}^Wp(w\vert w_i)=1<br>$$</p><p>那么，hierarchical softmax框架下的skip-gram结构是什么样的呢?</p><p><img src="/picture/8.png" alt="hierarchical softmax skip-gram"></p><p>如上图所示，与basic版本的skip-gram模型相比，hierarchical softmax版本的模型输出层不再是线性结构，而是树形结构。</p><p>上面已经说过，线性结构的skip-gram模型的参数规模十分庞大，最大能够达到$10^7$数量级，再加上动辄10亿级别的语料库，训练代价很高，效率就特别低了。</p><p>引入了hierarchical softmax以后，输出层的维度得到了大幅降低。正如作者所说，计算$logp(w_o\vert w_i)$和$\nabla logp(w_o\vert w_i)$的代价是与$L(w_o)$也就是哈夫曼树的深度成正比的。</p><p><strong>下面假装正经地推导参数的更新过程：</strong></p><p>假设我们的训练样本是【今天我上课迟到了，然后被老师批评了】，对应词汇表中的词汇为【今天 我 上课 迟到 了 然后 被 老师 批评】</p><p>假设在语料库中，上述词汇对应的频数如下表所示：</p><table><thead><tr><th align="center">词汇</th><th align="center">频率</th></tr></thead><tbody><tr><td align="center">今天</td><td align="center">8</td></tr><tr><td align="center">我</td><td align="center">15</td></tr><tr><td align="center">上课</td><td align="center">2</td></tr><tr><td align="center">迟到</td><td align="center">1</td></tr><tr><td align="center">了</td><td align="center">20</td></tr><tr><td align="center">然后</td><td align="center">10</td></tr><tr><td align="center">被</td><td align="center">5</td></tr><tr><td align="center">老师</td><td align="center">3</td></tr><tr><td align="center">批评</td><td align="center">2</td></tr></tbody></table><p>将频率看做是哈夫曼树的权值，将大的权值放在左子树，小的权值放在右子树。</p><p>绘制出的带权值的哈夫曼树如下：</p><p><img src="%5Cpicture%5C9.png" alt="output"></p><p>说明如下：</p><ul><li><p>黄色的结点表示新生成的树</p></li><li><p>$\theta_i^w \in R^m$，非叶子结点对应的向量</p></li><li><p>$d_1^w,d_2^w,\cdots,d_{l(w)-1}^w \in {0,1}$：词$w$的哈夫曼编码，由$l(w)-1$位编码构成，$d_j^w$对应对应路径中第$j$个非叶子结点的编码(0 or 1).</p></li></ul><p>根据基于hierarchical softmax的skip-gram模型，投影层将输入的one-hot向量投影成$R^m$空间中的向量，即输出层接受的输入为投影层的输出，即$v_m$。</p><p>目标函数仍然是最大化对数似然概率，那么，当前的重点是条件概率函数的构造。</p><p>skip-gram模型将其定义为</p><p>$$P(context(w_i)\vert w_i)=\prod _{u\in context(w_i)}p(u\vert w_i)$$</p><p>而根据hierarchical softmax的思想，$p(u\vert w_i)$可以写出如下形式：</p><p>$$p(u\vert w_i)=\prod <em>{j=2}^{l(w)}p(d_j^u\vert v_m;\theta</em>{j-1}^u)$$</p><p>其中</p><p>$$p(d_j^u\vert v_m;\theta_{j-1}^u)=[\sigma(v_m^T\theta^u_{j-1})]^{1-d_j^u}\cdot[1-\sigma(v_m^T\theta^u_{j-1})]^{d_j^u}$$</p><p>那么，我们的目标函数可以写成如下形式：</p><p>$$L=\sum <em>{w\in C} log \prod _{u\in context(w)}\prod _{j=2}^{l(w)}[\sigma(v_m^T\theta^u</em>{j-1})]^{1-d_j^u}\cdot[1-\sigma(v_m^T\theta^u_{j-1})]^{d_j^u}$$<br>$$=\sum <em>{w\in C}\sum _{u \in context(w)}\sum _{j=2}^{l(w)}{(1-d_j^u)\cdot log[\sigma(v_m^T\theta^u</em>{j-1})]+d_j^u\cdot log[1-\sigma(v_m^T\theta^u_{j-1})]}$$</p><p>上面的函数就是skip-gram的目标函数，为了方便推导梯度，将三重求和符号下花括号的内容简记为$L(w,u,j)$，即</p><p>$$L(w,u,j)=(1-d_j^u)\cdot log[\sigma(v_m^T\theta^u_{j-1})]+d_j^u\cdot log[1-\sigma(v_m^T\theta^u_{j-1})]$$</p><p>首先考虑$L$关于$\theta_{j-1}^u$的梯度计算：</p><p>$$\frac{\partial L}{\partial \theta_{j-1}^u}=\frac{\partial L(w,u,j)}{\partial \theta_{j-1}^u}=\frac{\partial {(1-d_j^u)\cdot log[\sigma(v_m^T\theta^u_{j-1})]+d_j^u\cdot log[1-\sigma(v_m^T\theta^u_{j-1})]}}{\partial \theta_{j-1}^u}=[1-d_j^u-\sigma(v_m^T\theta_{j-1}^u)]\cdot v_m$$</p><p>于是，$\theta_{j-1}^u$的更新公式可写为：</p><p>$$\theta_{j-1}^u\leftarrow \theta_{j-1}^u+\epsilon [1-d_j^u-\sigma(v_m^T\theta_{j-1}^u)]\cdot v_m$$</p><p>接下来考虑$L$对$v_m$的梯度。由于在$L$中，$v_m$与$\theta_{j-1}^u$具有对称性，因此可以根据上述所求，直接写出$v_m$的更新公式：</p><p>$$v_m\leftarrow v_m+\epsilon \sum _{u\in C}\sum _{j=2}^{l(w)}\frac{\partial L(w,u,j)}{\partial v_m}$$</p><p>其中，$\epsilon$是学习率。</p><h2 id="Negative-Sampling-and-Subsampling-of-Frequent-Words"><a href="#Negative-Sampling-and-Subsampling-of-Frequent-Words" class="headerlink" title="Negative Sampling and Subsampling of Frequent Words"></a>Negative Sampling and Subsampling of Frequent Words</h2><p>考虑到basic版本的skip-gram模型拥有两个异常巨大的权重矩阵，再加上一个10亿数量级的语料库，神经网络跑起来会非常吃力。</p><p>word2vec的作者是这样处理这个问题的，主要有以下三个创新点：</p><ol><li><p>将常用词对或者词组看成一个单独的词</p></li><li><p>subsampling(降采样)出现频率很高的词以减小训练样本的大小</p></li><li><p>使用一种被称为”Negative Sampling”的方法来改变优化目标，这种方法在训练时只优化与训练样本有关的很小一丢丢权重。</p></li></ol><p><strong>值的一提的是，应用Subsampling和NEG方法不仅可以减小计算压力，还能提高最终产生的词向量的质量。</strong></p><h3 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h3><p>回想一下那个basic版本的skip-gram，神经网络的真实值或者说标签是一个one-hot矩阵，也就是说，只有一个分量是1，而其他分量都是0(成千上万个0)。但是我们在更新神经网络权重的时候，对于所有的输出为0的权重都进行了更新，这样效率是比较差的。</p><p>negative sampling的思想就是，当我们在训练一个特定样本的时候，能不能只更新几个输出为0的权重，这样计算起来就比较轻松了。</p><p>我们把one-hot向量分量为负的位置所对应的词称为negative words，在训练一次样本时，我们只更新几个(假如说5个吧)negative words，同时也更新我们的pisitive words(即分量为1所对应的位置在词汇表中对应的词)。</p><p>作者在论文中说到，当样本量比较小的时候，选择5-20个negative words效果会比较好，当样本量比较大的时候，2-5个negative words就能得到很好的效果。</p><p>现在假设我们在basic model中需要更新的权重矩阵为$300\times 10000$矩阵，那么basic model每次迭代需要更新$3\times 10^6$个参数，而采用了negative sampling 方法以后，我们只需要更新5个negative words和1个positive words对应的权重，也就是$300\times (5+1)=1800$个权重，是原来的$1800/3\times 10^6=0.06%$.</p><p>想法是好的，我们要怎么选择这5个negative words呢？</p><p>我们知道，语料库中的每个词都有一定的频率，那么我们就利用频率这个信息，对negative words进行采样。</p><p>由此可见，高频词被选为negative words的概率就比较大，同理，低频词被选为negative words的概率就比较小。</p><p>作者的处理方法是，赋予每一个词被选为negative words的概率，具体计算公式如下：<br>$$<br>P(w_i)=\frac{f(w_i)^{0.75}}{\sum _{j=0}^nf(w_i)^{0.75}}<br>$$<br>,</p><p>至于为什么是0.75，这是作者及其团队经过不断试验得出的效果比较好的，没有特别的原因。</p><p>计算出了概率，我们又怎么选择相应的negative words呢？</p><p>作者在其代码中给出了答案。</p><p><strong>下面给出了具体的方法：</strong></p><ol><li><p>构造一个很长的数组,作者的数组长度达到了1000000</p></li><li><p>将词汇表中的每个词对应的index(索引)向数组中填充多次，填充的次数是这样计算的：</p></li></ol><p>根据一元模型可以从语料库生成样本（也就是把语料库中的文本以词的形式展示）</p><p>填充次数times的计算公式为：</p><p>$$<br>times = P(w_i)\times 100000<br>$$</p><ol start="3"><li>欲选出你的negative words，只需要在0-100M之间随机生成一个整数，以这个整数为索引，在数组中查找元素，该元素也是一个索引，根据这个元素从词汇表中查找negative words。</li></ol><p><strong>不难理解，$P(w_i)$大的被选中的概率就大。</strong></p><h3 id="Subsampling-of-Frequent-Words"><a href="#Subsampling-of-Frequent-Words" class="headerlink" title="Subsampling of Frequent Words"></a>Subsampling of Frequent Words</h3><p>让我们再看一遍词对的生成过程。</p><p><img src="/picture/3.png" alt="word-pairs"></p><p>如上图所示，我们设定window size = 2 来生成样本。对于包含”the”的词对来说，有以下两个问题：</p><ol><li><p>当我们在寻找词对的时候，(“fox”,”the”)所能提供的信息并不比”fox”多。然而，”the”几乎在上面的所有词对中都有出现。</p></li><li><p>形如(“the”,…)这样的词对已经远远超过了我们的需求。</p></li></ol><p>word2vec 应用了一种称之为subsampling的方法来解决这个问题。</p><p>对于我们遇到每一个语料库中的词，都有一定的概率将它从语料库中删除，删除的概率与该词出现的频率有关。</p><p>如果我们将window size 设置为10，我们是这样删除”the”相关样本的：</p><ol><li><p>我们在训练其他词的时候，”the”不会出现在他们的窗口</p></li><li><p>当输入词是”the”的时候，将样本数减少10个(不减少的情况下是20)</p></li></ol><p>那么，我们怎么决定是否删除一个词呢？</p><p>假设$w_i$是待定删除的词，$z(w_i)$是$w_i$在语料库中出现的频率,$P(w_i)$是保留该词的概率:</p><p>$$<br>P(w_i)=(\sqrt{\frac{z(w_i)}{0.001}}+1)\cdot \frac{0.001}{z(w_i)}<br>$$</p><p><strong>0.001也是一个经验参数，如果比0.001还要小，那么保留词的概率就会更小。</strong></p><p>下面是$P(w_i)$函数的图像：</p><p><img src="/picture/10.png" alt="p(wi)"></p><p>从图像可以看出：</p><ul><li><p>当频率等于0.0026的时候，被保留的概率为1，也就是说，当频率大于0.0026的时候，就有可能被删除。</p></li><li><p>当频率为0.0074的时候，有一半的概率会被保留</p></li><li><p>当频率为0.1的时候，被保留的概率就骤减到0.1</p></li></ul><h3 id="Learning-Phrase"><a href="#Learning-Phrase" class="headerlink" title="Learning Phrase"></a>Learning Phrase</h3><p>learning phrase 方法使得样本更加接近真实世界。考虑以下句子：</p><pre><code>New York is a beautiful and modern city where I want to have a travel.</code></pre><p>我们设置窗口大小为4，对beautiful进行采样，获得如下样本：</p><pre><code>(beautiful,New) (beautiful,York) (beautiful,is) (beautiful,a) (beautiful,New) (beautiful,and) (beautiful,modern) (beautiful,city) </code></pre><p>采用一元的方法，会将常用的词组分开，从而降低词向量的质量。</p><p>我们可以采取一个简单的数据驱动的方法，来对两个词是否能组成词组进行打分：</p><p>$$<br>score(w_i,w_j) = \frac{count(w_iw_j)-\delta}{count(w_i)\times count(w_j)}<br>$$</p><p>$\delta$作为一个折扣系数的作用，用来防止那些不怎么经常一起出现的词语形成词组。当score超过了我们设置的阈值时，将两个词视为词组。</p><p>上述打分程序可以多进行几次，以获得三元组或者更多词的词组。</p><hr><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]McCormick, C. (2016, April 19). Word2Vec Tutorial - The Skip-Gram Model. Retrieved from <a href="http://www.mccormickml.com" target="_blank" rel="noopener">http://www.mccormickml.com</a></p><p>[2]Pennington J, Socher R, Manning C. Glove: Global Vectors for Word Representation[C]// Conference on Empirical Methods in Natural Language Processing. 2014:1532-1543.</p><p>[3]Mikolov T, Le Q V, Sutskever I. Exploiting Similarities among Languages for Machine Translation[J]. Computer Science, 2013.</p><p>[4]Mikolov T, Chen K, Corrado G, et al. Efficient Estimation of Word Representations in Vector Space[J]. Computer Science, 2013.</p><p>[5]Bengio, Y &amp; Ducharme, Réjean &amp; Vincent, Pascal. (2000). A Neural Probabilistic Language Model. Journal of Machine Learning Research. 3. 932-938. 10.1162/153244303322533223. </p><p>[6]NSS,(JUNE 4, 2017).An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec.Retrieved from <a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/" target="_blank" rel="noopener">https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/</a></p><p>[7]peghoty,2014年07月.word2vec 中的数学原理详解.<a href="http://blog.csdn.net/itplus/article/details/37969519" target="_blank" rel="noopener">http://blog.csdn.net/itplus/article/details/37969519</a></p><p>[8]吴军．数学之美[M]．北京：人民邮电出版社，2014.</p><p>==========================The End=============================</p>]]></content>
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
