<!DOCTYPE html>
<html  lang="zh">
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
<title>标签: 机器学习 - Young&#39;s Blog</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />



    <meta name="description" content="每个人都是平凡的，同时也是与众不同的。">
<meta property="og:type" content="website">
<meta property="og:title" content="Young&#39;s Blog">
<meta property="og:url" content="http://yoursite.com/tags/机器学习/index.html">
<meta property="og:site_name" content="Young&#39;s Blog">
<meta property="og:description" content="每个人都是平凡的，同时也是与众不同的。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://yoursite.com/images/og_image.png">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Young&#39;s Blog">
<meta name="twitter:description" content="每个人都是平凡的，同时也是与众不同的。">
<meta name="twitter:image" content="http://yoursite.com/images/og_image.png">







<link rel="icon" href="/images/favicon.svg">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css">


    
    
    
    <style>body>.footer,body>.navbar,body>.section{opacity:0}</style>
    

    
    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">
    

    
    

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">


    
    
    
    

<link rel="stylesheet" href="/css/back-to-top.css">


    
    
    
    

    
    
<link rel="stylesheet" href="/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>

    
    
    

    
    
    


<link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>
<body class="is-3-column">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/">
            
                <img src="/images/logo.svg" alt="Young&#39;s Blog" height="28">
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item"
                href="/">主页</a>
                
                <a class="navbar-item"
                href="/archives">归档</a>
                
                <a class="navbar-item"
                href="/categories">分类</a>
                
                <a class="navbar-item"
                href="/tags">标签</a>
                
                <a class="navbar-item"
                href="/about">关于</a>
                
            </div>
            
            <div class="navbar-end">
                
                
                
                <a class="navbar-item search" title="搜索" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-8-tablet is-8-desktop is-6-widescreen has-order-2 column-main"><div class="card">
    <div class="card-content">
        <nav class="breadcrumb" aria-label="breadcrumbs">
        <ul>
            <li><a href="/tags">标签</a></li>
            <li class="is-active"><a href="#" aria-current="page">机器学习</a></li>
        </ul>
        </nav>
    </div>
</div>

    <div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2018-05-04T16:00:00.000Z">2018-05-05</time>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    32 分钟 读完 (大约 4805 个字)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2018/05/05/梯度下降/">线性回归与梯度下降算法</a>
            
        </h1>
        <div class="content">
            <blockquote>
<p>【摘要】</p>
<p>本文以线性回归为例，讲解了批量梯度下降、随机梯度下降、小批量梯度下降、冲量梯度下降等算法，由浅入深，并结合精心设计的例子，使读者最快掌握这种最常用的优化方法。每一种优化方法，笔者都基于R语言给出了相应的代码，供读者参考，</p>
</blockquote>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>假如我们有以下身高和体重的数据，我们希望用身高来预测体重。如果你学过统计，那么很自然地就能想到建立一个线性回归模型：</p>
<script type="math/tex; mode=display">y=a+bx</script><p>其中$a$是截距，$b$是斜率，$y$是体重，$x$是身高。</p>
<p><img src="/picture/1525406306009-1525855946681.png" alt="1525406306009"></p>
<p><img src="/picture/1525406028458.png" alt="1525406028458"></p>
<p>我们将身高与体重的关系在Excel里面用折线图表示，并且添加了线性的趋势线。蓝色的线条是真实数据，红色的实线是模型给出的预测值。蓝色线条与红色线条之间的距离绝对值是预测误差。所以，我们要找到最优的$a$和$b$来拟合这条直线，使得我们模型的总误差最小。</p>
<script type="math/tex; mode=display">Error = \frac{1}{2}(Actual\ weight - Predicted\ weight)^2=\frac{1}{2}(Y-Ypred)^2</script><p>我们使用均方误差来表示模型的误差，由于$Ypred = a + bx$，因此，模型的均方误差可以表示为</p>
<script type="math/tex; mode=display">SSE = \sum \frac{1}{2}(Y-a-bx)^2</script><p>也就是说，$SSE$是关于$a$和$b$的函数，我们只需要不断调整$a$和$b$，使$SSE$降到最低就可以了。这个时候，我们就可以利用梯度下降算法，来求解$a$和$b$的值。</p>
<p>梯度下降的计算过程如下：</p>
<blockquote>
<p>step 1:随机初始化权重$a$和$b$，计算出误差$SSE$</p>
<p>step 2:计算梯度。    $a$和$b$的轻微变化都会导致$SSE$的变化，因此，我们只需要找到能使$SSE$减小的$a$和$b$的变化方向就可以了。这个方向，一般就是由梯度决定的。</p>
<p>step 3:调整权重值，使得$SSE$不断接近最小值。</p>
<p>step 4:使用新的权重去做预测，并且计算出新的$SSE$。</p>
<p>step 5:重复step2-step3，直到权重不再显著变化为止。</p>
</blockquote>
<p>我们在Excel中进行上述步骤。为了计算能够快一点，我们首先对数据进行Min-Max标准化。得到如下数据：</p>
<p><img src="/picture/1525407652713.png" alt="1525407652713"></p>
<p>step1:随机选取一组权重(此处我们设置a=0,b=1),我们计算出预测值和误差：</p>
<p><img src="/picture/1525408865090.png" alt="1525408865090"></p>
<p>step2:计算梯度</p>
<script type="math/tex; mode=display">\frac{\partial SSE}{\partial a} = \sum-(Y-a-bx)=\sum-(Y-Ypred)</script><script type="math/tex; mode=display">\frac{\partial SSE}{\partial b}=\sum-(Y-a-bx)x=\sum-(Y-Ypred)x</script><p>$\frac{\partial SSE}{\partial a}$和$\frac{\partial SSE}{\partial b}$就是梯度，他们决定了$a$和$b$的移动方向和距离。    </p>
<p>step3: 调整权重值，使得$SSE$不断接近最小值。</p>
<p>调整规则为:</p>
<script type="math/tex; mode=display">a_{new} = a_{old} - \eta \nabla a = a_{old} - \eta \cdot \partial SSE/\partial a</script><script type="math/tex; mode=display">b_{new} = b_{old} - \eta \nabla b = b_{old} - \eta \cdot \partial SSE / \partial b</script><p>其中，$\eta$是一个被我们称之为学习率(learning rate)的东西，一般设置为0.01或者你希望的任何比较小的数值。</p>
<p>本文选择0.01作为学习率。</p>
<script type="math/tex; mode=display">a_{new} = 0 - 0.01 \times 1.925 = -0.01925</script><script type="math/tex; mode=display">b_{new} = 1 - 0.01 \times 1.117 = 0.98883</script><p>step4:使用新的权重去做预测，并且计算出新的$SSE$。</p>
<p><img src="/picture/1525410198759.png" alt="1525410198759"></p>
<p>可以看出，SSE从0.155降低到0.111，说明系数有改善。</p>
<p>step 5:重复step2-step3，直到权重不再显著变化为止。</p>
<p>我们知道，一元线性回归的系数可以用公式计算，我们用R的lm()函数来计算权重，结果为</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">lm(y~x,dat)</span><br><span class="line"></span><br><span class="line">Call:</span><br><span class="line">lm(formula = y ~ x, data = dat)</span><br><span class="line"></span><br><span class="line">Coefficients:</span><br><span class="line">(Intercept)            x  </span><br><span class="line">-<span class="hljs-number">0.1167</span>       <span class="hljs-number">0.9777</span></span><br></pre></td></tr></table></figure>
<p>然后，我在R里面写了一个梯度下降的函数，当精度调到0.0000001的时候，与lm的结果已经很接近了。</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">gradientDescent &lt;- <span class="hljs-keyword">function</span>(dat,start = c(<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),learning_rate = <span class="hljs-number">0.01</span>,tol = <span class="hljs-number">0.001</span>)</span><br><span class="line">&#123;</span><br><span class="line">	a = start[<span class="hljs-number">1</span>]</span><br><span class="line">	b = start[<span class="hljs-number">2</span>]</span><br><span class="line">	x = dat[,<span class="hljs-number">1</span>]</span><br><span class="line">	y = dat[,<span class="hljs-number">2</span>]</span><br><span class="line">	iters = <span class="hljs-number">0</span></span><br><span class="line">	<span class="hljs-keyword">while</span>(<span class="hljs-literal">TRUE</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		Ypred = a + b * x</span><br><span class="line">		old_a = a</span><br><span class="line">		old_b = b</span><br><span class="line">		a = a + learning_rate * sum(y - Ypred)</span><br><span class="line">		b = b + learning_rate * sum((y-Ypred) * x)</span><br><span class="line">		iters = iters + <span class="hljs-number">1</span></span><br><span class="line">		<span class="hljs-keyword">if</span>(abs(a-old_a) &lt;= tol &amp; abs(b-old_b) &lt;= <span class="hljs-number">0.01</span>)</span><br><span class="line">			<span class="hljs-keyword">break</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	list(weights = c(a,b),iters = iters)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">gradientDescent(dat,tol=<span class="hljs-number">0.0000001</span>)</span><br><span class="line">$weights</span><br><span class="line">[<span class="hljs-number">1</span>] -<span class="hljs-number">0.1167315</span>  <span class="hljs-number">0.9776839</span></span><br><span class="line"></span><br><span class="line">$iters <span class="hljs-comment">#迭代了975次</span></span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">975</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>我们常说的梯度，其实是指向量，其方向与切线方向相同。</p>
<p>利用梯度下降法进行权重更新的公式为:</p>
<script type="math/tex; mode=display">weight_{new} = weight_{old} - \eta \cdot \nabla</script><p>其中，那个倒三角形就是梯度的意思。我们在高中数学学过，切线方向是函数变化速度最快的方向，</p>
</blockquote>
<h2 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h2><p>梯度下降算法，又可以称为Batch-Gradient-Gescent,即批量梯度下降算法。从上面的例子可以看出，批量梯度下降算法，每次更新系数都需要所有的样本参与计算，当样本规模达到一定数量以后，这个更新速度会非常慢。另外，还有可能导致内存溢出。</p>
<p>为了克服批量梯度下降的这个缺点，有人提出了随机梯度下降(Stochastic Gradient Descent)算法，即每次更新系数只需要一个样本参与计算，因此既可以减少迭代次数，节省计算时间，又可以防止内存溢出。</p>
<p>对于上述问题，随机梯度下降的算法过程如下：</p>
<blockquote>
<p>for every $Y_i$:</p>
<p>$Ypred = a + bx$</p>
<p>$a = a + \eta (Y-Ypred)$</p>
<p>$b = b+\eta(Y-Ypred)\cdot x$</p>
</blockquote>
<p>随机梯度下降算法适用于大数量的计算，对于小数据量不一定准确。为了检验随机梯度下降算法，我们构造了一个有10000个样本的数据，同样是计算一元线性回归的系数。</p>
<p>随机梯度下降的函数如下：</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">stochasticGradientDescent &lt;- <span class="hljs-keyword">function</span>(dat,start = c(<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),learning_rate = <span class="hljs-number">0.01</span>,tol = <span class="hljs-number">0.000001</span>,iteratons = <span class="hljs-number">100</span>)</span><br><span class="line">&#123;</span><br><span class="line">	<span class="hljs-comment">#start:初始参数</span></span><br><span class="line">	<span class="hljs-comment">#learning_rate:学习率</span></span><br><span class="line">	<span class="hljs-comment">#tol:精度</span></span><br><span class="line">	<span class="hljs-comment">#iterations:迭代次数</span></span><br><span class="line">	</span><br><span class="line">	dat = as.matrix(dat)</span><br><span class="line">	a = start[<span class="hljs-number">1</span>]</span><br><span class="line">	b = start[<span class="hljs-number">2</span>]</span><br><span class="line">	x = dat[,<span class="hljs-number">1</span>]</span><br><span class="line">	y = dat[,<span class="hljs-number">2</span>]</span><br><span class="line">	iters = <span class="hljs-number">0</span></span><br><span class="line">	<span class="hljs-keyword">while</span>(iters &lt; iteratons)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="hljs-comment">#重排，即将样本的顺序打乱</span></span><br><span class="line">		index = sample(length(x))</span><br><span class="line">		old_a = a</span><br><span class="line">		old_b = b</span><br><span class="line">		<span class="hljs-keyword">for</span>(i <span class="hljs-keyword">in</span> index)</span><br><span class="line">		&#123;</span><br><span class="line">			</span><br><span class="line">			Ypred = a + b * x[i]</span><br><span class="line">			a = a + learning_rate * (y[i] - Ypred)</span><br><span class="line">			b = b + learning_rate * (y[i]-Ypred) * x[i]</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="hljs-keyword">if</span>(abs(a-old_a) &lt;= tol &amp; abs(b-old_b) &lt;= tol)</span><br><span class="line">		 	<span class="hljs-keyword">break</span>;</span><br><span class="line">		learning_rate = learning_rate / (<span class="hljs-number">1</span> + <span class="hljs-number">0.01</span> * iters) <span class="hljs-comment">#自适应学习率</span></span><br><span class="line">		iters = iters + <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">if</span>(iters &gt; iterations)</span><br><span class="line">            <span class="hljs-keyword">break</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	list(weights = c(a,b),iters = iters)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>然后我们构造一个相对大的样本用来检验算法：</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">set.seed(<span class="hljs-number">100</span>)</span><br><span class="line">x &lt;- seq(<span class="hljs-number">1</span>,<span class="hljs-number">10</span>,length.out = <span class="hljs-number">10000</span>)</span><br><span class="line">y &lt;- <span class="hljs-number">2</span> * x + rnorm(<span class="hljs-number">10000</span>) * <span class="hljs-number">10</span> + <span class="hljs-number">2</span></span><br><span class="line">bigdata &lt;- data.frame(x ,y )</span><br><span class="line">plot(x,y)</span><br><span class="line">cor(x,y)</span><br></pre></td></tr></table></figure>
<p><img src="/picture/sgd.png" alt="sgd"></p>
<p>回归结果：</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lm(y~x,data = bigdata )</span><br><span class="line">Call:</span><br><span class="line">lm(formula = y ~ x, data = bigdata)</span><br><span class="line"></span><br><span class="line">Coefficients:</span><br><span class="line">(Intercept)            x  </span><br><span class="line">      <span class="hljs-number">2.004</span>        <span class="hljs-number">2.006</span></span><br></pre></td></tr></table></figure>
<p>随机梯度下降的结果：</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">stochasticGradientDescent(bigdata,learning_rate = <span class="hljs-number">0.001</span>,tol=<span class="hljs-number">0.000000001</span>)</span><br><span class="line">$weights</span><br><span class="line"><span class="hljs-number">2.01138749995603</span> <span class="hljs-number">2.00584502615877</span></span><br><span class="line">$iters</span><br><span class="line"><span class="hljs-number">69</span></span><br></pre></td></tr></table></figure>
<p>批量梯度下降的结果：</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">batchGradientDescent(bigdata,learning_rate = <span class="hljs-number">0.000001</span>,tol = <span class="hljs-number">0.000000001</span>)</span><br><span class="line">$weights</span><br><span class="line"><span class="hljs-number">2.00385275101478</span> <span class="hljs-number">2.00634924457312</span></span><br><span class="line">$iters</span><br><span class="line"><span class="hljs-number">8345</span></span><br></pre></td></tr></table></figure>
<p>可以看到，在同样的精度要求下，随机梯度下降进行59次迭代以后即收敛，而批量梯度下降则需要迭代8345次。</p>
<p>但是随机梯度下降也有一个缺点，即参数更新频率太快，有可能出现目标函数值在最优质附近的震荡现象，因为高频率的参数更新导致了高方差。 同时也可以看出，在相同精度要求下，随机梯度下降计算出来的系数与精确值离差较大，而批量随机下降则更接近精确值。</p>
<h2 id="Mini-batch-Gradient-Descent"><a href="#Mini-batch-Gradient-Descent" class="headerlink" title="Mini-batch Gradient Descent"></a>Mini-batch Gradient Descent</h2><p>小批量梯度下降(Mini-batch Gradient Descent)是介于上述两种方法之间的优化方法，即在更新参数时，只使用一部分样本（一般256以下）来更新参数，这样既可以保证训练过程更稳定，又可以利用批量训练方法中的矩阵计算的优势。</p>
<p>具体更新哪些样本，通常是随机确定的，下面，我们定义一下小批量梯度下降的函数，用来求解上述bigdata的系数。</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">miniBatchGradientDescent &lt;- <span class="hljs-keyword">function</span>(dat,start = c(<span class="hljs-number">0</span>,<span class="hljs-number">0</span>),learning_rate = <span class="hljs-number">0.01</span>,tol = <span class="hljs-number">0.001</span>,batchSize = <span class="hljs-number">256</span>,iterations = <span class="hljs-number">10000</span>)</span><br><span class="line">&#123;</span><br><span class="line">    a = start[<span class="hljs-number">1</span>]</span><br><span class="line">    b = start[<span class="hljs-number">2</span>]</span><br><span class="line">    iters = <span class="hljs-number">0</span></span><br><span class="line">    len = length(y)</span><br><span class="line">    <span class="hljs-keyword">while</span>(<span class="hljs-literal">TRUE</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        mini_index = sample(len,batchSize,replace = <span class="hljs-literal">FALSE</span>)</span><br><span class="line">        x = dat[mini_index,<span class="hljs-number">1</span>]</span><br><span class="line">        y = dat[mini_index,<span class="hljs-number">2</span>]</span><br><span class="line">        Ypred = a + b * x</span><br><span class="line">        error = y - Ypred</span><br><span class="line">        old_a = a</span><br><span class="line">        old_b = b</span><br><span class="line">        a = a + learning_rate * sum(error)</span><br><span class="line">        b = b + learning_rate * sum((error) * x)</span><br><span class="line">        start = rbind(start,c(a,b))</span><br><span class="line">        iters = iters + <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">if</span>(abs(a-old_a) &lt;= tol &amp; abs(b-old_b) &lt;= tol)</span><br><span class="line">            <span class="hljs-keyword">break</span></span><br><span class="line">        <span class="hljs-keyword">if</span>(iters &gt;= iterations)</span><br><span class="line">            <span class="hljs-keyword">break</span></span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    list(weights = c(a,b),iters = iters,coes = start)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">miniBatchGradientDescent(bigdata,learning_rate = <span class="hljs-number">0.0001</span>,tol = <span class="hljs-number">0.00001</span>,batchSize = <span class="hljs-number">100</span>)</span><br><span class="line">$weights</span><br><span class="line"><span class="hljs-number">2.02646349019186</span> <span class="hljs-number">2.0439693915315</span></span><br><span class="line">$iters</span><br><span class="line"><span class="hljs-number">920064</span></span><br></pre></td></tr></table></figure>
<p>小梯度批量梯度下降收敛时需要迭代92万次，这显然有点多了。一般来说，当数据量非常大时，小批量梯度下降比较有效，否则计算结果很有可能出现偏移。</p>
<blockquote>
<p>先mark，偏移的原因待考究。</p>
</blockquote>
<h2 id="Momentum-optimization"><a href="#Momentum-optimization" class="headerlink" title="Momentum optimization"></a>Momentum optimization</h2><p>考虑这样一种情形，小球从山顶往下滚动，一开始很顺利，可是在接近最低点的时候，小球陷入了一段狭长的浅山谷。由于小球一开始并不是直接沿着山谷的方向滚下，因此小球会在这个浅浅的山谷中不断震荡——不断冲上墙壁，接着又从墙壁上滚下来。这种情况并不是我们想看到的，因为这增加了迭代时间。冲量(Momentnum)的引入，使得我们的目标更新的更快了，冲量的更新方式有以下两种，两种方式之间并无太大差异。</p>
<blockquote>
<p>第一种：</p>
<p>$Z^{k+1}=\beta Z_k + \nabla$</p>
<p>$w^{k+1} = w_k - \alpha Z^{k+1}$</p>
<p>其中，$Z$是一个与$w$方向相同的向量，</p>
<p>第二种：</p>
<p>$Z^{k+1}=\beta Z^k + \alpha \nabla$</p>
<p>$w^{k+1} = w^k - Z^{k+1}$</p>
</blockquote>
<p><img src="/picture/1525839842032.png" alt="1525839842032"></p>
<p>两者的差别仅仅在于$Z^{k+1}$的系数不同。</p>
<p>通常，这里的学习率要比随机梯度下降小一点，因为随机梯度下降的梯度大一点。$\beta$的取值决定了前一次的梯度有多少被纳入了本次的更新。一般来说，稳定前将$\beta$设置为0.5，稳定后可以设置为0.9或更高。</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#适用于求解一元或多元线性回归的回归系数，返回结果包括截距</span></span><br><span class="line">momentumGradientDescent &lt;- <span class="hljs-keyword">function</span>(dat,beta = <span class="hljs-number">0.9</span>,z = <span class="hljs-number">0</span>,start = rep(<span class="hljs-number">0</span>,dim(dat)[<span class="hljs-number">2</span>]),alpha = <span class="hljs-number">0.001</span>,tol=<span class="hljs-number">0.0000001</span>,iterations = <span class="hljs-number">100</span>)</span><br><span class="line">&#123;</span><br><span class="line">    dataSet = cbind(<span class="hljs-number">1</span>,dat)  <span class="hljs-comment">#将第一列加上1</span></span><br><span class="line">    cols = dim(dataSet)[<span class="hljs-number">2</span>] <span class="hljs-comment">#列数</span></span><br><span class="line">    x = as.matrix(dataSet[,<span class="hljs-number">1</span>:(cols - <span class="hljs-number">1</span>)])  <span class="hljs-comment">#自变量矩阵</span></span><br><span class="line">    y = as.matrix(dataSet[,cols],ncol = <span class="hljs-number">1</span>)  <span class="hljs-comment">#因变量，矩阵</span></span><br><span class="line">    w = as.matrix(start,ncol = <span class="hljs-number">1</span>)  <span class="hljs-comment">#权重矩阵</span></span><br><span class="line">    iters = <span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">while</span>(<span class="hljs-literal">TRUE</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        old_w = w</span><br><span class="line">        old_z = z</span><br><span class="line">        Ypred = x %*% w</span><br><span class="line">        error = y - Ypred</span><br><span class="line">        grad = - t(x) %*% error</span><br><span class="line">        z = beta * old_z + grad</span><br><span class="line">        w = old_w - alpha * z</span><br><span class="line">        <span class="hljs-keyword">if</span>(sum(abs(w-old_w)) &lt; tol)</span><br><span class="line">            <span class="hljs-keyword">break</span>;</span><br><span class="line">        iters = iters + <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">if</span>(is.integer(iterations))</span><br><span class="line">            <span class="hljs-keyword">if</span>(iters &gt;= iterations)</span><br><span class="line">                <span class="hljs-keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    list(weights = as.vector(w),iters = iters)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>利用冲量梯度下降求bigdata的系数：</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">momentumGradientDescent(bigdata,alpha=<span class="hljs-number">0.00001</span>)</span><br><span class="line">$weights</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">2.003851</span> <span class="hljs-number">2.006354</span></span><br><span class="line"></span><br><span class="line">$iters</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">248</span></span><br></pre></td></tr></table></figure>
<p>可以看到，迭代次数明显减少，并且系数与精确值更加接近了。</p>
<h2 id="Nesterov-Accelerated-Gradient"><a href="#Nesterov-Accelerated-Gradient" class="headerlink" title="Nesterov Accelerated Gradient"></a>Nesterov Accelerated Gradient</h2><p>然而，让一个小球盲目地沿着斜坡滚下山是不理想的。我们需要一个更聪明的球，它知道下一步要往哪里去，因此在斜坡有上升的时候，它能够自主调整方向。</p>
<p>Nesterov Accelerated Gradient 是基于冲量梯度下降算法进行改进的一种算法，也是梯度下降算法的变种。</p>
<p>在上一种算法中，我们使用了冲量$\beta Z^k$来调整我们的参数改变量，将上述第二种更新方法改写一下，得到如下式子：</p>
<blockquote>
<p>$Z^{k+1} = \beta Z^k + \alpha \nabla$</p>
<p>$w^{k+1} = w^k - Z^{k+1} = w^k - \beta Z_k - \alpha \nabla $</p>
</blockquote>
<p>$w^k - \beta Z_k $给出了下一个参数位置的近似，指明了参数该如何变化。现在，我们可以不用当前的参数，而是用未来的参数的近似位置来更新我们的参数，即</p>
<blockquote>
<p>$Z^{k+1} = \beta Z^k + \alpha \nabla _wJ(w - \beta Z^k)$</p>
<p>$w^{k+1} = w^k - Z^{k+1}$</p>
</blockquote>
<p>这里，我们仍然设置$\beta$的值在0.9附近。如下图所示，冲量梯度下降先计算当前的梯度（短的蓝色向量），然后根据累积的冲量向前跨越一大步（长的蓝色向量）。NAG首先根据之前的累积梯度向前迈一大步（棕色向量），然后对梯度进行修正（红色向量）。这种利用近似未来参数来更新参数的方法，可以防止梯度更新太快，并且增加了响应能力。</p>
<p><img src="/picture/nesterov_update_vector.png" alt="nesterov_update_vector"> </p>
<blockquote>
<p>假设我们的权重矩阵（系数矩阵）为$w$，自变量$x$，因变量$Y$，则损失函数为：</p>
<script type="math/tex; mode=display">J(w) = 1/2 \sum _{i=1} ^n (Y_i - w'x_i)^2</script><p>则</p>
<script type="math/tex; mode=display">\nabla J(w) = \frac{\partial J(w)}{\partial w} = - \sum_{i=1}^n(Y_i - w'x_i)x_i'</script><p>那么</p>
<script type="math/tex; mode=display">\nabla J(w - \beta Z^{k}) = - \sum _{i=1}^n(Y_i - (w'-\beta Z^k)'x_i)x_i'</script></blockquote>
<p>根据上述思想，编写的NAG代码如下 ：</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#适用于求解一元或多元线性回归的回归系数，返回结果包括截距</span></span><br><span class="line">NAG &lt;- <span class="hljs-keyword">function</span>(dat,beta = <span class="hljs-number">0.9</span>,z = <span class="hljs-number">0</span>,start = rep(<span class="hljs-number">0</span>,dim(dat)[<span class="hljs-number">2</span>]),alpha = <span class="hljs-number">0.001</span>,tol=<span class="hljs-number">0.0000001</span>,iterations = <span class="hljs-number">100</span>)</span><br><span class="line">&#123;</span><br><span class="line">    dataSet = cbind(<span class="hljs-number">1</span>,dat)  <span class="hljs-comment">#将第一列加上1</span></span><br><span class="line">    cols = dim(dataSet)[<span class="hljs-number">2</span>] <span class="hljs-comment">#列数</span></span><br><span class="line">    x = as.matrix(dataSet[,<span class="hljs-number">1</span>:(cols - <span class="hljs-number">1</span>)])  <span class="hljs-comment">#自变量矩阵</span></span><br><span class="line">    y = dataSet[,cols]  <span class="hljs-comment">#因变量，矩阵</span></span><br><span class="line">    w = start  <span class="hljs-comment">#权重矩阵</span></span><br><span class="line">    iters = <span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">while</span>(<span class="hljs-literal">TRUE</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        old_w = w</span><br><span class="line">        old_z = z</span><br><span class="line">        <span class="hljs-comment">#中间过程用mid1 mid2代替</span></span><br><span class="line">        mid1 = apply((old_w - beta * old_z) * t(x),<span class="hljs-number">2</span>,sum)</span><br><span class="line">        mid2 = y - mid1</span><br><span class="line">        grad = - apply(mid2 * x,<span class="hljs-number">2</span>,sum)</span><br><span class="line">        z = beta * old_z + alpha * grad</span><br><span class="line">        w = old_w - z</span><br><span class="line">        <span class="hljs-keyword">if</span>(sum(abs(w-old_w)) &lt; tol)</span><br><span class="line">            <span class="hljs-keyword">break</span>;</span><br><span class="line">        iters = iters + <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">if</span>(is.integer(iterations))</span><br><span class="line">            <span class="hljs-keyword">if</span>(iters &gt;= iterations)</span><br><span class="line">                <span class="hljs-keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    list(weights = as.vector(w),iters = iters)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>用NAG来求bigdata的系数：</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">NAG(bigdata,alpha=<span class="hljs-number">0.000001</span>)</span><br><span class="line">$weights</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">2.003849</span> <span class="hljs-number">2.006350</span></span><br><span class="line"></span><br><span class="line">$iters</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">598</span></span><br></pre></td></tr></table></figure>
<h2 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h2><p>尽管我们可以根据损失函数的梯度来加快更新参数，我们也希望能够根据参数的重要性来决定其更新的幅度。</p>
<p>AdaGrad是一种基于梯度算法的优化算法,它只做了一件事:根据参数来自适应调整学习率。对于不常出现的参数进行较大的更新，对于经常出现的参数进行较少的更新，因此，这种方法非常适合处理稀疏数据。</p>
<p>之前，我们对每一个参数更新所使用的学习率都是一样的，而AdaGrad在每一步都使用不同的学习率对不同的参数进行更新。我们先写出AdaGrad的单个参数的更新方法，然后将其向量化。长话短说，假设$g_{t,i}$表示损失函数对于参数$\theta_i$的梯度：</p>
<p>在步骤$t$:</p>
<script type="math/tex; mode=display">g_{t,i} = \nabla _\theta J(\theta_t,i)</script><p>那么，对于步骤$t$，使用随机梯度下将对$\theta_i$进行更新的公式为：</p>
<script type="math/tex; mode=display">\theta_{t+1,i} = \theta_{t,i} - \eta_i \cdot g_{t,i}</script><p>在上述更新过程中，AdaGrad在每一步都对参数$\theta_i$对应的学习率$\eta_i$进行调整，调整的方法基于过去的所有梯度：</p>
<script type="math/tex; mode=display">\theta_{t+1,i} = \theta_{t,i} - \frac{\eta_i}{\sqrt{G_{t,ii} + \epsilon}} \cdot g_{t,i}</script><p>$G_{t}\in R^{d\times d}$是一个对角矩阵，第$i$个对角元素是历史上损失函数对$\theta_i$的所有梯度的平方和，$\epsilon$是一个平滑参数，防止分母为0，通常取$10^{-8}$。有趣的是，如果不加开方，算法表现极差。</p>
<p>因为$G_t$包含了过去所有参数梯度的平方和，因此我们可以将其向量化：</p>
<script type="math/tex; mode=display">\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}}\odot g_t</script><p>AdaGrad的一个最大的好处是不用手动调整学习率的大小，通常设置默认值为0.01，然后顺其自然就好了。</p>
<p>AdaGrad的一个较大的缺点是，分母是不断增大的，当迭代次数不断增加时，分母会逐渐趋于无穷大，学习率进而趋于无穷小，此时，算法将变得不再有效。</p>
<p>我们延用上述符号来写出AdaGrad的函数。</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#SGD为基础</span></span><br><span class="line">AdaGrad &lt;- <span class="hljs-keyword">function</span>(dat,theta,learning_rate = <span class="hljs-number">0.01</span>,start = rep(<span class="hljs-number">0</span>,dim(dat)[<span class="hljs-number">2</span>]),tol = <span class="hljs-number">0.000001</span>,iterations = <span class="hljs-number">1000</span>)</span><br><span class="line">&#123;</span><br><span class="line">    dataSet = cbind(<span class="hljs-number">1</span>,dat) <span class="hljs-comment">#将自变量进行增广，第一列全为1</span></span><br><span class="line">    cols = dim(dataSet)[<span class="hljs-number">2</span>] <span class="hljs-comment">#列数</span></span><br><span class="line">    rows = dim(dataSet)[<span class="hljs-number">1</span>] <span class="hljs-comment">#行数</span></span><br><span class="line">    x = as.matrix(dataSet[,<span class="hljs-number">1</span>:(cols - <span class="hljs-number">1</span>)])  <span class="hljs-comment">#自变量矩阵</span></span><br><span class="line">    y = dataSet[,cols]  <span class="hljs-comment">#因变量，矩阵</span></span><br><span class="line">    g = <span class="hljs-number">0</span></span><br><span class="line">    theta = start  <span class="hljs-comment">#权重矩阵</span></span><br><span class="line">    iters = <span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">while</span>(<span class="hljs-literal">TRUE</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        old_theta = theta</span><br><span class="line">        index = sample(rows)</span><br><span class="line">        <span class="hljs-keyword">for</span>(i <span class="hljs-keyword">in</span> index)</span><br><span class="line">        &#123;</span><br><span class="line">            grad = - (y[i] - sum(theta * x[i,])) * x[i,]</span><br><span class="line">            g = g + grad * grad</span><br><span class="line">            theta = theta - learning_rate / sqrt(g + <span class="hljs-number">1e-8</span>) * grad</span><br><span class="line">        &#125;</span><br><span class="line">        iters = iters + <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">if</span>(is.integer(iterations))</span><br><span class="line">            <span class="hljs-keyword">if</span>(iters &gt; iterations)</span><br><span class="line">                <span class="hljs-keyword">break</span>;</span><br><span class="line">        <span class="hljs-keyword">if</span>(sum(abs(old_theta - theta)) &lt; tol)</span><br><span class="line">            <span class="hljs-keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    list(weights = as.vector(theta),iters = iters)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>AdaGrad的计算结果如下：</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">AdaGrad(bigdata)</span><br><span class="line">$weights</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">2.008219</span> <span class="hljs-number">2.005682</span></span><br><span class="line"></span><br><span class="line">$iters</span><br><span class="line">[<span class="hljs-number">1</span>] <span class="hljs-number">6579</span></span><br></pre></td></tr></table></figure>
<h2 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h2><p>AdaDelta是在AdaGrad的基础上发展而来的，目的是解决AdaGrad算法中学习率的单调减少问题。AdaDelta不再采用累积梯度平方和的方法来调整学习率，而是根据一些固定的$w$的大小来限制过去累积梯度的窗口。</p>
<p>AdaDelta不再无效率地存储历史梯度的平方和，而将历史梯度平方和定义为衰减均值(decaying average)。第$t$步的移动平均值$E[g^2]_t$仅仅取决于过去的平均值和当前梯度（有点类似于momentum）：</p>
<script type="math/tex; mode=display">E[g^2]_t = \gamma E[g^2]_{t-1} + (1-\gamma)g_t^2</script><p>同样的，我们把$\gamma$设置在0.9附近。为清楚起见，我们重新写下更新规则：</p>
<blockquote>
<script type="math/tex; mode=display">\Delta \theta _t = - \eta \cdot g_{t,i}</script><script type="math/tex; mode=display">\theta _{t+1} = \theta_t + \Delta \theta _t</script></blockquote>
<p>根据AdaGrad我们推出AdaDelta的参数更新公式：</p>
<blockquote>
<script type="math/tex; mode=display">\Delta \theta _t = - \frac{\eta}{\sqrt{G_t + \epsilon}} \odot g_t</script></blockquote>
<p>我们只需把$G_t$替换$E[g^2]_t$就行了：</p>
<blockquote>
<script type="math/tex; mode=display">\theta_t = - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}}</script></blockquote>
<p>因为分母刚好是梯度的均方根，我们将其简写为：</p>
<blockquote>
<script type="math/tex; mode=display">\Delta \theta_t = -\frac{\eta}{RMS[g]_t}g_t</script></blockquote>
<p>作者们发现上述更新中的单位不一致（可以理解为量纲不一致），因此，他们定义了另外一个指数衰减均值，这次不用梯度的平方了，而是用参数的平方来进行更新：</p>
<script type="math/tex; mode=display">E[\Delta \theta^2]_t = \gamma E[\Delta \theta^2]_{t-1} + (1-\gamma)\Delta\theta_t^2</script><p>参数更新的均方误差即：</p>
<script type="math/tex; mode=display">RMS[\Delta\theta]_t = \sqrt{E[\Delta\theta^2]_t + \epsilon}</script><p>因为$RMS[\theta]_t$是未知的，我们可以用之前所有的更新过的参数的RMS来代替。将之前的学习率$\eta$换成$RMS[\Delta\theta]_{t-1}$，那么，我们得出AdaDelta的更新规则：</p>
<script type="math/tex; mode=display">\Delta\theta_t = -\frac{RMS[\Delta\theta]_{t-1}}{RMS[g]_t}g_t</script><script type="math/tex; mode=display">\theta_{t+1} = \theta_t + \Delta\theta_t</script><p>AdaDelta甚至不需要初始化学习率，因为在更新规则中已经不见它的身影了。</p>
<p>根据上述思路，我们写出AdaDelta的函数:</p>
<figure class="highlight r hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">RMS &lt;- <span class="hljs-keyword">function</span>(x)</span><br><span class="line">&#123;</span><br><span class="line">    sqrt(mean(x^<span class="hljs-number">2</span>) - mean(x)^<span class="hljs-number">2</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">AdaDelta &lt;- <span class="hljs-keyword">function</span>()</span><br></pre></td></tr></table></figure>
<h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p><strong>上述所有改进方法均可以运用于批量梯度下降、小批量梯度下降和随机梯度下降！</strong></p>
<p>[1]<a href="http://netedu.xauat.edu.cn/jpkc/netedu/jpkc/gdsx/homepage/5jxsd/51/513/5308/530807.htm" target="_blank" rel="noopener">http://netedu.xauat.edu.cn/jpkc/netedu/jpkc/gdsx/homepage/5jxsd/51/513/5308/530807.htm</a></p>
<p>[2]<a href="https://www.kdnuggets.com/2017/04/simple-understand-gradient-descent-algorithm.html" target="_blank" rel="noopener">https://www.kdnuggets.com/2017/04/simple-understand-gradient-descent-algorithm.html</a></p>
<p>[3]<a href="https://distill.pub/2017/momentum/" target="_blank" rel="noopener">https://distill.pub/2017/momentum/</a></p>
<p>[4]<a href="http://ruder.io/optimizing-gradient-descent/index.html" target="_blank" rel="noopener">http://ruder.io/optimizing-gradient-descent/index.html</a></p>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2018-05-02T16:00:00.000Z">2018-05-03</time>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    24 分钟 读完 (大约 3638 个字)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2018/05/03/逻辑回归/">逻辑回归</a>
            
        </h1>
        <div class="content">
            <h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>逻辑回归常用来处理分类问题，最常用来处理二分类问题。</p>
<p>生活中经常遇到具有两种结果的情况（冬天的北京会下雪，或者不会下雪；暗恋的对象也喜欢我，或者不喜欢我；今年的期末考试会挂科，或者不会挂科……）。对于这些二分类结果，我们通常会有一些输入变量，或者是连续性，或者是离散型。那么，我们怎样来对这些数据建立模型并且进行分析呢？</p>
<p>我们可以尝试构建一种规则来根据输入变量猜测二分输出变量，这在统计机器学上被称为分类。然而，简单的给一个回答“是”或者“不是”显得太过粗鲁，尤其是当我们没有完美的规则的时候。总之呢，我们不希望给出的结果就是武断的“是”或“否”，我们希望能有一个概率来表示我们的结果。</p>
<p>一个很好的想法就是，在给定输入$X$的情况下，我们能够知道Y的条件概率$Pr(Y|X)$。一旦给出了这个概率，我们就能够知道我们预测结果的准确性。</p>
<p>让我们把其中一个类称为1，另一个类称为0。（具体哪一个是1，哪一个是0都无所谓）。$Y$变成了一个指示变量，现在，你要让自己相信，$Pr(Y=1)=EY$，类似的，$Pr(Y=1|X=x)=E[Y|X=x]$。</p>
<blockquote>
<p>假设$Y$有10个观测值，分别为 0 0 0 1 1 0 1 0 0 1.即6个0,4个1.那么，$Pr(Y=1)=\frac{count(1)}{count(n)}=\frac{4}{10}=0.4$，同时，$EY=\frac{sum(Y)}{count(n)}=\frac{4}{10}=0.4$</p>
</blockquote>
<p>换句话说，条件概率是就是指示变量（即$Y$)的条件期望。这对我们有帮助，因为从这个角度上，我们知道所有关于条件期望的估计。我们要做的最直接的事情是挑选出我们喜欢的平滑器，并估计指示变量的回归函数，这就是条件概率函数的估计。</p>
<p>有两个理由让我们放弃陷入上述想法。</p>
<ol>
<li>概率必须介于0和1之间，但是我们在上面估计出来的平滑函数的输出结果却不能保证如此，即使我们的指示变量$y_i$不是0就是1；</li>
<li>另一种情况是，我们可以更好地利用这个事实，即我们试图通过更显式地模拟概率来估计概率。</li>
</ol>
<p>假设$Pr(Y=1|X=x)=p(x;\theta)$,$p$是参数为$\theta$的函数。进一步，假设我们的所有观测都是相互独立的，那么条件似然函数可以写成：</p>
<script type="math/tex; mode=display">\prod _{i=1}^nPr(Y=y_i|X=x_i)=\prod _{i=1}^np(x_i;\theta)^{y_i}(1-p(x_i;\theta))^{1-y_i}</script><p>回忆一下，对于一系列的伯努利试验$y_1,y_2,\cdots,y_n$，如果成功的概率都是常数$p$，那么似然概率为：</p>
<script type="math/tex; mode=display">\prod _{i=1}^n p^{y_i}(1-p)^{1-y_i}</script><p>我们知道，当$p=\hat{p}=\frac{1}{n}\sum _{i=1}^ny_i$时，似然概率取得最大值。如果每一个试验都有对应的成功概率$p_i$，那么似然概率就变成了</p>
<script type="math/tex; mode=display">\prod _{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i}</script><p>不添加任何约束的通过最大化似然函数来估计上述模型是没有意义的。当$\hat{p_i}=1$的时候，$y_i=1$，当$\hat{p_i}=0$的时候，$y_i=0$。我们学不到任何东西。如果我们假设所有的$p_i$不是任意的数字，而是相互连接在一起的，这些约束给我们提供了一个很重要的参数，我们可以通过这个约束来求得似然函数的最大值。对于我们正在讨论的这种模型，约束条件就是$p_i=p(x_i;\theta)$，当$x_i$相同的时候，$p_i$也必须相同。因为我们假设的$p$是未知的，因此似然函数是参数为$\theta$的函数，我们可以通过估计$\theta$来最大化似然函数。</p>
<h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p>总结一下：我们有一个二分输出变量$Y$，我们希望构造一个关于$x$的函数来计算$Y$的条件概率$Pr（{Y=1|X=x}）$，所有的未知参数都可以通过最大似然法来估计。到目前为止，你不会惊讶于发现统计学家们通过问自己“我们如何使用线性回归来解决这个问题”。</p>
<ul>
<li>最简单的一个想法就是令$p(x)$为线性函数。无论$x$在什么位置，$x$每增加一个单位，$p$的变化量是一样的。由于线性函数不能保证预测结果位于0和1之间，因此从概念上线性函数就不适合。另外，在很多情况下，根据我们的经验可知，当$p$很大的时候，对于$p$的相同的变化量，$x$的变化量将会大于$p$在1/2附近的变化量。线性函数做不到这样。</li>
<li>另一个最直观的想法是令$log\ p(x)$为$x$的线性函数。但是对数函数只在一个方向上是无界的，因此也不符合要求。</li>
<li>最后，我们选择了$p$经过logit变换以后的函数$ln\frac{p}{1-p}$。这个函数就很好啊，满足了我们的所有需求。</li>
</ul>
<p>最后一个选择就是我们所说的逻辑回归。一般的，逻辑回归的模型可以表示为如下形式：</p>
<script type="math/tex; mode=display">ln\frac{p(x)}{1-p(x)}=\beta_0+x\beta</script><p>根据上式，解出$p$</p>
<script type="math/tex; mode=display">p(x;b,w)=\frac{e^{\beta_0+x\beta}}{1+e^{\beta_0+x\beta}}=\frac{1}{1+e^{-(\beta_0+x\beta)}}</script><p>为了最小化错分率，当$p\ge 0.5$的时候，我们预测$Y=1$，否则$Y=0$。这意味着当$\beta_0+x\beta$非负的时候，预测结果为1，否则为预测结果为0.因此，逻辑回归为我么提供了一个线性分类器。决策边界是$\beta_0+x\beta=0$，当$x$是一维的时候，决策边界是一个点，当$x$是二维的时候，决策边界是一条直线，以此类推。空间中某个点到决策边界的距离为$\beta_0/||\beta||+x\cdot\beta/||\beta||$.逻辑回归不仅告诉我们两个类的决策边界，还以一种独特的方式根据样本点到决策边界的距离给出该点分属于某类的概率。当$||\beta||$越大的时候，概率取极端值（0或1）的速度就越快。上述说明使得逻辑回归不仅仅是一个分类器，它能做出更简健壮、更详细的预测，并能以一种不同的方式进行拟合;但那些强有力的预测可能是错误的。</p>
<h2 id="似然函数和逻辑回归"><a href="#似然函数和逻辑回归" class="headerlink" title="似然函数和逻辑回归"></a>似然函数和逻辑回归</h2><p>因为逻辑回归的预测结果是概率，而不是类别，因此我们可以用似然函数来拟合模型。对于每一个样本点，我们有一个特征向量$x_i$，这个向量的维度就是特征的个数。同时还有一个观测类别$y_i$。当$y_i=1$的时候，该类的概率为$p$，否则为$1-p$。因此，似然函数为：</p>
<script type="math/tex; mode=display">L(\beta_0,\beta) = \prod _{i=1}^np(x_i)^{y_i}(1-p(x_i))^{1-y_i}</script><p>对数似然函数为：</p>
<script type="math/tex; mode=display">\begin{aligned} \ell(\beta_0,\beta) &= \sum_{i=1}^ny_iln(p(x_i))+(1-y_i)ln(1-p(x_i)) \\&=\sum_{i=1}^n(y_i(ln\frac{p(x_i)}{1-p(x_i)})+ln(1-p(x_i))) \\&=\sum_{i=1}^ny_i(\beta_0+x_i\beta)-ln(1+e^{\beta_0+x_i\beta}) \end{aligned}</script><p> 为了表示方便，统一将$\beta_0,\beta$表示成$\beta$,则$\ell$对$\beta$的一阶导数为： </p>
<script type="math/tex; mode=display">\begin{aligned} \frac{\partial \ell}{\partial \beta} &=\sum _{i=1}^n[y_i-\frac{e^{\beta^Tx_i}}{1+\beta^Tx_i}]x_i\\& = \sum _{i=1}^n(y_i-p_i)x_i\end{aligned}</script><h2 id="多分类逻辑回归"><a href="#多分类逻辑回归" class="headerlink" title="多分类逻辑回归"></a>多分类逻辑回归</h2><p>如果$Y$有多个类别，我们仍然可以使用逻辑回归。假如有$k$个类别，分别是$0,1,\cdots,k-1$，对于每一个类$k$，其都有对应的$\beta_0$和$\beta$，每个类对应的概率为:</p>
<script type="math/tex; mode=display">P(Y=c|X=x)=\frac{e^{\beta_0^c+x\beta^c}}{\sum e^{\beta_0^c+x\beta^c}}</script><p>观察上式可以发现，二分类逻辑回归求是多分类逻辑回归的特例.</p>
<blockquote>
<p>在这里，读者可能比较好奇，根据上式，二分类逻辑回归的分母中的1是怎么来的。其实，无论有多少个类，我们总是将第一类的系数设置为0，那么类别为0的那部分在分母中对应的就是1.这样做对模型的通用性没有任何影响。</p>
<p>有读者可能会问，为什么偏要把第一个类的系数设置为0，而不是其他的类。事实上，你可以设置任何一个类的系数为0，并且最终计算出来的结果都是一样的。所以，按照惯例，我们都是把第一个类的系数设置为0.</p>
</blockquote>
<h2 id="牛顿法求解参数"><a href="#牛顿法求解参数" class="headerlink" title="牛顿法求解参数"></a>牛顿法求解参数</h2><p>为了求出待估参数$\beta$，我们利用Newton-Raphson算法。首先对对数似然函数求二阶偏导： </p>
<script type="math/tex; mode=display">\frac{\partial ^2\ell (\beta)}{\partial \beta \partial \beta^T}=-\sum_{i=1}^nx_ix_i^Tp_i(1-p_i)</script><p>注意，上面的$x_i$是个向量，也就是上面所说的特征向量，维度为特征个数加一。即假设原始数据为$n\times m$矩阵，其中n表示观测数，m表示特征数。则$x_i$的长度为m+1。根据上述说明，上面的二阶偏导实际上是一个$(m+1)\times (m+1)$的矩阵。</p>
<p>如果给定一个$\hat{\beta}^{old}$，则一步牛顿迭代为（梯度下降）：</p>
<script type="math/tex; mode=display">\hat{\beta}^{new}=\hat{\beta}^{old}-(\frac{\partial ^2\ell (\beta)}{\partial \beta \partial \beta^T})^{-1} \cdot\frac{\partial \ell({\beta})}{\partial \beta}</script><p>将上述式子表示成矩阵的形式就是：</p>
<script type="math/tex; mode=display">\frac{\partial \ell(\beta)}{\partial \beta}=X^T(y-p)</script><script type="math/tex; mode=display">\frac{\partial ^2\ell (\beta)}{\partial \beta \partial \beta^T}=-X^TWX</script><p>其中，$X$为原始自变量矩阵，$y$为类别向量，$p$为预测概率向量，$W$是一个$n\times n$对角矩阵，第$i$个元素取值为$p(x_i,\hat{\beta}^{old})(1-p(x_i,\hat{\beta}^{old}))$.</p>
<p>联立上述两个式子，可以得出参数的迭代公式：</p>
<script type="math/tex; mode=display">\begin{aligned}\hat{\beta}^{new} &=\hat{\beta}^{old}+(X^TWX)^{-1}X^T(y-p) \\ &=(X^TWX)^{-1}X^TW(X\hat{\beta^{old}}+W^{-1}(y-p)) \\ &=(X^TWX)^{-1}X^TWz \end{aligned}</script><p>其中，$z=X\hat{\beta^{old}}+W^{-1}(y-p)$.</p>
<p>实际上，$X^TWX$是一个黑塞矩阵</p>
<script type="math/tex; mode=display">H(\beta) = \frac{\partial ^2\ell (\beta)}{\partial \beta \partial \beta^T}</script><p>即目标函数对$\beta$的二阶偏导，那么，上述迭代公式也可以写作：</p>
<script type="math/tex; mode=display">\begin{aligned} \hat{\beta}^{new} &=\hat{\beta}^{old} - H(\beta)^{-1}\frac{\partial \ell(\beta)}{\beta} \\ &= \hat{\beta}^{old} - H(\beta)^{-1}X^T(y-p) \end{aligned}</script><p>上述是我们熟悉的牛顿迭代公式。</p>
<p>矩阵相乘的计算不算复杂，但是当数据量上升以后，黑塞矩阵的求逆就非常复杂了，因此衍生出许多拟牛顿算法，本节不讨论优化算法。</p>
<p>很明显，本例的目标函数就是对数似然函数$\ell(\beta)$，也就是求其最大值。然而，很多同学已经习惯了牛顿法求最小值，因此，为了大家看着方便，下面介绍梯度下降法求解逻辑回归。</p>
<p>只需要在上述似然函数前面加一个负号，本例就变成了一个梯度下降的问题了。为了形式上好看，还可以在前面对数似然函数求一个均值，即除以样本量。</p>
<p>假设$J(\beta)$是我们的目标函数，则</p>
<script type="math/tex; mode=display">
\begin{aligned}
J(\beta) &= -\frac{1}{n}\ell(\beta) \\
&=-\frac{1}{n}\sum_{i=1}^ny_iln(p(x_i))+(1-y_i)ln(1-p(x_i))
\end{aligned}</script><p>此时我们的梯度公式就变成了：</p>
<script type="math/tex; mode=display">\frac{\partial J(\beta)}{\partial \beta}=-\frac{1}{n}\sum _{i=1}^n(y_i-p_i)x_i=\frac{1}{n}\sum _{i=1}^n(p_i-y_i)x_i</script><p>我们的二阶偏导数就变成了</p>
<script type="math/tex; mode=display">\frac{\partial ^2J(\beta)}{\partial \beta\partial\beta^T} =\frac{1}{n} \sum_{i=1}^nx_ix_i^Tp_i(1-p_i)</script><p>那么，此时为了求得我们的回归系数，即求使得$J(\beta)$最小的系数。牛顿迭代公式就变成了：</p>
<script type="math/tex; mode=display">\hat{\beta}^{new} = \hat{\beta}^{old}-H^{-1}\nabla=\hat{\beta}^{old}-\frac{1}{n}(X^T\cdot diag(p)\cdot diag(1-p) \cdot X)^{-1}\cdot \frac{1}{n}X^T(p-y)</script><p>按照上述思路，编程实现逻辑回归求解是比较简单的。</p>
<figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#迭代函数</span></span><br><span class="line"><span class="hljs-keyword">from</span> numpy <span class="hljs-keyword">import</span> *</span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span><span class="hljs-params">(x)</span>:</span></span><br><span class="line">    <span class="hljs-keyword">return</span> <span class="hljs-number">1.0</span>/(<span class="hljs-number">1.0</span>+exp(-x))</span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">LogitReg</span><span class="hljs-params">(x,y,tol = <span class="hljs-number">0.001</span>,maxiter = <span class="hljs-number">1000</span>)</span>:</span></span><br><span class="line">    samples,features = x.shape  <span class="hljs-comment">#分别表示观测样本数量和特征数量</span></span><br><span class="line">    features += <span class="hljs-number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment">#全部转换为矩阵</span></span><br><span class="line">    xdata = array(ones((samples,features)))</span><br><span class="line">    xdata[:,<span class="hljs-number">0</span>] = <span class="hljs-number">1</span></span><br><span class="line">    xdata[:,<span class="hljs-number">1</span>:] = x</span><br><span class="line">    xdata = mat(xdata)  <span class="hljs-comment">#sample行，features列的输入</span></span><br><span class="line">    </span><br><span class="line">    y = mat(y.reshape(samples,<span class="hljs-number">1</span>))  <span class="hljs-comment">#label，一个长度为samples的向量</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment">#首先初始化beta，令所有的系数为1,生成一个长度为features的列向量</span></span><br><span class="line">    beta = mat(zeros((features,<span class="hljs-number">1</span>)))</span><br><span class="line">    </span><br><span class="line">    iternum = <span class="hljs-number">0</span> <span class="hljs-comment">#迭代计数器</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment">#计算初始损失</span></span><br><span class="line">    </span><br><span class="line">    loss0 = float(<span class="hljs-string">'inf'</span>)</span><br><span class="line">    J = []</span><br><span class="line">    <span class="hljs-keyword">while</span> iternum &lt; maxiter:</span><br><span class="line">        <span class="hljs-keyword">try</span>:</span><br><span class="line">            p = sigmoid(xdata*beta) <span class="hljs-comment">#计算似然概率</span></span><br><span class="line">            nabla = <span class="hljs-number">1.0</span>/samples*xdata.T*(p-y)   <span class="hljs-comment">#计算梯度</span></span><br><span class="line">            H = <span class="hljs-number">1.0</span>/samples*xdata.T*diag(p.getA1())* diag((<span class="hljs-number">1</span>-p).getA1())*xdata  <span class="hljs-comment">#计算黑塞矩阵</span></span><br><span class="line">            </span><br><span class="line">            loss = <span class="hljs-number">1.0</span>/samples*sum(-y.getA1()*log(p.getA1())-(<span class="hljs-number">1</span>-y).getA1()*log((<span class="hljs-number">1</span>-p).getA1())) <span class="hljs-comment">#计算损失</span></span><br><span class="line">            J.append(loss)</span><br><span class="line">            beta =beta -  H.I * nabla  <span class="hljs-comment">#更新参数</span></span><br><span class="line">            iternum += <span class="hljs-number">1</span> <span class="hljs-comment">#迭代器加一</span></span><br><span class="line">            <span class="hljs-keyword">if</span> loss0 - loss &lt; tol:</span><br><span class="line">                <span class="hljs-keyword">break</span></span><br><span class="line">            loss0 = loss</span><br><span class="line">        <span class="hljs-keyword">except</span>:</span><br><span class="line">            H = H + <span class="hljs-number">0.0001</span></span><br><span class="line">            <span class="hljs-comment">#通常当黑塞矩阵奇异的时候，将矩阵加上一个很小的常数。</span></span><br><span class="line">            <span class="hljs-keyword">break</span></span><br><span class="line">        </span><br><span class="line">    <span class="hljs-keyword">return</span> beta,J</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#预测函数</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predictLR</span><span class="hljs-params">(data,beta)</span>:</span></span><br><span class="line">    data = array(data)</span><br><span class="line">    <span class="hljs-keyword">if</span> len(data.shape) == <span class="hljs-number">1</span>:</span><br><span class="line">        length = len(data)</span><br><span class="line">        newdata = tile(<span class="hljs-number">0</span>,length+<span class="hljs-number">1</span>)</span><br><span class="line">        newdata[<span class="hljs-number">0</span>] = <span class="hljs-number">1</span></span><br><span class="line">        newdata[<span class="hljs-number">1</span>:] = data</span><br><span class="line">        newdata = mat(newdata)</span><br><span class="line">        <span class="hljs-keyword">pass</span></span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        shape = data.shape</span><br><span class="line">        newdata = zeros((shape[<span class="hljs-number">0</span>],shape[<span class="hljs-number">1</span>]+<span class="hljs-number">1</span>))</span><br><span class="line">        newdata[:,<span class="hljs-number">0</span>] = <span class="hljs-number">1</span></span><br><span class="line">        newdata[:,<span class="hljs-number">1</span>:] = data</span><br><span class="line">        newdata = mat(newdata)</span><br><span class="line">    <span class="hljs-keyword">return</span> sigmoid(newdata*beta)</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="hljs-string">'df.csv'</span>,header=<span class="hljs-keyword">None</span>)</span><br><span class="line">df = array(df)</span><br><span class="line">df.shape</span><br><span class="line">xdata = df[:,:<span class="hljs-number">3</span>]</span><br><span class="line">ydata = df[:,<span class="hljs-number">3</span>]</span><br><span class="line"></span><br><span class="line">beta,J = LogitReg(xdata,ydata) <span class="hljs-comment">#拟合</span></span><br><span class="line"></span><br><span class="line">testdata = xdata[<span class="hljs-number">1</span>:<span class="hljs-number">10</span>,]</span><br><span class="line"></span><br><span class="line">predictLR(testdata,beta)</span><br><span class="line"></span><br><span class="line">matrix([[ <span class="hljs-number">0.4959212</span> ],</span><br><span class="line">        [ <span class="hljs-number">0.44642627</span>],</span><br><span class="line">        [ <span class="hljs-number">0.47419207</span>],</span><br><span class="line">        [ <span class="hljs-number">0.42209742</span>],</span><br><span class="line">        [ <span class="hljs-number">0.41802565</span>],</span><br><span class="line">        [ <span class="hljs-number">0.51283217</span>],</span><br><span class="line">        [ <span class="hljs-number">0.44833226</span>],</span><br><span class="line">        [ <span class="hljs-number">0.41252982</span>],</span><br><span class="line">        [ <span class="hljs-number">0.47853786</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><a href="http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf" target="_blank" rel="noopener">http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf</a></p>
<p>[1]周志华.机器学习[M].清华大学出版社,2016.</p>
<p>[2]李航著.统计学习方法[M].清华大学出版社,2012.</p>
</blockquote>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2018-04-03T17:15:49.176Z">2018-04-04</time>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    几秒 读完 (大约 0 个字)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2018/04/04/线性模型/">线性模型</a>
            
        </h1>
        <div class="content">
            
        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2018-04-03T17:15:49.176Z">2018-04-04</time>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    几秒 读完 (大约 0 个字)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2018/04/04/贝叶斯分类器/">贝叶斯分类器</a>
            
        </h1>
        <div class="content">
            
        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2018-04-03T17:15:49.176Z">2018-04-04</time>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    几秒 读完 (大约 29 个字)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2018/04/04/降维方法/">降维方法</a>
            
        </h1>
        <div class="content">
            <h2 id="线性判别分析-LDA"><a href="#线性判别分析-LDA" class="headerlink" title="线性判别分析(LDA)"></a>线性判别分析(LDA)</h2><h2 id="主成分分析-PCA"><a href="#主成分分析-PCA" class="headerlink" title="主成分分析(PCA)"></a>主成分分析(PCA)</h2><h2 id="奇异值分解-SVD"><a href="#奇异值分解-SVD" class="headerlink" title="奇异值分解(SVD)"></a>奇异值分解(SVD)</h2><h2 id="非负矩阵分解-NMF"><a href="#非负矩阵分解-NMF" class="headerlink" title="非负矩阵分解(NMF)"></a>非负矩阵分解(NMF)</h2><h2 id="tsne分解"><a href="#tsne分解" class="headerlink" title="tsne分解"></a>tsne分解</h2>
        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2018-04-03T17:15:49.176Z">2018-04-04</time>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    31 分钟 读完 (大约 4590 个字)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2018/04/04/集成学习/">集成学习</a>
            
        </h1>
        <div class="content">
            <h2 id="个体与集成"><a href="#个体与集成" class="headerlink" title="个体与集成"></a>个体与集成</h2><blockquote>
<p>Hoeffding不等式：给定m个取值在[0,1]区间的独立随机变量$x_1,x_2,\cdots,x_n$，对任意$\epsilon &gt; 0$有如下等式成立：</p>
<script type="math/tex; mode=display">P(|\frac{1}{m}\sum _{1=1}^mx_i-\frac{1}{m}E(x_i)|\ge \epsilon) \le 2e^{-2m\epsilon^2}</script></blockquote>
<h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p>提升（boosting）方法是一种常用的机器学习方法，应用广泛且有效。在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。</p>
<h3 id="提升方法的基本思路"><a href="#提升方法的基本思路" class="headerlink" title="提升方法的基本思路"></a>提升方法的基本思路</h3><p>提升方法基于这样一种思想：对于一个复杂的任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好。实际上，就是“三个臭皮匠赛过诸葛亮”的道理。</p>
<p>历史上，Kearns和Valiant首先提出了“强可学习（strong learnable）”和“弱可学习（weakly learnable）”的概念。指出：在概率近似正确学习的框架中，一个概念（一个类），如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的；一个概念，如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。非常有趣的是Schapire后来证明强可学习与弱可学习是等价的，也就是说，在PAC学习的框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。</p>
<p>这样一来，问题便成为，在学习中，如果已经发现了“弱学习算法”，那么能否将它提升（boost）为“强学习算法”。大家知道，发现弱学习算法比发现强学习算法容易得多。那么如何具体实施提升，便成为开发提升方法时所要解决的问题。关于提升方法的研究很多，有很多算法被提出。最具有代表性的是AdaBoost算法。</p>
<p>对于分类问题而言，给定一个训练样本集，求比较粗糙的分类规则（弱分类器）要比求精确的分类规则（强分类器）容易的多。提升方法就是从弱学习算法出发，反复学习，得到一系列弱分类器，然后组合这些弱分类器，构成一个强分类器。大多数的提升方法都是改变训练数据的概率分布（训练数据的权值分布），针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。</p>
<p>这样，对提升方法来说，有两个问题需要回答：一是在每一轮如何改变数据的权值或概率分布；二是如何将弱分类器组合为一个强分类器。关于第1个问题，AdaBoost的做法是，提高那些被前一轮弱分类器分错的样本权值，而降低那些被正确分类样本的权值。这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注。于是，分类问题被一系列弱分类器“分而治之”。至于第2个问题，即弱分类器的组合，AdaBoost采取加权多数表决的方法。具体地，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。</p>
<p>AdaBoost的巧妙之处在于它将这些想法自然且有效地实现在一种算法里。</p>
<h3 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h3><p>现在叙述AdaBoost算法。假设给定一个二分类的训练数据集</p>
<script type="math/tex; mode=display">T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}</script><p>其中，每个样本点由实例与标记组成。实例$x_i\in R^n$,标记$y_i\in \{-1,1\}$。AdaBoost利用以下算法，从训练数据中学习一系列弱分类器或基本分类器，并将这些弱分类器线性组合为一个强分类器。</p>
<p>输入：训练数据集$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$,其中$x_i\in R^n$,标记$y_i\in \{-1,1\}$；弱分类算法。</p>
<p>输出：最终分类器$G(x)$。</p>
<ol>
<li>初始化训练数据的权值分布，</li>
</ol>
<script type="math/tex; mode=display">D_1=(w_{11},\cdots,w_{1i},\cdots,w_{1N}),w_{1i}=\frac{1}{N},i=1,2,\cdots,N</script><ol>
<li>对$m=1,2,\cdots,M$  </li>
</ol>
<ul>
<li>使用具有权值分布$D_m$的训练数据集学习，得到基本分类器</li>
</ul>
<blockquote>
<script type="math/tex; mode=display">G_m(x):R^n \rightarrow \{-1,1\}</script></blockquote>
<ul>
<li>计算$G_m(x)$在训练数据上的分类误差率</li>
</ul>
<blockquote>
<script type="math/tex; mode=display">e_m=\sum _{i=1}^NP(G_m(x)\ne y_i)=\sum _{i=1}^Nw_{mi}I(G_m(x)\ne y_i)</script></blockquote>
<ul>
<li>计算$G_m(x)$的系数</li>
</ul>
<blockquote>
<script type="math/tex; mode=display">\alpha _m = \frac{1}{2} ln \frac{1-e_m}{e_m}</script></blockquote>
<ul>
<li>更新训练数据集的权值分布</li>
</ul>
<blockquote>
<script type="math/tex; mode=display">D_{m+1} = (w_{m+1,1},\cdots,w_{m+1,i},\cdots,w_{m+1,N})</script><script type="math/tex; mode=display">w_{m+1,i} = \frac{w_{mi}}{Z_m}e^{-\alpha _m y_i G_m(x_i)},i=1,2,\cdots,N</script></blockquote>
<p>这里，$Z_m$是规范化因子</p>
<script type="math/tex; mode=display">Z_m = \sum _{i=1}^N e^{-\alpha _m y_i G_m(x_i)}</script><p>它使$D_{m+1}$成为一个概率分布。</p>
<ol>
<li>构建基本分类器的线性组合</li>
</ol>
<script type="math/tex; mode=display">f(x) = \sum _{m=1}^M \alpha_m G_m(x)</script><p>得到最终的分类器</p>
<script type="math/tex; mode=display">G(X) = sign(f(x)) = sign(\sum _{m=1}^M \alpha_m G_m(x))</script><p>对AdaBoost算法作如下说明：</p>
<p>步骤1 假设数据集具有均匀的权值分布，即每个训练样本在基本分类器中作用相同，这一假设保证第1步能够在原始数据集上学习基本分类器$G_1(x)$。</p>
<p>步骤2 AdaBoost反复学习基本分类器，在每一轮$m=1,2,\cdots,M$中顺次地执行下列操作：</p>
<p>（a）使用当前分布$D_m$加权的训练数据集，学习基本分类器$G_m(x)$。</p>
<p>（b）计算基本分类器$G_m(x)$在加权训练数据集上的分类误差率：</p>
<script type="math/tex; mode=display">e_m = \sum _{i=1}^N P(G_m(x_i)\ne y_i) = \sum _{G_m(x_i)\ne y_i} w_{mi}</script><p>这里，$w_{mi}$表示第$m$轮中第$i$个实例的权值，$\sum _{i=1}^N w_{mi}=1$。这表明，$G_m(x)$在加权的训练数据上的分类误差率是被$G_m(x)$误分类样本的权值之和，由此可以看出数据权值分布$D_m$与基本分类器$G_m(x)$的分类误差率的关系。</p>
<p>（c）计算基本分类器$G_m(x)$的系数$\alpha _m$，$\alpha _m$表示$G_m(x)$在最终分类器中的重要性。由(2)可知，当$e_m\le \frac{1}{2}$时，$\alpha_m \ge 0$，并且$\alpha_m$随着$e_m$的减小而增大，所以分类误差率越小的基本分类器在最终分类器中的所用越大。</p>
<p>（d）更新训练数据的权值分布为下一轮作准备，式(4)可以写成：</p>
<script type="math/tex; mode=display">w_{m+1,i}=\left\{\begin{aligned}\frac{w_{mi}}{Z_m}e^{-\alpha_m} &  & G_m(x_i)=y_i \\\frac{w_{mi}}{Z_m}e^{\alpha_m} &  & G_m(x_i) \ne y_i \end{aligned}\right.</script><p>由此可知，被基本分类器$G_m(x)$误分类样本的权值得以扩大，而被正确分类样本的权值却得以缩小。两相比较，由式(2)知误分类样本的权值被放大$e^{2\alpha_m}=\frac{1-e_m}{e_m}$倍。因此，误分类样本在下一轮学习中起更大的作用。不改变所给的训练数据，而不断改变训练数据的权值分布，使得训练数据在基本分类器的学习中起到不同的作用，这是AdaBoost的一个特点。</p>
<p>步骤3 线性组合$f(x)$实现$M$个基本分类器的加权表决。系数$\alpha_m$表示了基本分类器$G_m(x)$的重要性，这里，所有$\alpha_m$之和并不为1.$f(x)$的符号决定实例$x$的分类，$f(x)$的绝对值表示分类的确信度。利用基本分类器的线性组合构建最终分类器是AdaBoost的另一个特点。</p>
<h3 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h3><h3 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h3><p>XGBoost是”极端梯度上升”(Extreme Gradient Boosting)的简称,从技术上说，XGBoost是Extreme Gradient Boosting的缩写。它的流行源于在著名的Kaggle数据科学竞赛上被称为”奥托分类”的挑战。它可以处理多种目标函数，包括回归，分类和排序，是一个较为全面的分类器。<br>由于其他许多分类器，不管是强分类器或是集成分类器，在预测性能上的强大但是相对缓慢的实现，如上一章的Adaboost集成算法，不管是在在运行时间上还是在内存占有上开销都很大。XGBoost成为很多比赛的理想选择。XGBoost包还添加了做交叉验证和发现关键变量的额外功能。在优化模型时，这个算法使用了好几个参数。所以为了提高模型的表现，参数的调整十分必要。本节将讨论这些因素。</p>
<h4 id="XGBoost优势"><a href="#XGBoost优势" class="headerlink" title="XGBoost优势"></a>XGBoost优势</h4><p>XGBoost算法总结起来大致其有三个优点：高效、准确度、模型的交互性。</p>
<ul>
<li>正则化：标准GBDT提升树算法的实现没有像XGBoost这样的正则化步骤。正则化用于控制模型的复杂度，对减少过拟合也是有帮助的。XGBoost也正是以“正则化提升”技术而闻名。</li>
<li>并行处理：XGBoost可以实现并行处理，相比GBM有了速度的飞跃。不过，需要注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。XGBoost的并行是在特征粒度上的。决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点）。因此XGBoost在R重定义了一个自己数据矩阵类DMatrix。XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复利用索引地使用这个结构，获得每个节点的梯度，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</li>
<li>高度灵活性：XGBoost允许用户定义自定义优化目标和评价标准，它对模型增加了一个全新的维度，所以我们的处理不会受到任何限制。</li>
<li>缺失值处理：XGBoost内置处理缺失值的规则。 用户需要提供一个和其它样本不同的值，然后把它作为一个参数传进去，以此来作为缺失值的取值。XGBoost在不同节点遇到缺失值时采用不同的处理方法，并且会学习未来遇到缺失值时的处理方法。</li>
<li>剪枝：当分裂时遇到一个负损失时，传统GBDT会停止分裂。因此传统GBDT实际上是一个贪心算法。XGBoost会一直分裂到指定的最大深度(max_depth)，然后回过头来剪枝。如果某个节点之后不再有正值，它会去除这个分裂。这种做法的优点，当一个负损失（如-2）后面有个正损失（如+10）的时候，就显现出来了。GBM会在-2处停下来，因为它遇到了一个负值。但是XGBoost会继续分裂，然后发现这两个分裂综合起来会得到+8，因此会保留这两个分裂。</li>
<li>内置交叉验证：XGBoost允许在每一轮boosting迭代中使用交叉验证。因此，可以方便地获得最优boosting迭代次数。而传统的GBDT使用网格搜索，只能检测有限个值。</li>
</ul>
<h4 id="XGBoost算法推导"><a href="#XGBoost算法推导" class="headerlink" title="XGBoost算法推导"></a>XGBoost算法推导</h4><p>XGBoost 在函数空间中用牛顿法进行优化。首先，boosting是一种加法模型。XGBoost同样属于GBDT梯度提升法，模型的基分类器都包含有树，对于给定的数据集D={($x_i​$,$y_i​$)}，XGBoost进行additive learning，学习K棵树，采用以下函数对样本进行预测。</p>
<script type="math/tex; mode=display">\widehat{y}=\phi(x_i)=\sum_{k=1}^{K}f_{k}(x_i)\qquad f_k\in F</script><p>这里F是函数空间，$f(x)$是回归树CART。</p>
<script type="math/tex; mode=display">F=\{f(x)=w_{q(x)}\}(q:R^m\to T,w\in R)</script><p>$q(x)$标识将样本x分到了某个叶子节点上，$w$是叶子节点的分数，（leaf score），所以$w_q(x)$ 表示回归树对样本的预测值。<br>回归树的预测输出是实数分数，可以用于回归，分类，排序等任务中，对于回归问题，可以直接作为目标值，对于分类问题，需要映射成概率，比如采用逻辑函数，然后可以控制阈值，进行两种分类错误的把控。<br>XGBoost对传统的提升树算法Adaboost等的改进，在于在参数空间的目标函数中加入了正则化项，来惩罚模型的复杂程度，进而控制过拟合。和Adaboost一样都是通过最小化损失函数求解最优模型，并加入了阈值，如下公式所示：</p>
<script type="math/tex; mode=display">L(\phi)=\sum_{i}l(\widehat{y}_l,y_i)+\sum_{k}\Omega(f_k)</script><p>误差函数可以是square loss，logloss等，也可以自己定义损失函数，只要能够求出目标函数的梯度和Hessian矩阵，用户就可以自定义训练模型时的目标函数 这也正是XGBoost的优势之一，可以通过研究目的的不同自己定义损失函数，<br>在公式（3-3）中，相比于原始的GBDT，XGBoost的目标函数多了正则项，是学习出来的模型更加不容易过拟合。衡量树的复杂程度主要与树的深度，内部节点的个数，叶子节点的个数，叶子节点的权重有关，因此XGBoost对这些参量进行了约束。得出了正则项为：</p>
<script type="math/tex; mode=display">\Omega(f)=\gamma T+\frac{1}{2}\rho\|w\|^2</script><p>正则项对每棵树的复杂程度都应进行惩罚，对每个节点进行了复杂度的惩罚，从另一种角度来说也就进行了自动的剪枝。<br>另外，还可以选择使用线性模型替代树模型，从而得到带$L1+L2$惩罚的线性回归正则项可以是$L1$正则，$L2$正则 。<br>第$t$次迭代后，模型的预测等于前$t-1$次的模型预测加上第t棵树的预测。将目标函数在前$t-1$次的模型$y_i^{t-1}$处进行泰勒展开，并将常数项去掉即得到：</p>
<script type="math/tex; mode=display">L^{(t)}=\sum_{i=1}^{n}[g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]+\Omega(f_t)</script><p>公式中，$g_i=\delta_{\widehat{y}^{(t-1)}}l(y_i,\widehat{y}^{(t-1)})\qquad h_i=\delta^2_{\widehat{y}^{(t-1)}}l(y_i,\widehat{y}^{(t-1)})$<br>把$f_t$, $\Omega(f_t)$写成树结构的形式，得到：</p>
<script type="math/tex; mode=display">L^{(t)}=\sum_{i=1}^{n}[g_iw_{q(x_i)}+\frac{1}{2}h_iw^2_{q(x_i)}]+\gamma T+\rho\frac{1}{2}\sum_{j=i}^{T}w_j^2</script><p>则目标函数可以写成按叶节点累加的形式：</p>
<script type="math/tex; mode=display">L^{(t)}=\sum_{j=1}^{T}[G_iw_j+\frac{1}{2}(H_j+\rho)w_j^2]+\gamma T</script><p>如果确定了树的结构（即$q(x)$确定），为了使目标函数最小，可以令其导数为0，解得每个叶节点的最优预测分数为：</p>
<script type="math/tex; mode=display">W_j^*=-\frac{G_j}{H_j+\rho}</script><script type="math/tex; mode=display">\widehat{L}^*=-\frac{1}{2}\sum_{j=1}^{T}\frac{G_j^2}{H_j+\rho}+\gamma T</script><p>公式（3-9）的负部衡量了每个叶子节点对总体损失的的贡献，我们希望损失越小越好，则公式的（3-9）的负部值越大越好。<br>因此，对一个叶子节点进行分裂，分裂前后的增益定义为：</p>
<script type="math/tex; mode=display">Gain=\frac{G_L^2}{H_L+\rho}+\frac{G_R^2}{H_R+\rho}-\frac{(G_L+G_R)^2}{H_L+H_R+\rho}-\gamma</script><p>Gain的值越大，分裂后L减小越多。所以当对一个叶节点分割时，计算所有候选特征值所对应的gain，选取gain最大的进行分割。但由于精确遍历所有可能的分割点是效率很低的，所以，实际上XGBoost采用的是对于每个特征，不是简单地按照样本个数进行分位，而是以二阶导数值作为权重，进行分位点的选择，以此减少计算复杂度。在学习每棵树前，提出候选切分点，这也是XGBoost可以实现并行的原因之一，可以提前切割分位点。<br> 最后，XGBoost算法还借鉴了bagging的bootstrap自助法行抽样，还借鉴了随机森林的列抽样，即特征抽样。这样减少过拟合同时还降低了计算复杂度。</p>
<h2 id="Bagging-amp-随机森林"><a href="#Bagging-amp-随机森林" class="headerlink" title="Bagging &amp; 随机森林"></a>Bagging &amp; 随机森林</h2>
        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2018-04-03T17:15:49.161Z">2018-04-04</time>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    几秒 读完 (大约 0 个字)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2018/04/04/支持向量机/">支持向量机</a>
            
        </h1>
        <div class="content">
            
        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2018-04-03T17:15:49.161Z">2018-04-04</time>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    几秒 读完 (大约 0 个字)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2018/04/04/特征选择/">特征选择</a>
            
        </h1>
        <div class="content">
            
        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2018-04-03T17:15:49.145Z">2018-04-04</time>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    13 分钟 读完 (大约 1937 个字)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2018/04/04/决策树/">决策树算法</a>
            
        </h1>
        <div class="content">
            <h2 id="几种主要的决策树"><a href="#几种主要的决策树" class="headerlink" title="几种主要的决策树"></a>几种主要的决策树</h2><p>决策树算法的关键是选择最优划分属性，据此人们提出了三种决策树模型。</p>
<h3 id="ID3决策树"><a href="#ID3决策树" class="headerlink" title="ID3决策树"></a>ID3决策树</h3><p>信息熵(Information entropy)是度量样本集合纯度最常用的一种指标，假定当前样本集合$D$中第$k$类样本所占的比例为$p_k,(k=1,2,\cdots,n)$，则$D$的信息熵定义为：</p>
<script type="math/tex; mode=display">Ent(D)=-\sum _{k=1}^np_k\ log_2p_k</script><p>$Ent(D)$的值越小，则$D$的纯度越高</p>
<blockquote>
<p>其实我们在高中化学就接触过“熵”的概念，指的是物质的混乱程度。是纯度的对立面。我们说熵越大，混乱程度越高，也就是纯度越低。同理，熵越小，混乱程度越低，即纯度越高。 </p>
</blockquote>
<p>假设离散属性$a$有$V$个可能的取值$\{a^1,a^2,\cdots,a^V\}$,若使用$a$来对样本集$D$进行划分，则会产生$V$个分支结点，其中第$v$个分支结点包含了$D$中所有在属性$a$上取值为$a^v$的样本，记为$D^v$。我们可以根据上式计算法$D^v$的信息熵，再考虑到不同的分支结点所包含的样本数不同，给分支结点赋予权重$|D^v|/|D|$，即样本越多的分支结点的影响越大，于是计算出用属性$a$对样本集$D$进行划分所获得的“信息增益”：</p>
<script type="math/tex; mode=display">Gain(D,a) = Ent(D) - \sum _{v=1}^V \frac{|D^v|}{|D|}Ent(D^v)</script><p>一般来说，信息增益越大，意味着使用属性$a$来进行划分所获得的“纯度提升”越大。因此，我们总是选择使得信息增益最大的那个属性来进行划分。</p>
<p>我们用下面的例子来说明划分的过程。该数据是一份医学数据，根据病人的一些特征，给出佩戴硬质隐形眼镜、软质隐形眼镜和不佩戴隐形眼镜的建议。数据共有5个变量，其中4个自变量，1个因变量。</p>
<ul>
<li>age of patient：简称age，患者年龄，(1) young, (2) pre-presbyopic, (3) presbyopic</li>
<li>spectacle prescription：简称sp，视力情况，(1)近视myope，(2)远视hypermetrope</li>
<li>astigmatic：是否散光，(1) no, (2) yes</li>
<li>tear production rate:  简称tpr，眼泪生成率，(1) reduced, (2) normal</li>
<li>suggestion：1 : hard contact lenses,     2 : soft contact lenses,     3 : should not</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>id</th>
<th>age</th>
<th>sp</th>
<th>astigmatic</th>
<th>tpr</th>
<th>suggestion</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>4</td>
<td>1</td>
<td>2</td>
<td>2</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>5</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>6</td>
<td>2</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>7</td>
<td>2</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>8</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>9</td>
<td>3</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>10</td>
<td>3</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>11</td>
<td>3</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>12</td>
<td>3</td>
<td>2</td>
<td>2</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>13</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>14</td>
<td>1</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>15</td>
<td>2</td>
<td>1</td>
<td>2</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>16</td>
<td>3</td>
<td>1</td>
<td>2</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>17</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>18</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>19</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>20</td>
<td>2</td>
<td>2</td>
<td>1</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>21</td>
<td>3</td>
<td>2</td>
<td>1</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>22</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>3</td>
</tr>
<tr>
<td>23</td>
<td>3</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>3</td>
</tr>
<tr>
<td>24</td>
<td>3</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>3</td>
</tr>
</tbody>
</table>
</div>
<p>我们首先计算出总的信息熵：</p>
<script type="math/tex; mode=display">Ent(D) = \sum_{k=1}^3 -p_klog_2p_k = -(\frac{4}{24}log_2\frac{4}{24} + \frac{5}{24}log_2\frac{5}{24} + \frac{15}{24}log_2\frac{15}{24})=1.326</script><p>我们首先选择age作为分支结点，age有三个取值，分布为1,2,3。当age=1时，suggestion取值为2个1,2个2,4个3。则当age=1时，$D^1$的信息熵为：</p>
<script type="math/tex; mode=display">Ent(D^1) = \sum_{k=1}^3 -p_klog_2p_k=-(\frac{2}{8}log_2\frac{2}{8} + \frac{2}{8}log_2\frac{2}{8} + \frac{4}{8}log_2\frac{4}{8})=1.5</script><p>同理，</p>
<script type="math/tex; mode=display">Ent(D^2)== \sum_{k=1}^3 -p_klog_2p_k=-(\frac{1}{8}log_2\frac{1}{8} + \frac{2}{8}log_2\frac{2}{8} + \frac{5}{8}log_2\frac{5}{8})=1.299</script><script type="math/tex; mode=display">Ent(D^2)== \sum_{k=1}^3 -p_klog_2p_k=-(\frac{1}{8}log_2\frac{1}{8} + \frac{1}{8}log_2\frac{1}{8} + \frac{6}{8}log_2\frac{6}{8})=1.061</script><p>那么，age的信息增益为：</p>
<script type="math/tex; mode=display">\begin{aligned}Gain(D,age) = & Ent(D) - \sum _{v=1}^3\frac{|D^v|}{|D|}Ent(D^v) \\ =&   1.326 - (\frac{1}{3}\times 1.5 + \frac{1}{3}\times 1.299+ \frac{1}{3}\times 1.061)=0.0393\end{aligned}</script><p>类似的，我们可以计算出其他属性的信息增益：</p>
<script type="math/tex; mode=display">Gain(D,sp)=0.0395</script><script type="math/tex; mode=display">Gain(D,astigmatic ) = 0.377</script><script type="math/tex; mode=display">Gain(D,tpr) = 0.549</script><p>可以看到，$tpr$的信息增益最大，因此把它选为划分属性，下图表示基于$tpr$进行划分的结果。</p>
<p><img src="/picture/图2-1525346184642.png" alt="图2"></p>
<p>然后，决策树按照同样的规则，对上面两个已经生成的结点继续划分。这个时候，决策树有以下三个停止原则：</p>
<ul>
<li>当前结点包含的样本全部属于同一个类别，无需划分。上述左子树就是这种情况；</li>
<li>当前属性集为空，或是所有样本在所有属性上取值相同，无法划分。</li>
<li>当前结点包含的样本集合为空，不能划分。</li>
</ul>
<p>接下来，由于左子树无法继续划分，因此我们继续划分右子树。此时，右子树相当于根节点，我们计算其他三个属性的信息增益。</p>
<p>上述计算出的右子树的熵为1.555.</p>
<p>则有：</p>
<script type="math/tex; mode=display">Gain(D^2,age)=0.221</script><script type="math/tex; mode=display">Gain(D^2,sp)=0.095</script><script type="math/tex; mode=display">Gain(D^2,astigmatic) = 0.771</script><p>此轮选择的划分属性为$astigmatic$，下图表示继续划分的结果。</p>
<p><img src="/picture/1525347678542.png" alt="1525347678542"></p>
<p>截止目前，决策树还没结束，以上两个新的叶子节点包含的样本仍然归属于不同的类型，因此还需要继续进行划分。</p>
<p>我们仍然沿用上面的符号，根据计算可知，</p>
<script type="math/tex; mode=display">Ent(D^1) = 0.650</script><p>$Ent(D^2) = 0.918$</p>
<p>目前还剩下age和sp两个属性，计算出相应的信息增益：</p>
<script type="math/tex; mode=display">Gain(D^1,age) = 0.32</script><script type="math/tex; mode=display">Gain(D^1,sp) = 0.191</script><script type="math/tex; mode=display">Gain(D^2,age) = 0.251</script><script type="math/tex; mode=display">Gain(D^2,sp) = 0.459</script><p>因此，对于$D^1$，选择age进行划分，对于$D^2$，选择sp进行划分。划分结果如下。</p>
<p><img src="/picture/1525348835180.png" alt="1525348835180"></p>
<p>截止目前，我们的决策树还没有完全将样本划分开来。比如21和23分属于2和3。但是目前每个分支只剩下一个属性可以继续划分，因此我们不需要再计算信息增益，直接划分即可。</p>
<p><img src="/picture/1525349480833.png" alt="1525349480833"></p>
<p>上述每个叶子结点包含的样本都归属于同一类别，且所有的属性被用于划分。</p>
<blockquote>
<p>上面的决策树是根据所有样本进行划分的，因此无法用来进行预测。如果要用于预测，应该把数据分成训练集和测试集。</p>
</blockquote>
<p>ID3决策树就是根据信息增益来选择最优划分属性，然后构建决策树的。该算法简单易懂，十分容易上手。</p>
<h3 id="C4-5决策树"><a href="#C4-5决策树" class="headerlink" title="C4.5决策树"></a>C4.5决策树</h3><p>如果在上述划分的过程中，把id也作为一个候选属性参与划分，那么可计算出$Gain(D,id)=1.326$，是最大的。但是根据id会划分出24个分支，再出现一个新样本的话，则无法预测该样本属于哪一类，也就是泛化能力较差。</p>
<p>实际上，信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，C4.5算法使用增益率(gain ratio)来选择最有划分属性。延用上述符号，增益率定义为：</p>
<script type="math/tex; mode=display">Gain\_ratio(D,a) = \frac{Gain(D,a)}{IV(a)}</script><p>其中</p>
<script type="math/tex; mode=display">IV(a) = - \sum _{v=1}^V\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}</script><p>称为$a$的固有值。属性$a$的可能取值数目却大，则$IV(a)$的值通常会越大。</p>
<h3 id="CART决策树"><a href="#CART决策树" class="headerlink" title="CART决策树"></a>CART决策树</h3><p>CART树使用基尼指数(Gini index)来选择划分属性，延用上述符号，数据集$D$的纯度可用基尼值来度量：</p>
<script type="math/tex; mode=display">Gini(D) = \sum _{k=1}^n\sum _{k' \ne k}p_kp_{k'} = 1- \sum _{k=1}^n p_k^2</script><p>直观来说，$Gini(D)$反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率。因此，Gini(D)越小，则数据的纯度越高。</p>
<blockquote>
<p>假设所有的类别都是一样的，则Gini(D)=0,纯度最高</p>
</blockquote>
<p>那么，属性$a$的基尼指数定义为：</p>
<script type="math/tex; mode=display">Gini\_index(D,a)=\sum _{v=1}^V\frac{|D^v|}{|D|}Gini(D^v)</script><p>于是，我们在候选属性集合$A$中，选择那个使得划分后基尼指数最小的属性作为划分属性。</p>
<blockquote>
<p>可以看出，CART树的构建比前两种树都简单一点。</p>
</blockquote>
<h3 id="剪枝处理"><a href="#剪枝处理" class="headerlink" title="剪枝处理"></a>剪枝处理</h3><h4 id="前剪枝"><a href="#前剪枝" class="headerlink" title="前剪枝"></a>前剪枝</h4><h4 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h4>
        </div>
        
        
        
    </div>
</div>








</div>
                




<div class="column is-4-tablet is-4-desktop is-3-widescreen  has-order-1 column-left is-sticky">
    
        
<div class="card widget">
    <div class="card-content">
        <nav class="level">
            <div class="level-item has-text-centered">
                <div>
                    
                        <img class="image is-128x128 has-mb-6" src="/images/touxiang.jpg" alt="Feng Yangyang">
                    
                    
                    <p class="is-size-4 is-block">
                        Feng Yangyang
                    </p>
                    
                    
                    <p class="is-size-6 is-block">
                        Data Analyst
                    </p>
                    
                    
                    <p class="is-size-6 is-flex is-flex-center has-text-grey">
                        <i class="fas fa-map-marker-alt has-mr-7"></i>
                        <span>PuDong,ShangHai</span>
                    </p>
                    
                </div>
            </div>
        </nav>
        <nav class="level is-mobile">
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        文章
                    </p>
                    <p class="title has-text-weight-normal">
                        25
                    </p>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        分类
                    </p>
                    <p class="title has-text-weight-normal">
                        2
                    </p>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        标签
                    </p>
                    <p class="title has-text-weight-normal">
                        22
                    </p>
                </div>
            </div>
        </nav>
        <div class="level">
            <a class="level-item button is-link is-rounded" href="/" target="_blank">
                关注我</a>
        </div>
        
        
    </div>
</div>
    
        
    
        

<div class="card widget">
    <div class="card-content">
        <div class="menu">
        <h3 class="menu-label">
            链接
        </h3>
        <ul class="menu-list">
        
            <li>
                <a class="level is-mobile" href="http://pcgan.site/" target="_blank">
                    <span class="level-left">
                        <span class="level-item">甘医生</span>
                    </span>
                    <span class="level-right">
                        <span class="level-item tag">pcgan.site</span>
                    </span>
                </a>
            </li>
        
            <li>
                <a class="level is-mobile" href="http://yinhongyu.com" target="_blank">
                    <span class="level-left">
                        <span class="level-item">尹宏宇</span>
                    </span>
                    <span class="level-right">
                        <span class="level-item tag">yinhongyu.com</span>
                    </span>
                </a>
            </li>
        
        </ul>
        </div>
    </div>
</div>


    
        
<div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                分类
            </h3>
            <ul class="menu-list">
            <li>
        <a class="level is-marginless" href="/categories/NLP/">
            <span class="level-start">
                <span class="level-item">NLP</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/大数据/">
            <span class="level-start">
                <span class="level-item">大数据</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li>
            </ul>
        </div>
    </div>
</div>
    
        
<div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            标签云
        </h3>
        <a href="/tags/ETL/" style="font-size: 10px;">ETL</a> <a href="/tags/Lubridate/" style="font-size: 10px;">Lubridate</a> <a href="/tags/MapReduce/" style="font-size: 10px;">MapReduce</a> <a href="/tags/R/" style="font-size: 10px;">R</a> <a href="/tags/clustering/" style="font-size: 10px;">clustering</a> <a href="/tags/kmeans/" style="font-size: 10px;">kmeans</a> <a href="/tags/优化算法/" style="font-size: 10px;">优化算法</a> <a href="/tags/岭回归/" style="font-size: 10px;">岭回归</a> <a href="/tags/感知机/" style="font-size: 10px;">感知机</a> <a href="/tags/排序/" style="font-size: 10px;">排序</a> <a href="/tags/数据仓库/" style="font-size: 10px;">数据仓库</a> <a href="/tags/数据结构/" style="font-size: 10px;">数据结构</a> <a href="/tags/机器学习/" style="font-size: 20px;">机器学习</a> <a href="/tags/梯度下降/" style="font-size: 10px;">梯度下降</a> <a href="/tags/深度学习/" style="font-size: 10px;">深度学习</a> <a href="/tags/爬虫/" style="font-size: 16.67px;">爬虫</a> <a href="/tags/矩阵分解/" style="font-size: 13.33px;">矩阵分解</a> <a href="/tags/神经网络/" style="font-size: 10px;">神经网络</a> <a href="/tags/线性回归/" style="font-size: 10px;">线性回归</a> <a href="/tags/自然语言处理/" style="font-size: 10px;">自然语言处理</a> <a href="/tags/逐步回归/" style="font-size: 10px;">逐步回归</a> <a href="/tags/降维/" style="font-size: 10px;">降维</a>
    </div>
</div>

    
    
        <div class="column-right-shadow is-hidden-widescreen is-sticky">
        
            
<div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            最新文章
        </h3>
        
        <article class="media">
            
            <a href="/2018/07/02/R语言简明教程/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="R语言简明教程">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-07-02T03:43:55.696Z">2018-07-02</time></div>
                    <a href="/2018/07/02/R语言简明教程/" class="has-link-black-ter is-size-6">R语言简明教程</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2018/05/09/排序算法/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="排序算法">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-05-08T16:00:00.000Z">2018-05-09</time></div>
                    <a href="/2018/05/09/排序算法/" class="has-link-black-ter is-size-6">排序算法</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2018/05/05/梯度下降/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="线性回归与梯度下降算法">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-05-04T16:00:00.000Z">2018-05-05</time></div>
                    <a href="/2018/05/05/梯度下降/" class="has-link-black-ter is-size-6">线性回归与梯度下降算法</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2018/05/04/牛顿法/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="平方根与牛顿法">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-05-04T02:50:15.447Z">2018-05-04</time></div>
                    <a href="/2018/05/04/牛顿法/" class="has-link-black-ter is-size-6">平方根与牛顿法</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2018/05/03/逻辑回归/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="逻辑回归">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-05-02T16:00:00.000Z">2018-05-03</time></div>
                    <a href="/2018/05/03/逻辑回归/" class="has-link-black-ter is-size-6">逻辑回归</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
    </div>
</div>

        
            <div class="card widget">
    <div class="card-content">
        <div class="menu">
        <h3 class="menu-label">
            归档
        </h3>
        <ul class="menu-list">
        
        <li>
            <a class="level is-marginless" href="/archives/2018/07/">
                <span class="level-start">
                    <span class="level-item">七月 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2018/05/">
                <span class="level-start">
                    <span class="level-item">五月 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">5</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2018/04/">
                <span class="level-start">
                    <span class="level-item">四月 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">16</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2018/03/">
                <span class="level-start">
                    <span class="level-item">三月 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">3</span>
                </span>
            </a>
        </li>
        
        </ul>
        </div>
    </div>
</div>
        
            <div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                标签
            </h3>
            <div class="field is-grouped is-grouped-multiline">
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/ETL/">
                        <span class="tag">ETL</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Lubridate/">
                        <span class="tag">Lubridate</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/MapReduce/">
                        <span class="tag">MapReduce</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/R/">
                        <span class="tag">R</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/clustering/">
                        <span class="tag">clustering</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/kmeans/">
                        <span class="tag">kmeans</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/优化算法/">
                        <span class="tag">优化算法</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/岭回归/">
                        <span class="tag">岭回归</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/感知机/">
                        <span class="tag">感知机</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/排序/">
                        <span class="tag">排序</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/数据仓库/">
                        <span class="tag">数据仓库</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/数据结构/">
                        <span class="tag">数据结构</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/机器学习/">
                        <span class="tag">机器学习</span>
                        <span class="tag is-grey">9</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/梯度下降/">
                        <span class="tag">梯度下降</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/深度学习/">
                        <span class="tag">深度学习</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/爬虫/">
                        <span class="tag">爬虫</span>
                        <span class="tag is-grey">4</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/矩阵分解/">
                        <span class="tag">矩阵分解</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/神经网络/">
                        <span class="tag">神经网络</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/线性回归/">
                        <span class="tag">线性回归</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/自然语言处理/">
                        <span class="tag">自然语言处理</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/逐步回归/">
                        <span class="tag">逐步回归</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/降维/">
                        <span class="tag">降维</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
            </div>
        </div>
    </div>
</div>
        
        </div>
    
</div>

                




<div class="column is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only has-order-3 column-right is-sticky">
    
        
<div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            最新文章
        </h3>
        
        <article class="media">
            
            <a href="/2018/07/02/R语言简明教程/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="R语言简明教程">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-07-02T03:43:55.696Z">2018-07-02</time></div>
                    <a href="/2018/07/02/R语言简明教程/" class="has-link-black-ter is-size-6">R语言简明教程</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2018/05/09/排序算法/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="排序算法">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-05-08T16:00:00.000Z">2018-05-09</time></div>
                    <a href="/2018/05/09/排序算法/" class="has-link-black-ter is-size-6">排序算法</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2018/05/05/梯度下降/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="线性回归与梯度下降算法">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-05-04T16:00:00.000Z">2018-05-05</time></div>
                    <a href="/2018/05/05/梯度下降/" class="has-link-black-ter is-size-6">线性回归与梯度下降算法</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2018/05/04/牛顿法/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="平方根与牛顿法">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-05-04T02:50:15.447Z">2018-05-04</time></div>
                    <a href="/2018/05/04/牛顿法/" class="has-link-black-ter is-size-6">平方根与牛顿法</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2018/05/03/逻辑回归/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="逻辑回归">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2018-05-02T16:00:00.000Z">2018-05-03</time></div>
                    <a href="/2018/05/03/逻辑回归/" class="has-link-black-ter is-size-6">逻辑回归</a>
                    <p class="is-size-7 is-uppercase">
                        
                    </p>
                </div>
            </div>
        </article>
        
    </div>
</div>

    
        <div class="card widget">
    <div class="card-content">
        <div class="menu">
        <h3 class="menu-label">
            归档
        </h3>
        <ul class="menu-list">
        
        <li>
            <a class="level is-marginless" href="/archives/2018/07/">
                <span class="level-start">
                    <span class="level-item">七月 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2018/05/">
                <span class="level-start">
                    <span class="level-item">五月 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">5</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2018/04/">
                <span class="level-start">
                    <span class="level-item">四月 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">16</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2018/03/">
                <span class="level-start">
                    <span class="level-item">三月 2018</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">3</span>
                </span>
            </a>
        </li>
        
        </ul>
        </div>
    </div>
</div>
    
        <div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                标签
            </h3>
            <div class="field is-grouped is-grouped-multiline">
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/ETL/">
                        <span class="tag">ETL</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Lubridate/">
                        <span class="tag">Lubridate</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/MapReduce/">
                        <span class="tag">MapReduce</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/R/">
                        <span class="tag">R</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/clustering/">
                        <span class="tag">clustering</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/kmeans/">
                        <span class="tag">kmeans</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/优化算法/">
                        <span class="tag">优化算法</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/岭回归/">
                        <span class="tag">岭回归</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/感知机/">
                        <span class="tag">感知机</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/排序/">
                        <span class="tag">排序</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/数据仓库/">
                        <span class="tag">数据仓库</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/数据结构/">
                        <span class="tag">数据结构</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/机器学习/">
                        <span class="tag">机器学习</span>
                        <span class="tag is-grey">9</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/梯度下降/">
                        <span class="tag">梯度下降</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/深度学习/">
                        <span class="tag">深度学习</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/爬虫/">
                        <span class="tag">爬虫</span>
                        <span class="tag is-grey">4</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/矩阵分解/">
                        <span class="tag">矩阵分解</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/神经网络/">
                        <span class="tag">神经网络</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/线性回归/">
                        <span class="tag">线性回归</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/自然语言处理/">
                        <span class="tag">自然语言处理</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/逐步回归/">
                        <span class="tag">逐步回归</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/降维/">
                        <span class="tag">降维</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
            </div>
        </div>
    </div>
</div>
    
    
</div>

            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/">
                
                    <img src="/images/logo.svg" alt="Young&#39;s Blog" height="28">
                
                </a>
                <p class="is-size-7">
                &copy; 2019 冯洋洋&nbsp;
                Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> & <a
                        href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a>
                
                </p>
            </div>
            <div class="level-end">
            
            </div>
        </div>
    </div>
</footer>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("zh-CN");</script>


    
    
    
    <script src="/js/animation.js"></script>
    

    
    
    
    <script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
    <script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
    <script src="/js/gallery.js" defer></script>
    

    
    

<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>


    
    
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        'HTML-CSS': {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)'],
				['$$','$$']
            ]
        }
    });
});
</script>

    
    

<a id="back-to-top" title="回到顶端" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/js/back-to-top.js" defer></script>


    
    
    
    

    
    
    
    
    
    <script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>
    <script src="/js/clipboard.js" defer></script>
    

    
    
    


<script src="/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="想要查找什么..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: '文章',
                PAGES: '页面',
                CATEGORIES: '分类',
                TAGS: '标签',
                UNTITLED: '(无标题)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js" defer></script>
<link rel="stylesheet" href="/css/search.css">
<link rel="stylesheet" href="/css/insight.css">
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>